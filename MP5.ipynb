{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrq1ruuZFMnQ"
   },
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9MMOSRTFMnW"
   },
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4209,
     "status": "ok",
     "timestamp": 1607561612930,
     "user": {
      "displayName": "Darren Anco",
      "photoUrl": "",
      "userId": "04554837860039334829"
     },
     "user_tz": 360
    },
    "id": "E5U6o6qyFMnW",
    "outputId": "40bc5dec-5f7e-4e96-808f-c1d5604f6f13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
      "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
      "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
      "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
      "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gym pyvirtualdisplay\n",
    "!sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7245,
     "status": "ok",
     "timestamp": 1607561629266,
     "user": {
      "displayName": "Darren Anco",
      "photoUrl": "",
      "userId": "04554837860039334829"
     },
     "user_tz": 360
    },
    "id": "asYOHjhEFMnX",
    "outputId": "28a97869-d4a7-4ab1-d2bb-f95c563f9ace"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (51.0.0)\n",
      "Requirement already satisfied: ez_setup in /usr/local/lib/python3.6/dist-packages (0.9)\n",
      "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.18.5)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.1)\n",
      "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
      "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n",
      "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade setuptools\n",
    "!pip3 install ez_setup \n",
    "!pip3 install gym[atari] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VumMyVGtFMnY"
   },
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 6363,
     "status": "ok",
     "timestamp": 1607561638480,
     "user": {
      "displayName": "Darren Anco",
      "photoUrl": "",
      "userId": "04554837860039334829"
     },
     "user_tz": 360
    },
    "id": "o2k-iyFIFMnY"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZn_GO5GFMnY"
   },
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4o0wVIaiFMnY"
   },
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 795,
     "status": "ok",
     "timestamp": 1607561642225,
     "user": {
      "displayName": "Darren Anco",
      "photoUrl": "",
      "userId": "04554837860039334829"
     },
     "user_tz": 360
    },
    "id": "lgrozXUYFMnZ"
   },
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1607561642807,
     "user": {
      "displayName": "Darren Anco",
      "photoUrl": "",
      "userId": "04554837860039334829"
     },
     "user_tz": 360
    },
    "id": "R_Kj7k2iFMnZ"
   },
   "outputs": [],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McyPudFyFMnZ"
   },
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQxyesw-FMnZ"
   },
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 11621,
     "status": "ok",
     "timestamp": 1607561657199,
     "user": {
      "displayName": "Darren Anco",
      "photoUrl": "",
      "userId": "04554837860039334829"
     },
     "user_tz": 360
    },
    "id": "bah2WmRCFMnZ"
   },
   "outputs": [],
   "source": [
    "double_dqn = True # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyNR7poNFMna"
   },
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RUnz5AaFMna"
   },
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4tCZSBJFMna"
   },
   "source": [
    "# DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0RicOsUFMna",
    "outputId": "454fa6d6-2950-4551-d997-a0b1be54b6e2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 3.0   memory length: 262   epsilon: 1.0    steps: 262    lr: 0.0001     evaluation reward: 3.0\n",
      "episode: 1   score: 2.0   memory length: 460   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 2   score: 1.0   memory length: 629   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 3   score: 1.0   memory length: 798   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 4   score: 0.0   memory length: 920   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 5   score: 0.0   memory length: 1042   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.1666666666666667\n",
      "episode: 6   score: 0.0   memory length: 1165   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.0\n",
      "episode: 7   score: 5.0   memory length: 1483   epsilon: 1.0    steps: 318    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 8   score: 2.0   memory length: 1681   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5555555555555556\n",
      "episode: 9   score: 0.0   memory length: 1803   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 10   score: 1.0   memory length: 1954   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3636363636363635\n",
      "episode: 11   score: 0.0   memory length: 2077   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 12   score: 2.0   memory length: 2276   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.3076923076923077\n",
      "episode: 13   score: 0.0   memory length: 2399   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2142857142857142\n",
      "episode: 14   score: 7.0   memory length: 2720   epsilon: 1.0    steps: 321    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 15   score: 2.0   memory length: 2917   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.625\n",
      "episode: 16   score: 1.0   memory length: 3088   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.588235294117647\n",
      "episode: 17   score: 3.0   memory length: 3334   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 18   score: 0.0   memory length: 3457   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5789473684210527\n",
      "episode: 19   score: 2.0   memory length: 3655   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 20   score: 4.0   memory length: 3924   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.7142857142857142\n",
      "episode: 21   score: 0.0   memory length: 4046   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
      "episode: 22   score: 1.0   memory length: 4215   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.608695652173913\n",
      "episode: 23   score: 1.0   memory length: 4384   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5833333333333333\n",
      "episode: 24   score: 0.0   memory length: 4506   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 25   score: 0.0   memory length: 4628   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4615384615384615\n",
      "episode: 26   score: 4.0   memory length: 4926   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.5555555555555556\n",
      "episode: 27   score: 1.0   memory length: 5095   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5357142857142858\n",
      "episode: 28   score: 1.0   memory length: 5264   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5172413793103448\n",
      "episode: 29   score: 0.0   memory length: 5387   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4666666666666666\n",
      "episode: 30   score: 2.0   memory length: 5585   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4838709677419355\n",
      "episode: 31   score: 1.0   memory length: 5754   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46875\n",
      "episode: 32   score: 2.0   memory length: 5973   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.4848484848484849\n",
      "episode: 33   score: 2.0   memory length: 6172   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 34   score: 1.0   memory length: 6323   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4857142857142858\n",
      "episode: 35   score: 0.0   memory length: 6446   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4444444444444444\n",
      "episode: 36   score: 2.0   memory length: 6665   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.4594594594594594\n",
      "episode: 37   score: 0.0   memory length: 6788   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4210526315789473\n",
      "episode: 38   score: 1.0   memory length: 6938   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.4102564102564104\n",
      "episode: 39   score: 0.0   memory length: 7061   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.375\n",
      "episode: 40   score: 3.0   memory length: 7325   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.4146341463414633\n",
      "episode: 41   score: 0.0   memory length: 7448   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.380952380952381\n",
      "episode: 42   score: 1.0   memory length: 7598   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.372093023255814\n",
      "episode: 43   score: 3.0   memory length: 7843   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.4090909090909092\n",
      "episode: 44   score: 1.0   memory length: 7994   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 45   score: 1.0   memory length: 8145   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.391304347826087\n",
      "episode: 46   score: 1.0   memory length: 8296   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3829787234042554\n",
      "episode: 47   score: 1.0   memory length: 8464   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.375\n",
      "episode: 48   score: 0.0   memory length: 8587   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.346938775510204\n",
      "episode: 49   score: 1.0   memory length: 8737   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 50   score: 0.0   memory length: 8860   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3137254901960784\n",
      "episode: 51   score: 3.0   memory length: 9128   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.3461538461538463\n",
      "episode: 52   score: 3.0   memory length: 9355   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.3773584905660377\n",
      "episode: 53   score: 1.0   memory length: 9524   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3703703703703705\n",
      "episode: 54   score: 1.0   memory length: 9675   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3636363636363635\n",
      "episode: 55   score: 0.0   memory length: 9798   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3392857142857142\n",
      "episode: 56   score: 3.0   memory length: 10044   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.368421052631579\n",
      "episode: 57   score: 0.0   memory length: 10166   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3448275862068966\n",
      "episode: 58   score: 1.0   memory length: 10336   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.3389830508474576\n",
      "episode: 59   score: 4.0   memory length: 10629   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.3833333333333333\n",
      "episode: 60   score: 1.0   memory length: 10780   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3770491803278688\n",
      "episode: 61   score: 1.0   memory length: 10948   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.3709677419354838\n",
      "episode: 62   score: 0.0   memory length: 11070   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3492063492063493\n",
      "episode: 63   score: 1.0   memory length: 11239   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.34375\n",
      "episode: 64   score: 0.0   memory length: 11362   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.323076923076923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 65   score: 4.0   memory length: 11637   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.3636363636363635\n",
      "episode: 66   score: 0.0   memory length: 11760   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3432835820895523\n",
      "episode: 67   score: 0.0   memory length: 11883   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3235294117647058\n",
      "episode: 68   score: 2.0   memory length: 12081   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
      "episode: 69   score: 0.0   memory length: 12204   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3142857142857143\n",
      "episode: 70   score: 0.0   memory length: 12327   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.295774647887324\n",
      "episode: 71   score: 3.0   memory length: 12574   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.3194444444444444\n",
      "episode: 72   score: 2.0   memory length: 12794   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.3287671232876712\n",
      "episode: 73   score: 1.0   memory length: 12962   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.3243243243243243\n",
      "episode: 74   score: 1.0   memory length: 13113   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 75   score: 0.0   memory length: 13236   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3026315789473684\n",
      "episode: 76   score: 0.0   memory length: 13359   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2857142857142858\n",
      "episode: 77   score: 4.0   memory length: 13654   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.3205128205128205\n",
      "episode: 78   score: 1.0   memory length: 13804   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.3164556962025316\n",
      "episode: 79   score: 0.0   memory length: 13927   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 80   score: 2.0   memory length: 14128   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.308641975308642\n",
      "episode: 81   score: 1.0   memory length: 14279   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3048780487804879\n",
      "episode: 82   score: 0.0   memory length: 14401   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2891566265060241\n",
      "episode: 83   score: 1.0   memory length: 14572   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.2857142857142858\n",
      "episode: 84   score: 2.0   memory length: 14751   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.2941176470588236\n",
      "episode: 85   score: 2.0   memory length: 14949   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.302325581395349\n",
      "episode: 86   score: 2.0   memory length: 15147   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3103448275862069\n",
      "episode: 87   score: 3.0   memory length: 15391   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.3295454545454546\n",
      "episode: 88   score: 1.0   memory length: 15542   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3258426966292134\n",
      "episode: 89   score: 0.0   memory length: 15664   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3111111111111111\n",
      "episode: 90   score: 2.0   memory length: 15880   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.3186813186813187\n",
      "episode: 91   score: 1.0   memory length: 16051   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.315217391304348\n",
      "episode: 92   score: 0.0   memory length: 16174   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3010752688172043\n",
      "episode: 93   score: 1.0   memory length: 16344   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.297872340425532\n",
      "episode: 94   score: 0.0   memory length: 16466   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2842105263157895\n",
      "episode: 95   score: 4.0   memory length: 16740   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.3125\n",
      "episode: 96   score: 0.0   memory length: 16863   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2989690721649485\n",
      "episode: 97   score: 1.0   memory length: 17013   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.2959183673469388\n",
      "episode: 98   score: 1.0   memory length: 17181   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.292929292929293\n",
      "episode: 99   score: 0.0   memory length: 17304   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 100   score: 0.0   memory length: 17427   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 101   score: 1.0   memory length: 17578   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 102   score: 0.0   memory length: 17700   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 103   score: 1.0   memory length: 17869   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 104   score: 0.0   memory length: 17992   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 105   score: 0.0   memory length: 18114   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 106   score: 0.0   memory length: 18236   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 107   score: 0.0   memory length: 18358   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 108   score: 1.0   memory length: 18528   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.17\n",
      "episode: 109   score: 2.0   memory length: 18726   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 110   score: 1.0   memory length: 18895   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 111   score: 1.0   memory length: 19067   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 112   score: 1.0   memory length: 19218   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 113   score: 0.0   memory length: 19341   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 114   score: 4.0   memory length: 19640   epsilon: 1.0    steps: 299    lr: 0.0001     evaluation reward: 1.16\n",
      "episode: 115   score: 2.0   memory length: 19858   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.16\n",
      "episode: 116   score: 3.0   memory length: 20102   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 117   score: 2.0   memory length: 20300   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.17\n",
      "episode: 118   score: 1.0   memory length: 20470   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 119   score: 1.0   memory length: 20638   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.17\n",
      "episode: 120   score: 4.0   memory length: 20953   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.17\n",
      "episode: 121   score: 3.0   memory length: 21200   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 122   score: 3.0   memory length: 21449   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 123   score: 3.0   memory length: 21678   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 124   score: 3.0   memory length: 21904   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 125   score: 2.0   memory length: 22102   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 126   score: 0.0   memory length: 22224   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 127   score: 2.0   memory length: 22442   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 128   score: 1.0   memory length: 22593   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 129   score: 3.0   memory length: 22839   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 130   score: 0.0   memory length: 22962   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 131   score: 2.0   memory length: 23181   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 132   score: 2.0   memory length: 23399   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 133   score: 2.0   memory length: 23597   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 134   score: 1.0   memory length: 23766   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 135   score: 1.0   memory length: 23935   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 136   score: 0.0   memory length: 24057   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 137   score: 1.0   memory length: 24208   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 138   score: 0.0   memory length: 24331   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 139   score: 2.0   memory length: 24512   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 140   score: 1.0   memory length: 24663   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 141   score: 2.0   memory length: 24861   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 142   score: 0.0   memory length: 24983   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 143   score: 3.0   memory length: 25210   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 144   score: 4.0   memory length: 25509   epsilon: 1.0    steps: 299    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 145   score: 0.0   memory length: 25632   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 146   score: 2.0   memory length: 25830   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 147   score: 0.0   memory length: 25952   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 148   score: 3.0   memory length: 26222   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 149   score: 2.0   memory length: 26420   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 150   score: 4.0   memory length: 26697   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 151   score: 1.0   memory length: 26847   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 152   score: 2.0   memory length: 27063   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 153   score: 0.0   memory length: 27186   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 154   score: 1.0   memory length: 27337   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 155   score: 2.0   memory length: 27555   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 156   score: 0.0   memory length: 27678   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 157   score: 0.0   memory length: 27801   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 158   score: 2.0   memory length: 27999   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 159   score: 1.0   memory length: 28149   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 160   score: 0.0   memory length: 28272   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 161   score: 4.0   memory length: 28564   epsilon: 1.0    steps: 292    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 162   score: 2.0   memory length: 28762   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 163   score: 2.0   memory length: 28980   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 164   score: 1.0   memory length: 29151   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 165   score: 1.0   memory length: 29303   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 166   score: 1.0   memory length: 29454   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 167   score: 0.0   memory length: 29577   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 168   score: 0.0   memory length: 29700   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 169   score: 2.0   memory length: 29897   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 170   score: 2.0   memory length: 30113   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 171   score: 0.0   memory length: 30236   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 172   score: 0.0   memory length: 30358   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 173   score: 4.0   memory length: 30633   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 174   score: 1.0   memory length: 30802   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 175   score: 0.0   memory length: 30925   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 176   score: 0.0   memory length: 31047   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 177   score: 3.0   memory length: 31312   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 178   score: 1.0   memory length: 31480   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 179   score: 0.0   memory length: 31602   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 180   score: 2.0   memory length: 31800   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 181   score: 1.0   memory length: 31950   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 182   score: 1.0   memory length: 32119   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 183   score: 2.0   memory length: 32337   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 184   score: 2.0   memory length: 32534   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 185   score: 0.0   memory length: 32657   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 186   score: 1.0   memory length: 32808   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 187   score: 0.0   memory length: 32931   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 188   score: 3.0   memory length: 33180   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 189   score: 4.0   memory length: 33495   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 190   score: 4.0   memory length: 33809   epsilon: 1.0    steps: 314    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 191   score: 0.0   memory length: 33932   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 192   score: 3.0   memory length: 34198   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 193   score: 2.0   memory length: 34416   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 194   score: 1.0   memory length: 34586   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 195   score: 3.0   memory length: 34856   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 196   score: 4.0   memory length: 35130   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 197   score: 2.0   memory length: 35348   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 198   score: 3.0   memory length: 35615   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 199   score: 1.0   memory length: 35765   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 200   score: 3.0   memory length: 35994   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 201   score: 0.0   memory length: 36117   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 202   score: 4.0   memory length: 36413   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 203   score: 0.0   memory length: 36536   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 204   score: 1.0   memory length: 36706   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 205   score: 0.0   memory length: 36829   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 206   score: 0.0   memory length: 36952   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 207   score: 3.0   memory length: 37199   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 208   score: 5.0   memory length: 37495   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 209   score: 1.0   memory length: 37646   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 210   score: 1.0   memory length: 37815   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 211   score: 0.0   memory length: 37938   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 212   score: 0.0   memory length: 38061   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 213   score: 3.0   memory length: 38286   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 214   score: 2.0   memory length: 38504   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 215   score: 1.0   memory length: 38655   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 216   score: 2.0   memory length: 38852   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 217   score: 0.0   memory length: 38975   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 218   score: 0.0   memory length: 39097   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 219   score: 0.0   memory length: 39220   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 220   score: 3.0   memory length: 39468   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 221   score: 3.0   memory length: 39732   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 222   score: 1.0   memory length: 39901   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 223   score: 0.0   memory length: 40024   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 224   score: 0.0   memory length: 40147   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 225   score: 0.0   memory length: 40270   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 226   score: 0.0   memory length: 40392   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 227   score: 0.0   memory length: 40515   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 228   score: 1.0   memory length: 40686   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 229   score: 0.0   memory length: 40809   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 230   score: 2.0   memory length: 41029   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 231   score: 3.0   memory length: 41261   epsilon: 1.0    steps: 232    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 232   score: 3.0   memory length: 41510   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 233   score: 0.0   memory length: 41633   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 234   score: 3.0   memory length: 41861   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 235   score: 1.0   memory length: 42012   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 236   score: 0.0   memory length: 42135   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 237   score: 1.0   memory length: 42304   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 238   score: 1.0   memory length: 42454   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 239   score: 2.0   memory length: 42652   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 240   score: 1.0   memory length: 42820   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 241   score: 1.0   memory length: 42989   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 242   score: 0.0   memory length: 43112   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 243   score: 3.0   memory length: 43358   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 244   score: 2.0   memory length: 43579   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 245   score: 0.0   memory length: 43702   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 246   score: 3.0   memory length: 43946   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 247   score: 0.0   memory length: 44069   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 248   score: 3.0   memory length: 44296   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 249   score: 0.0   memory length: 44419   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 250   score: 2.0   memory length: 44617   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 251   score: 0.0   memory length: 44739   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 252   score: 1.0   memory length: 44889   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 253   score: 1.0   memory length: 45058   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 254   score: 1.0   memory length: 45228   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 255   score: 0.0   memory length: 45351   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 256   score: 0.0   memory length: 45474   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 257   score: 1.0   memory length: 45643   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 258   score: 0.0   memory length: 45765   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 259   score: 3.0   memory length: 46011   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 260   score: 0.0   memory length: 46134   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 261   score: 0.0   memory length: 46256   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 262   score: 0.0   memory length: 46379   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 263   score: 0.0   memory length: 46502   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 264   score: 0.0   memory length: 46625   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 265   score: 3.0   memory length: 46851   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 266   score: 1.0   memory length: 47002   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 267   score: 4.0   memory length: 47262   epsilon: 1.0    steps: 260    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 268   score: 1.0   memory length: 47430   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 269   score: 0.0   memory length: 47553   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 270   score: 1.0   memory length: 47722   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 271   score: 3.0   memory length: 47969   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 272   score: 0.0   memory length: 48091   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 273   score: 0.0   memory length: 48214   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 274   score: 0.0   memory length: 48336   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 275   score: 1.0   memory length: 48487   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 276   score: 2.0   memory length: 48706   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 277   score: 4.0   memory length: 49020   epsilon: 1.0    steps: 314    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 278   score: 1.0   memory length: 49170   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 279   score: 4.0   memory length: 49468   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 280   score: 0.0   memory length: 49591   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 281   score: 1.0   memory length: 49762   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 282   score: 2.0   memory length: 49960   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 283   score: 3.0   memory length: 50205   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 284   score: 1.0   memory length: 50356   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 285   score: 2.0   memory length: 50572   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 286   score: 2.0   memory length: 50788   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 287   score: 1.0   memory length: 50957   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 288   score: 1.0   memory length: 51108   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 289   score: 0.0   memory length: 51231   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 290   score: 1.0   memory length: 51400   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 291   score: 4.0   memory length: 51693   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 292   score: 1.0   memory length: 51843   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 293   score: 1.0   memory length: 52012   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 294   score: 0.0   memory length: 52134   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 295   score: 4.0   memory length: 52393   epsilon: 1.0    steps: 259    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 296   score: 7.0   memory length: 52714   epsilon: 1.0    steps: 321    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 297   score: 2.0   memory length: 52899   epsilon: 1.0    steps: 185    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 298   score: 2.0   memory length: 53097   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 299   score: 0.0   memory length: 53219   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 300   score: 2.0   memory length: 53437   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 301   score: 1.0   memory length: 53587   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 302   score: 2.0   memory length: 53785   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 303   score: 4.0   memory length: 54027   epsilon: 1.0    steps: 242    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 304   score: 2.0   memory length: 54244   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 305   score: 1.0   memory length: 54413   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 306   score: 1.0   memory length: 54582   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 307   score: 0.0   memory length: 54705   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 308   score: 0.0   memory length: 54828   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 309   score: 1.0   memory length: 54979   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 310   score: 0.0   memory length: 55102   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 311   score: 1.0   memory length: 55273   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 312   score: 5.0   memory length: 55638   epsilon: 1.0    steps: 365    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 313   score: 3.0   memory length: 55905   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 314   score: 0.0   memory length: 56027   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 315   score: 0.0   memory length: 56150   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 316   score: 1.0   memory length: 56301   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 317   score: 5.0   memory length: 56617   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 318   score: 5.0   memory length: 56983   epsilon: 1.0    steps: 366    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 319   score: 1.0   memory length: 57133   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 320   score: 0.0   memory length: 57256   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 321   score: 1.0   memory length: 57425   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 322   score: 1.0   memory length: 57593   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 323   score: 0.0   memory length: 57715   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 324   score: 0.0   memory length: 57837   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 325   score: 1.0   memory length: 57989   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 326   score: 0.0   memory length: 58112   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 327   score: 0.0   memory length: 58235   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 328   score: 5.0   memory length: 58519   epsilon: 1.0    steps: 284    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 329   score: 3.0   memory length: 58786   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 330   score: 1.0   memory length: 58956   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 331   score: 1.0   memory length: 59107   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 332   score: 4.0   memory length: 59383   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 333   score: 2.0   memory length: 59600   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 334   score: 3.0   memory length: 59845   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 335   score: 2.0   memory length: 60042   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 336   score: 4.0   memory length: 60336   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 337   score: 1.0   memory length: 60505   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 338   score: 6.0   memory length: 60860   epsilon: 1.0    steps: 355    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 339   score: 2.0   memory length: 61057   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 340   score: 2.0   memory length: 61239   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 341   score: 3.0   memory length: 61468   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 342   score: 1.0   memory length: 61619   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 343   score: 2.0   memory length: 61801   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 344   score: 0.0   memory length: 61924   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 345   score: 3.0   memory length: 62150   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 346   score: 1.0   memory length: 62319   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 347   score: 2.0   memory length: 62537   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 348   score: 0.0   memory length: 62659   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 349   score: 6.0   memory length: 62997   epsilon: 1.0    steps: 338    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 350   score: 3.0   memory length: 63266   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 351   score: 1.0   memory length: 63435   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 352   score: 4.0   memory length: 63712   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 353   score: 1.0   memory length: 63881   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 354   score: 2.0   memory length: 64098   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 355   score: 0.0   memory length: 64220   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 356   score: 2.0   memory length: 64438   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 357   score: 2.0   memory length: 64638   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 358   score: 0.0   memory length: 64761   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 359   score: 0.0   memory length: 64884   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 360   score: 3.0   memory length: 65110   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 361   score: 2.0   memory length: 65312   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 362   score: 1.0   memory length: 65481   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 363   score: 0.0   memory length: 65604   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 364   score: 1.0   memory length: 65776   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 365   score: 3.0   memory length: 66002   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 366   score: 1.0   memory length: 66171   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 367   score: 3.0   memory length: 66382   epsilon: 1.0    steps: 211    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 368   score: 1.0   memory length: 66553   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 369   score: 2.0   memory length: 66751   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 370   score: 2.0   memory length: 66969   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 371   score: 0.0   memory length: 67092   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 372   score: 1.0   memory length: 67242   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 373   score: 1.0   memory length: 67414   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 374   score: 0.0   memory length: 67537   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 375   score: 1.0   memory length: 67688   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 376   score: 2.0   memory length: 67886   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 377   score: 1.0   memory length: 68037   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 378   score: 2.0   memory length: 68218   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 379   score: 0.0   memory length: 68341   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 380   score: 1.0   memory length: 68509   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 381   score: 3.0   memory length: 68739   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 382   score: 0.0   memory length: 68861   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 383   score: 4.0   memory length: 69159   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 384   score: 2.0   memory length: 69357   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 385   score: 2.0   memory length: 69573   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 386   score: 2.0   memory length: 69770   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 387   score: 1.0   memory length: 69921   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 388   score: 3.0   memory length: 70189   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 389   score: 0.0   memory length: 70312   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 390   score: 0.0   memory length: 70435   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 391   score: 2.0   memory length: 70633   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 392   score: 0.0   memory length: 70755   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 393   score: 0.0   memory length: 70877   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 394   score: 2.0   memory length: 71098   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 395   score: 0.0   memory length: 71221   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 396   score: 2.0   memory length: 71419   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 397   score: 2.0   memory length: 71618   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 398   score: 1.0   memory length: 71788   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 399   score: 2.0   memory length: 71985   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 400   score: 1.0   memory length: 72135   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 401   score: 2.0   memory length: 72333   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 402   score: 2.0   memory length: 72534   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 403   score: 0.0   memory length: 72656   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 404   score: 0.0   memory length: 72779   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 405   score: 1.0   memory length: 72949   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 406   score: 2.0   memory length: 73167   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 407   score: 2.0   memory length: 73348   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 408   score: 2.0   memory length: 73548   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 409   score: 1.0   memory length: 73700   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 410   score: 0.0   memory length: 73823   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 411   score: 1.0   memory length: 73974   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 412   score: 0.0   memory length: 74097   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 413   score: 3.0   memory length: 74345   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 414   score: 0.0   memory length: 74468   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 415   score: 3.0   memory length: 74716   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 416   score: 1.0   memory length: 74885   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 417   score: 4.0   memory length: 75144   epsilon: 1.0    steps: 259    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 418   score: 2.0   memory length: 75344   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 419   score: 3.0   memory length: 75609   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 420   score: 1.0   memory length: 75778   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 421   score: 0.0   memory length: 75901   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 422   score: 1.0   memory length: 76070   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 423   score: 0.0   memory length: 76193   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 424   score: 2.0   memory length: 76390   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 425   score: 1.0   memory length: 76558   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 426   score: 0.0   memory length: 76681   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 427   score: 2.0   memory length: 76880   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 428   score: 1.0   memory length: 77031   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 429   score: 6.0   memory length: 77406   epsilon: 1.0    steps: 375    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 430   score: 2.0   memory length: 77624   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 431   score: 3.0   memory length: 77871   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 432   score: 3.0   memory length: 78120   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 433   score: 3.0   memory length: 78346   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 434   score: 5.0   memory length: 78690   epsilon: 1.0    steps: 344    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 435   score: 2.0   memory length: 78887   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 436   score: 1.0   memory length: 79037   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 437   score: 1.0   memory length: 79206   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 438   score: 3.0   memory length: 79436   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 439   score: 3.0   memory length: 79664   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 440   score: 1.0   memory length: 79832   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 441   score: 2.0   memory length: 80050   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 442   score: 5.0   memory length: 80390   epsilon: 1.0    steps: 340    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 443   score: 0.0   memory length: 80512   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 444   score: 0.0   memory length: 80635   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 445   score: 3.0   memory length: 80881   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 446   score: 2.0   memory length: 81078   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 447   score: 1.0   memory length: 81249   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 448   score: 0.0   memory length: 81372   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 449   score: 0.0   memory length: 81495   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 450   score: 2.0   memory length: 81692   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 451   score: 1.0   memory length: 81843   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 452   score: 1.0   memory length: 82013   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 453   score: 0.0   memory length: 82136   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 454   score: 3.0   memory length: 82383   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 455   score: 0.0   memory length: 82506   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 456   score: 2.0   memory length: 82728   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 457   score: 0.0   memory length: 82851   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 458   score: 2.0   memory length: 83051   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 459   score: 0.0   memory length: 83173   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 460   score: 4.0   memory length: 83468   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 461   score: 2.0   memory length: 83666   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 462   score: 3.0   memory length: 83910   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 463   score: 1.0   memory length: 84060   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 464   score: 0.0   memory length: 84182   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 465   score: 3.0   memory length: 84410   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 466   score: 3.0   memory length: 84653   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 467   score: 5.0   memory length: 84980   epsilon: 1.0    steps: 327    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 468   score: 4.0   memory length: 85296   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 469   score: 3.0   memory length: 85564   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 470   score: 1.0   memory length: 85732   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 471   score: 1.0   memory length: 85883   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 472   score: 2.0   memory length: 86084   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 473   score: 1.0   memory length: 86253   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 474   score: 0.0   memory length: 86376   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 475   score: 1.0   memory length: 86547   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 476   score: 0.0   memory length: 86669   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 477   score: 0.0   memory length: 86792   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 478   score: 1.0   memory length: 86960   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 479   score: 0.0   memory length: 87082   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 480   score: 1.0   memory length: 87233   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 481   score: 1.0   memory length: 87402   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 482   score: 1.0   memory length: 87571   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 483   score: 2.0   memory length: 87768   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 484   score: 0.0   memory length: 87890   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 485   score: 0.0   memory length: 88013   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 486   score: 1.0   memory length: 88164   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 487   score: 0.0   memory length: 88287   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 488   score: 1.0   memory length: 88438   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 489   score: 2.0   memory length: 88655   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 490   score: 0.0   memory length: 88778   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 491   score: 1.0   memory length: 88929   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 492   score: 1.0   memory length: 89099   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 493   score: 2.0   memory length: 89318   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 494   score: 2.0   memory length: 89536   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 495   score: 1.0   memory length: 89705   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 496   score: 2.0   memory length: 89903   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 497   score: 5.0   memory length: 90212   epsilon: 1.0    steps: 309    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 498   score: 4.0   memory length: 90504   epsilon: 1.0    steps: 292    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 499   score: 0.0   memory length: 90627   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 500   score: 3.0   memory length: 90853   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 501   score: 5.0   memory length: 91197   epsilon: 1.0    steps: 344    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 502   score: 1.0   memory length: 91367   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 503   score: 5.0   memory length: 91683   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 504   score: 2.0   memory length: 91901   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 505   score: 6.0   memory length: 92252   epsilon: 1.0    steps: 351    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 506   score: 2.0   memory length: 92450   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 507   score: 3.0   memory length: 92716   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 508   score: 4.0   memory length: 92993   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 509   score: 3.0   memory length: 93239   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 510   score: 2.0   memory length: 93437   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 511   score: 0.0   memory length: 93560   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 512   score: 0.0   memory length: 93682   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 513   score: 1.0   memory length: 93833   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 514   score: 2.0   memory length: 94051   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 515   score: 1.0   memory length: 94220   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 516   score: 0.0   memory length: 94343   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 517   score: 0.0   memory length: 94466   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 518   score: 2.0   memory length: 94664   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 519   score: 4.0   memory length: 94979   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 520   score: 3.0   memory length: 95205   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 521   score: 2.0   memory length: 95405   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 522   score: 0.0   memory length: 95527   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 523   score: 4.0   memory length: 95805   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 524   score: 1.0   memory length: 95977   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 525   score: 3.0   memory length: 96222   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 526   score: 2.0   memory length: 96440   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 527   score: 1.0   memory length: 96612   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 528   score: 1.0   memory length: 96780   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 529   score: 2.0   memory length: 96978   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 530   score: 0.0   memory length: 97101   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 531   score: 1.0   memory length: 97269   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 532   score: 1.0   memory length: 97419   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 533   score: 1.0   memory length: 97588   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 534   score: 1.0   memory length: 97759   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 535   score: 3.0   memory length: 98007   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 536   score: 4.0   memory length: 98282   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 537   score: 2.0   memory length: 98497   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 538   score: 3.0   memory length: 98727   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 539   score: 2.0   memory length: 98924   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 540   score: 2.0   memory length: 99143   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 541   score: 1.0   memory length: 99313   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 542   score: 1.0   memory length: 99482   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 543   score: 0.0   memory length: 99605   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 544   score: 0.0   memory length: 99728   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 545   score: 0.0   memory length: 99850   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 546   score: 0.0   memory length: 99972   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 547   score: 3.0   memory length: 100220   epsilon: 0.9995624200000095    steps: 248    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 548   score: 2.0   memory length: 100438   epsilon: 0.9991307800000189    steps: 218    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 549   score: 3.0   memory length: 100684   epsilon: 0.9986437000000294    steps: 246    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 550   score: 2.0   memory length: 100881   epsilon: 0.9982536400000379    steps: 197    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 551   score: 2.0   memory length: 101078   epsilon: 0.9978635800000464    steps: 197    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 552   score: 4.0   memory length: 101375   epsilon: 0.9972755200000591    steps: 297    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 553   score: 4.0   memory length: 101671   epsilon: 0.9966894400000719    steps: 296    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 554   score: 3.0   memory length: 101938   epsilon: 0.9961607800000833    steps: 267    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 555   score: 2.0   memory length: 102135   epsilon: 0.9957707200000918    steps: 197    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 556   score: 4.0   memory length: 102411   epsilon: 0.9952242400001037    steps: 276    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 557   score: 4.0   memory length: 102723   epsilon: 0.9946064800001171    steps: 312    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 558   score: 0.0   memory length: 102845   epsilon: 0.9943649200001223    steps: 122    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 559   score: 0.0   memory length: 102968   epsilon: 0.9941213800001276    steps: 123    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 560   score: 0.0   memory length: 103090   epsilon: 0.9938798200001329    steps: 122    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 561   score: 0.0   memory length: 103212   epsilon: 0.9936382600001381    steps: 122    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 562   score: 2.0   memory length: 103412   epsilon: 0.9932422600001467    steps: 200    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 563   score: 0.0   memory length: 103535   epsilon: 0.992998720000152    steps: 123    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 564   score: 4.0   memory length: 103833   epsilon: 0.9924086800001648    steps: 298    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 565   score: 2.0   memory length: 104013   epsilon: 0.9920522800001725    steps: 180    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 566   score: 1.0   memory length: 104185   epsilon: 0.9917117200001799    steps: 172    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 567   score: 1.0   memory length: 104354   epsilon: 0.9913771000001872    steps: 169    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 568   score: 0.0   memory length: 104476   epsilon: 0.9911355400001924    steps: 122    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 569   score: 1.0   memory length: 104644   epsilon: 0.9908029000001997    steps: 168    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 570   score: 1.0   memory length: 104795   epsilon: 0.9905039200002062    steps: 151    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 571   score: 5.0   memory length: 105102   epsilon: 0.9898960600002193    steps: 307    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 572   score: 2.0   memory length: 105318   epsilon: 0.9894683800002286    steps: 216    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 573   score: 0.0   memory length: 105441   epsilon: 0.9892248400002339    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 574   score: 1.0   memory length: 105591   epsilon: 0.9889278400002404    steps: 150    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 575   score: 2.0   memory length: 105773   epsilon: 0.9885674800002482    steps: 182    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 576   score: 0.0   memory length: 105896   epsilon: 0.9883239400002535    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 577   score: 2.0   memory length: 106094   epsilon: 0.987931900000262    steps: 198    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 578   score: 0.0   memory length: 106217   epsilon: 0.9876883600002673    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 579   score: 1.0   memory length: 106367   epsilon: 0.9873913600002737    steps: 150    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 580   score: 3.0   memory length: 106634   epsilon: 0.9868627000002852    steps: 267    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 581   score: 0.0   memory length: 106756   epsilon: 0.9866211400002904    steps: 122    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 582   score: 0.0   memory length: 106879   epsilon: 0.9863776000002957    steps: 123    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 583   score: 1.0   memory length: 107030   epsilon: 0.9860786200003022    steps: 151    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 584   score: 1.0   memory length: 107181   epsilon: 0.9857796400003087    steps: 151    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 585   score: 1.0   memory length: 107349   epsilon: 0.9854470000003159    steps: 168    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 586   score: 0.0   memory length: 107471   epsilon: 0.9852054400003212    steps: 122    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 587   score: 3.0   memory length: 107696   epsilon: 0.9847599400003308    steps: 225    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 588   score: 1.0   memory length: 107847   epsilon: 0.9844609600003373    steps: 151    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 589   score: 1.0   memory length: 107997   epsilon: 0.9841639600003438    steps: 150    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 590   score: 2.0   memory length: 108177   epsilon: 0.9838075600003515    steps: 180    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 591   score: 0.0   memory length: 108299   epsilon: 0.9835660000003568    steps: 122    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 592   score: 2.0   memory length: 108497   epsilon: 0.9831739600003653    steps: 198    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 593   score: 0.0   memory length: 108620   epsilon: 0.9829304200003706    steps: 123    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 594   score: 1.0   memory length: 108789   epsilon: 0.9825958000003778    steps: 169    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 595   score: 2.0   memory length: 108986   epsilon: 0.9822057400003863    steps: 197    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 596   score: 0.0   memory length: 109108   epsilon: 0.9819641800003915    steps: 122    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 597   score: 0.0   memory length: 109231   epsilon: 0.9817206400003968    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 598   score: 3.0   memory length: 109496   epsilon: 0.9811959400004082    steps: 265    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 599   score: 0.0   memory length: 109619   epsilon: 0.9809524000004135    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 600   score: 1.0   memory length: 109769   epsilon: 0.98065540000042    steps: 150    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 601   score: 2.0   memory length: 109987   epsilon: 0.9802237600004293    steps: 218    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 602   score: 3.0   memory length: 110213   epsilon: 0.979776280000439    steps: 226    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 603   score: 4.0   memory length: 110485   epsilon: 0.9792377200004507    steps: 272    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 604   score: 1.0   memory length: 110654   epsilon: 0.978903100000458    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 605   score: 2.0   memory length: 110852   epsilon: 0.9785110600004665    steps: 198    lr: 0.0001     evaluation reward: 1.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 606   score: 0.0   memory length: 110975   epsilon: 0.9782675200004718    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 607   score: 2.0   memory length: 111173   epsilon: 0.9778754800004803    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 608   score: 2.0   memory length: 111389   epsilon: 0.9774478000004896    steps: 216    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 609   score: 1.0   memory length: 111540   epsilon: 0.9771488200004961    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 610   score: 2.0   memory length: 111740   epsilon: 0.9767528200005047    steps: 200    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 611   score: 0.0   memory length: 111863   epsilon: 0.97650928000051    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 612   score: 2.0   memory length: 112043   epsilon: 0.9761528800005177    steps: 180    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 613   score: 0.0   memory length: 112165   epsilon: 0.9759113200005229    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 614   score: 3.0   memory length: 112433   epsilon: 0.9753806800005345    steps: 268    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 615   score: 2.0   memory length: 112631   epsilon: 0.974988640000543    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 616   score: 4.0   memory length: 112910   epsilon: 0.974436220000555    steps: 279    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 617   score: 2.0   memory length: 113128   epsilon: 0.9740045800005643    steps: 218    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 618   score: 2.0   memory length: 113345   epsilon: 0.9735749200005737    steps: 217    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 619   score: 5.0   memory length: 113690   epsilon: 0.9728918200005885    steps: 345    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 620   score: 0.0   memory length: 113813   epsilon: 0.9726482800005938    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 621   score: 0.0   memory length: 113936   epsilon: 0.9724047400005991    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 622   score: 2.0   memory length: 114136   epsilon: 0.9720087400006077    steps: 200    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 623   score: 2.0   memory length: 114334   epsilon: 0.9716167000006162    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 624   score: 0.0   memory length: 114456   epsilon: 0.9713751400006214    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 625   score: 1.0   memory length: 114625   epsilon: 0.9710405200006287    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 626   score: 3.0   memory length: 114851   epsilon: 0.9705930400006384    steps: 226    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 627   score: 2.0   memory length: 115049   epsilon: 0.9702010000006469    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 628   score: 0.0   memory length: 115172   epsilon: 0.9699574600006522    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 629   score: 0.0   memory length: 115295   epsilon: 0.9697139200006575    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 630   score: 2.0   memory length: 115497   epsilon: 0.9693139600006662    steps: 202    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 631   score: 1.0   memory length: 115667   epsilon: 0.9689773600006735    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 632   score: 3.0   memory length: 115933   epsilon: 0.9684506800006849    steps: 266    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 633   score: 4.0   memory length: 116224   epsilon: 0.9678745000006974    steps: 291    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 634   score: 0.0   memory length: 116346   epsilon: 0.9676329400007027    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 635   score: 3.0   memory length: 116572   epsilon: 0.9671854600007124    steps: 226    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 636   score: 3.0   memory length: 116838   epsilon: 0.9666587800007238    steps: 266    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 637   score: 0.0   memory length: 116961   epsilon: 0.9664152400007291    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 638   score: 2.0   memory length: 117158   epsilon: 0.9660251800007376    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 639   score: 3.0   memory length: 117389   epsilon: 0.9655678000007475    steps: 231    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 640   score: 0.0   memory length: 117512   epsilon: 0.9653242600007528    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 641   score: 0.0   memory length: 117635   epsilon: 0.9650807200007581    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 642   score: 0.0   memory length: 117758   epsilon: 0.9648371800007634    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 643   score: 1.0   memory length: 117909   epsilon: 0.9645382000007698    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 644   score: 1.0   memory length: 118060   epsilon: 0.9642392200007763    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 645   score: 0.0   memory length: 118183   epsilon: 0.9639956800007816    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 646   score: 2.0   memory length: 118400   epsilon: 0.963566020000791    steps: 217    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 647   score: 1.0   memory length: 118551   epsilon: 0.9632670400007974    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 648   score: 2.0   memory length: 118750   epsilon: 0.962873020000806    steps: 199    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 649   score: 4.0   memory length: 119044   epsilon: 0.9622909000008186    steps: 294    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 650   score: 0.0   memory length: 119166   epsilon: 0.9620493400008239    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 651   score: 2.0   memory length: 119363   epsilon: 0.9616592800008323    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 652   score: 3.0   memory length: 119608   epsilon: 0.9611741800008429    steps: 245    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 653   score: 1.0   memory length: 119777   epsilon: 0.9608395600008501    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 654   score: 2.0   memory length: 119975   epsilon: 0.9604475200008586    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 655   score: 2.0   memory length: 120194   epsilon: 0.9600139000008681    steps: 219    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 656   score: 1.0   memory length: 120344   epsilon: 0.9597169000008745    steps: 150    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 657   score: 0.0   memory length: 120467   epsilon: 0.9594733600008798    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 658   score: 0.0   memory length: 120589   epsilon: 0.959231800000885    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 659   score: 3.0   memory length: 120855   epsilon: 0.9587051200008965    steps: 266    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 660   score: 0.0   memory length: 120978   epsilon: 0.9584615800009018    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 661   score: 8.0   memory length: 121310   epsilon: 0.957804220000916    steps: 332    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 662   score: 1.0   memory length: 121481   epsilon: 0.9574656400009234    steps: 171    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 663   score: 2.0   memory length: 121679   epsilon: 0.9570736000009319    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 664   score: 4.0   memory length: 121976   epsilon: 0.9564855400009447    steps: 297    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 665   score: 3.0   memory length: 122222   epsilon: 0.9559984600009552    steps: 246    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 666   score: 2.0   memory length: 122439   epsilon: 0.9555688000009646    steps: 217    lr: 0.0001     evaluation reward: 1.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 667   score: 0.0   memory length: 122562   epsilon: 0.9553252600009698    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 668   score: 0.0   memory length: 122685   epsilon: 0.9550817200009751    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 669   score: 1.0   memory length: 122836   epsilon: 0.9547827400009816    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 670   score: 5.0   memory length: 123178   epsilon: 0.9541055800009963    steps: 342    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 671   score: 2.0   memory length: 123398   epsilon: 0.9536699800010058    steps: 220    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 672   score: 4.0   memory length: 123677   epsilon: 0.9531175600010178    steps: 279    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 673   score: 1.0   memory length: 123845   epsilon: 0.952784920001025    steps: 168    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 674   score: 0.0   memory length: 123968   epsilon: 0.9525413800010303    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 675   score: 2.0   memory length: 124166   epsilon: 0.9521493400010388    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 676   score: 3.0   memory length: 124375   epsilon: 0.9517355200010478    steps: 209    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 677   score: 3.0   memory length: 124621   epsilon: 0.9512484400010583    steps: 246    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 678   score: 0.0   memory length: 124743   epsilon: 0.9510068800010636    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 679   score: 1.0   memory length: 124913   epsilon: 0.9506702800010709    steps: 170    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 680   score: 2.0   memory length: 125092   epsilon: 0.9503158600010786    steps: 179    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 681   score: 2.0   memory length: 125308   epsilon: 0.9498881800010879    steps: 216    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 682   score: 2.0   memory length: 125506   epsilon: 0.9494961400010964    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 683   score: 0.0   memory length: 125629   epsilon: 0.9492526000011017    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 684   score: 4.0   memory length: 125910   epsilon: 0.9486962200011138    steps: 281    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 685   score: 1.0   memory length: 126079   epsilon: 0.948361600001121    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 686   score: 2.0   memory length: 126280   epsilon: 0.9479636200011297    steps: 201    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 687   score: 1.0   memory length: 126448   epsilon: 0.9476309800011369    steps: 168    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 688   score: 0.0   memory length: 126571   epsilon: 0.9473874400011422    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 689   score: 2.0   memory length: 126772   epsilon: 0.9469894600011508    steps: 201    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 690   score: 1.0   memory length: 126944   epsilon: 0.9466489000011582    steps: 172    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 691   score: 1.0   memory length: 127095   epsilon: 0.9463499200011647    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 692   score: 2.0   memory length: 127311   epsilon: 0.945922240001174    steps: 216    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 693   score: 2.0   memory length: 127527   epsilon: 0.9454945600011833    steps: 216    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 694   score: 2.0   memory length: 127742   epsilon: 0.9450688600011925    steps: 215    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 695   score: 0.0   memory length: 127865   epsilon: 0.9448253200011978    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 696   score: 2.0   memory length: 128065   epsilon: 0.9444293200012064    steps: 200    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 697   score: 0.0   memory length: 128188   epsilon: 0.9441857800012117    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 698   score: 3.0   memory length: 128456   epsilon: 0.9436551400012232    steps: 268    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 699   score: 1.0   memory length: 128607   epsilon: 0.9433561600012297    steps: 151    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 700   score: 0.0   memory length: 128730   epsilon: 0.943112620001235    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 701   score: 1.0   memory length: 128899   epsilon: 0.9427780000012422    steps: 169    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 702   score: 0.0   memory length: 129021   epsilon: 0.9425364400012475    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 703   score: 0.0   memory length: 129144   epsilon: 0.9422929000012528    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 704   score: 2.0   memory length: 129361   epsilon: 0.9418632400012621    steps: 217    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 705   score: 1.0   memory length: 129512   epsilon: 0.9415642600012686    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 706   score: 0.0   memory length: 129635   epsilon: 0.9413207200012739    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 707   score: 1.0   memory length: 129804   epsilon: 0.9409861000012811    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 708   score: 3.0   memory length: 130052   epsilon: 0.9404950600012918    steps: 248    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 709   score: 3.0   memory length: 130316   epsilon: 0.9399723400013031    steps: 264    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 710   score: 0.0   memory length: 130438   epsilon: 0.9397307800013084    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 711   score: 1.0   memory length: 130588   epsilon: 0.9394337800013148    steps: 150    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 712   score: 3.0   memory length: 130855   epsilon: 0.9389051200013263    steps: 267    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 713   score: 1.0   memory length: 131006   epsilon: 0.9386061400013328    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 714   score: 0.0   memory length: 131129   epsilon: 0.9383626000013381    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 715   score: 1.0   memory length: 131299   epsilon: 0.9380260000013454    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 716   score: 0.0   memory length: 131422   epsilon: 0.9377824600013507    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 717   score: 0.0   memory length: 131545   epsilon: 0.937538920001356    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 718   score: 1.0   memory length: 131714   epsilon: 0.9372043000013632    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 719   score: 3.0   memory length: 131960   epsilon: 0.9367172200013738    steps: 246    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 720   score: 3.0   memory length: 132210   epsilon: 0.9362222200013846    steps: 250    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 721   score: 0.0   memory length: 132332   epsilon: 0.9359806600013898    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 722   score: 2.0   memory length: 132529   epsilon: 0.9355906000013983    steps: 197    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 723   score: 2.0   memory length: 132727   epsilon: 0.9351985600014068    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 724   score: 0.0   memory length: 132850   epsilon: 0.9349550200014121    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 725   score: 1.0   memory length: 133001   epsilon: 0.9346560400014186    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 726   score: 3.0   memory length: 133247   epsilon: 0.9341689600014291    steps: 246    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 727   score: 4.0   memory length: 133510   epsilon: 0.9336482200014404    steps: 263    lr: 0.0001     evaluation reward: 1.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 728   score: 0.0   memory length: 133633   epsilon: 0.9334046800014457    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 729   score: 0.0   memory length: 133756   epsilon: 0.933161140001451    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 730   score: 1.0   memory length: 133906   epsilon: 0.9328641400014575    steps: 150    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 731   score: 0.0   memory length: 134029   epsilon: 0.9326206000014627    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 732   score: 3.0   memory length: 134281   epsilon: 0.9321216400014736    steps: 252    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 733   score: 0.0   memory length: 134404   epsilon: 0.9318781000014789    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 734   score: 2.0   memory length: 134620   epsilon: 0.9314504200014881    steps: 216    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 735   score: 1.0   memory length: 134788   epsilon: 0.9311177800014954    steps: 168    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 736   score: 3.0   memory length: 135034   epsilon: 0.9306307000015059    steps: 246    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 737   score: 1.0   memory length: 135204   epsilon: 0.9302941000015132    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 738   score: 2.0   memory length: 135404   epsilon: 0.9298981000015218    steps: 200    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 739   score: 3.0   memory length: 135649   epsilon: 0.9294130000015324    steps: 245    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 740   score: 1.0   memory length: 135817   epsilon: 0.9290803600015396    steps: 168    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 741   score: 1.0   memory length: 135968   epsilon: 0.9287813800015461    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 742   score: 2.0   memory length: 136168   epsilon: 0.9283853800015547    steps: 200    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 743   score: 2.0   memory length: 136367   epsilon: 0.9279913600015632    steps: 199    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 744   score: 1.0   memory length: 136517   epsilon: 0.9276943600015697    steps: 150    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 745   score: 0.0   memory length: 136640   epsilon: 0.927450820001575    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 746   score: 0.0   memory length: 136762   epsilon: 0.9272092600015802    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 747   score: 3.0   memory length: 137028   epsilon: 0.9266825800015916    steps: 266    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 748   score: 1.0   memory length: 137179   epsilon: 0.9263836000015981    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 749   score: 2.0   memory length: 137377   epsilon: 0.9259915600016067    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 750   score: 1.0   memory length: 137546   epsilon: 0.9256569400016139    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 751   score: 2.0   memory length: 137743   epsilon: 0.9252668800016224    steps: 197    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 752   score: 1.0   memory length: 137894   epsilon: 0.9249679000016289    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 753   score: 2.0   memory length: 138115   epsilon: 0.9245303200016384    steps: 221    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 754   score: 0.0   memory length: 138237   epsilon: 0.9242887600016436    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 755   score: 1.0   memory length: 138388   epsilon: 0.9239897800016501    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 756   score: 3.0   memory length: 138641   epsilon: 0.923488840001661    steps: 253    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 757   score: 0.0   memory length: 138764   epsilon: 0.9232453000016663    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 758   score: 3.0   memory length: 139014   epsilon: 0.922750300001677    steps: 250    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 759   score: 0.0   memory length: 139137   epsilon: 0.9225067600016823    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 760   score: 1.0   memory length: 139306   epsilon: 0.9221721400016896    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 761   score: 3.0   memory length: 139535   epsilon: 0.9217187200016994    steps: 229    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 762   score: 4.0   memory length: 139811   epsilon: 0.9211722400017113    steps: 276    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 763   score: 2.0   memory length: 140008   epsilon: 0.9207821800017197    steps: 197    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 764   score: 2.0   memory length: 140206   epsilon: 0.9203901400017283    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 765   score: 2.0   memory length: 140422   epsilon: 0.9199624600017375    steps: 216    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 766   score: 2.0   memory length: 140622   epsilon: 0.9195664600017461    steps: 200    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 767   score: 0.0   memory length: 140745   epsilon: 0.9193229200017514    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 768   score: 0.0   memory length: 140868   epsilon: 0.9190793800017567    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 769   score: 0.0   memory length: 140991   epsilon: 0.918835840001762    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 770   score: 5.0   memory length: 141332   epsilon: 0.9181606600017767    steps: 341    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 771   score: 0.0   memory length: 141454   epsilon: 0.9179191000017819    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 772   score: 3.0   memory length: 141718   epsilon: 0.9173963800017932    steps: 264    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 773   score: 1.0   memory length: 141887   epsilon: 0.9170617600018005    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 774   score: 0.0   memory length: 142009   epsilon: 0.9168202000018058    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 775   score: 2.0   memory length: 142229   epsilon: 0.9163846000018152    steps: 220    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 776   score: 0.0   memory length: 142352   epsilon: 0.9161410600018205    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 777   score: 2.0   memory length: 142552   epsilon: 0.9157450600018291    steps: 200    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 778   score: 3.0   memory length: 142783   epsilon: 0.915287680001839    steps: 231    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 779   score: 2.0   memory length: 143000   epsilon: 0.9148580200018483    steps: 217    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 780   score: 2.0   memory length: 143198   epsilon: 0.9144659800018569    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 781   score: 2.0   memory length: 143414   epsilon: 0.9140383000018661    steps: 216    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 782   score: 0.0   memory length: 143537   epsilon: 0.9137947600018714    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 783   score: 0.0   memory length: 143659   epsilon: 0.9135532000018767    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 784   score: 0.0   memory length: 143782   epsilon: 0.913309660001882    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 785   score: 1.0   memory length: 143954   epsilon: 0.9129691000018894    steps: 172    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 786   score: 2.0   memory length: 144151   epsilon: 0.9125790400018978    steps: 197    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 787   score: 0.0   memory length: 144274   epsilon: 0.9123355000019031    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 788   score: 1.0   memory length: 144424   epsilon: 0.9120385000019096    steps: 150    lr: 0.0001     evaluation reward: 1.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 789   score: 1.0   memory length: 144593   epsilon: 0.9117038800019168    steps: 169    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 790   score: 2.0   memory length: 144811   epsilon: 0.9112722400019262    steps: 218    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 791   score: 6.0   memory length: 145174   epsilon: 0.9105535000019418    steps: 363    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 792   score: 1.0   memory length: 145324   epsilon: 0.9102565000019482    steps: 150    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 793   score: 4.0   memory length: 145612   epsilon: 0.9096862600019606    steps: 288    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 794   score: 1.0   memory length: 145784   epsilon: 0.909345700001968    steps: 172    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 795   score: 1.0   memory length: 145952   epsilon: 0.9090130600019752    steps: 168    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 796   score: 2.0   memory length: 146149   epsilon: 0.9086230000019837    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 797   score: 2.0   memory length: 146367   epsilon: 0.9081913600019931    steps: 218    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 798   score: 1.0   memory length: 146536   epsilon: 0.9078567400020003    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 799   score: 1.0   memory length: 146706   epsilon: 0.9075201400020076    steps: 170    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 800   score: 2.0   memory length: 146904   epsilon: 0.9071281000020162    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 801   score: 2.0   memory length: 147105   epsilon: 0.9067301200020248    steps: 201    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 802   score: 1.0   memory length: 147274   epsilon: 0.9063955000020321    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 803   score: 2.0   memory length: 147472   epsilon: 0.9060034600020406    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 804   score: 1.0   memory length: 147640   epsilon: 0.9056708200020478    steps: 168    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 805   score: 0.0   memory length: 147762   epsilon: 0.905429260002053    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 806   score: 4.0   memory length: 148058   epsilon: 0.9048431800020658    steps: 296    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 807   score: 3.0   memory length: 148308   epsilon: 0.9043481800020765    steps: 250    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 808   score: 1.0   memory length: 148458   epsilon: 0.904051180002083    steps: 150    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 809   score: 0.0   memory length: 148580   epsilon: 0.9038096200020882    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 810   score: 2.0   memory length: 148778   epsilon: 0.9034175800020967    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 811   score: 2.0   memory length: 148977   epsilon: 0.9030235600021053    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 812   score: 1.0   memory length: 149128   epsilon: 0.9027245800021118    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 813   score: 1.0   memory length: 149279   epsilon: 0.9024256000021182    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 814   score: 2.0   memory length: 149477   epsilon: 0.9020335600021268    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 815   score: 2.0   memory length: 149674   epsilon: 0.9016435000021352    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 816   score: 4.0   memory length: 149990   epsilon: 0.9010178200021488    steps: 316    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 817   score: 4.0   memory length: 150287   epsilon: 0.9004297600021616    steps: 297    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 818   score: 3.0   memory length: 150534   epsilon: 0.8999407000021722    steps: 247    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 819   score: 0.0   memory length: 150657   epsilon: 0.8996971600021775    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 820   score: 2.0   memory length: 150873   epsilon: 0.8992694800021868    steps: 216    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 821   score: 2.0   memory length: 151071   epsilon: 0.8988774400021953    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 822   score: 3.0   memory length: 151300   epsilon: 0.8984240200022051    steps: 229    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 823   score: 2.0   memory length: 151518   epsilon: 0.8979923800022145    steps: 218    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 824   score: 5.0   memory length: 151855   epsilon: 0.897325120002229    steps: 337    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 825   score: 3.0   memory length: 152120   epsilon: 0.8968004200022404    steps: 265    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 826   score: 0.0   memory length: 152243   epsilon: 0.8965568800022456    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 827   score: 3.0   memory length: 152468   epsilon: 0.8961113800022553    steps: 225    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 828   score: 3.0   memory length: 152713   epsilon: 0.8956262800022659    steps: 245    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 829   score: 3.0   memory length: 152959   epsilon: 0.8951392000022764    steps: 246    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 830   score: 4.0   memory length: 153258   epsilon: 0.8945471800022893    steps: 299    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 831   score: 2.0   memory length: 153457   epsilon: 0.8941531600022978    steps: 199    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 832   score: 3.0   memory length: 153722   epsilon: 0.8936284600023092    steps: 265    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 833   score: 2.0   memory length: 153920   epsilon: 0.8932364200023177    steps: 198    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 834   score: 2.0   memory length: 154138   epsilon: 0.8928047800023271    steps: 218    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 835   score: 2.0   memory length: 154357   epsilon: 0.8923711600023365    steps: 219    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 836   score: 2.0   memory length: 154573   epsilon: 0.8919434800023458    steps: 216    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 837   score: 2.0   memory length: 154771   epsilon: 0.8915514400023543    steps: 198    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 838   score: 0.0   memory length: 154894   epsilon: 0.8913079000023596    steps: 123    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 839   score: 3.0   memory length: 155139   epsilon: 0.8908228000023701    steps: 245    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 840   score: 0.0   memory length: 155261   epsilon: 0.8905812400023754    steps: 122    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 841   score: 1.0   memory length: 155433   epsilon: 0.8902406800023828    steps: 172    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 842   score: 6.0   memory length: 155800   epsilon: 0.8895140200023985    steps: 367    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 843   score: 9.0   memory length: 156182   epsilon: 0.888757660002415    steps: 382    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 844   score: 2.0   memory length: 156401   epsilon: 0.8883240400024244    steps: 219    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 845   score: 1.0   memory length: 156570   epsilon: 0.8879894200024316    steps: 169    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 846   score: 1.0   memory length: 156720   epsilon: 0.8876924200024381    steps: 150    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 847   score: 1.0   memory length: 156890   epsilon: 0.8873558200024454    steps: 170    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 848   score: 3.0   memory length: 157136   epsilon: 0.886868740002456    steps: 246    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 849   score: 3.0   memory length: 157362   epsilon: 0.8864212600024657    steps: 226    lr: 0.0001     evaluation reward: 1.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 850   score: 1.0   memory length: 157513   epsilon: 0.8861222800024722    steps: 151    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 851   score: 2.0   memory length: 157734   epsilon: 0.8856847000024817    steps: 221    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 852   score: 4.0   memory length: 158012   epsilon: 0.8851342600024936    steps: 278    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 853   score: 7.0   memory length: 158438   epsilon: 0.8842907800025119    steps: 426    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 854   score: 2.0   memory length: 158638   epsilon: 0.8838947800025205    steps: 200    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 855   score: 1.0   memory length: 158807   epsilon: 0.8835601600025278    steps: 169    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 856   score: 2.0   memory length: 159009   epsilon: 0.8831602000025365    steps: 202    lr: 0.0001     evaluation reward: 1.98\n",
      "episode: 857   score: 3.0   memory length: 159253   epsilon: 0.882677080002547    steps: 244    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 858   score: 0.0   memory length: 159376   epsilon: 0.8824335400025523    steps: 123    lr: 0.0001     evaluation reward: 1.98\n",
      "episode: 859   score: 2.0   memory length: 159574   epsilon: 0.8820415000025608    steps: 198    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 860   score: 3.0   memory length: 159821   epsilon: 0.8815524400025714    steps: 247    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 861   score: 2.0   memory length: 160001   epsilon: 0.8811960400025791    steps: 180    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 862   score: 2.0   memory length: 160217   epsilon: 0.8807683600025884    steps: 216    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 863   score: 3.0   memory length: 160443   epsilon: 0.8803208800025981    steps: 226    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 864   score: 3.0   memory length: 160689   epsilon: 0.8798338000026087    steps: 246    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 865   score: 1.0   memory length: 160840   epsilon: 0.8795348200026152    steps: 151    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 866   score: 3.0   memory length: 161087   epsilon: 0.8790457600026258    steps: 247    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 867   score: 2.0   memory length: 161284   epsilon: 0.8786557000026343    steps: 197    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 868   score: 4.0   memory length: 161581   epsilon: 0.878067640002647    steps: 297    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 869   score: 2.0   memory length: 161799   epsilon: 0.8776360000026564    steps: 218    lr: 0.0001     evaluation reward: 2.09\n",
      "episode: 870   score: 2.0   memory length: 161980   epsilon: 0.8772776200026642    steps: 181    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 871   score: 4.0   memory length: 162295   epsilon: 0.8766539200026777    steps: 315    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 872   score: 3.0   memory length: 162542   epsilon: 0.8761648600026883    steps: 247    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 873   score: 2.0   memory length: 162744   epsilon: 0.875764900002697    steps: 202    lr: 0.0001     evaluation reward: 2.11\n",
      "episode: 874   score: 1.0   memory length: 162913   epsilon: 0.8754302800027043    steps: 169    lr: 0.0001     evaluation reward: 2.12\n",
      "episode: 875   score: 3.0   memory length: 163159   epsilon: 0.8749432000027149    steps: 246    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 876   score: 4.0   memory length: 163414   epsilon: 0.8744383000027258    steps: 255    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 877   score: 2.0   memory length: 163633   epsilon: 0.8740046800027352    steps: 219    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 878   score: 1.0   memory length: 163784   epsilon: 0.8737057000027417    steps: 151    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 879   score: 3.0   memory length: 164053   epsilon: 0.8731730800027533    steps: 269    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 880   score: 2.0   memory length: 164271   epsilon: 0.8727414400027627    steps: 218    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 881   score: 4.0   memory length: 164569   epsilon: 0.8721514000027755    steps: 298    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 882   score: 1.0   memory length: 164719   epsilon: 0.8718544000027819    steps: 150    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 883   score: 0.0   memory length: 164842   epsilon: 0.8716108600027872    steps: 123    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 884   score: 4.0   memory length: 165103   epsilon: 0.8710940800027984    steps: 261    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 885   score: 2.0   memory length: 165283   epsilon: 0.8707376800028062    steps: 180    lr: 0.0001     evaluation reward: 2.24\n",
      "episode: 886   score: 3.0   memory length: 165492   epsilon: 0.8703238600028151    steps: 209    lr: 0.0001     evaluation reward: 2.25\n",
      "episode: 887   score: 3.0   memory length: 165738   epsilon: 0.8698367800028257    steps: 246    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 888   score: 1.0   memory length: 165910   epsilon: 0.8694962200028331    steps: 172    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 889   score: 0.0   memory length: 166033   epsilon: 0.8692526800028384    steps: 123    lr: 0.0001     evaluation reward: 2.27\n",
      "episode: 890   score: 6.0   memory length: 166406   epsilon: 0.8685141400028544    steps: 373    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 891   score: 1.0   memory length: 166578   epsilon: 0.8681735800028618    steps: 172    lr: 0.0001     evaluation reward: 2.26\n",
      "episode: 892   score: 2.0   memory length: 166775   epsilon: 0.8677835200028703    steps: 197    lr: 0.0001     evaluation reward: 2.27\n",
      "episode: 893   score: 6.0   memory length: 167116   epsilon: 0.867108340002885    steps: 341    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 894   score: 2.0   memory length: 167315   epsilon: 0.8667143200028935    steps: 199    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 895   score: 1.0   memory length: 167484   epsilon: 0.8663797000029008    steps: 169    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 896   score: 0.0   memory length: 167607   epsilon: 0.866136160002906    steps: 123    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 897   score: 3.0   memory length: 167876   epsilon: 0.8656035400029176    steps: 269    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 898   score: 6.0   memory length: 168224   epsilon: 0.8649145000029326    steps: 348    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 899   score: 4.0   memory length: 168521   epsilon: 0.8643264400029453    steps: 297    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 900   score: 1.0   memory length: 168673   epsilon: 0.8640254800029519    steps: 152    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 901   score: 3.0   memory length: 168920   epsilon: 0.8635364200029625    steps: 247    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 902   score: 3.0   memory length: 169129   epsilon: 0.8631226000029715    steps: 209    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 903   score: 2.0   memory length: 169347   epsilon: 0.8626909600029808    steps: 218    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 904   score: 1.0   memory length: 169498   epsilon: 0.8623919800029873    steps: 151    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 905   score: 0.0   memory length: 169621   epsilon: 0.8621484400029926    steps: 123    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 906   score: 3.0   memory length: 169850   epsilon: 0.8616950200030025    steps: 229    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 907   score: 1.0   memory length: 170021   epsilon: 0.8613564400030098    steps: 171    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 908   score: 2.0   memory length: 170240   epsilon: 0.8609228200030192    steps: 219    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 909   score: 3.0   memory length: 170487   epsilon: 0.8604337600030298    steps: 247    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 910   score: 1.0   memory length: 170655   epsilon: 0.8601011200030371    steps: 168    lr: 0.0001     evaluation reward: 2.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 911   score: 0.0   memory length: 170778   epsilon: 0.8598575800030424    steps: 123    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 912   score: 2.0   memory length: 170975   epsilon: 0.8594675200030508    steps: 197    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 913   score: 1.0   memory length: 171126   epsilon: 0.8591685400030573    steps: 151    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 914   score: 2.0   memory length: 171344   epsilon: 0.8587369000030667    steps: 218    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 915   score: 4.0   memory length: 171640   epsilon: 0.8581508200030794    steps: 296    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 916   score: 3.0   memory length: 171886   epsilon: 0.85766374000309    steps: 246    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 917   score: 4.0   memory length: 172145   epsilon: 0.8571509200031011    steps: 259    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 918   score: 5.0   memory length: 172484   epsilon: 0.8564797000031157    steps: 339    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 919   score: 0.0   memory length: 172607   epsilon: 0.856236160003121    steps: 123    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 920   score: 3.0   memory length: 172833   epsilon: 0.8557886800031307    steps: 226    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 921   score: 4.0   memory length: 173129   epsilon: 0.8552026000031434    steps: 296    lr: 0.0001     evaluation reward: 2.44\n",
      "episode: 922   score: 2.0   memory length: 173326   epsilon: 0.8548125400031519    steps: 197    lr: 0.0001     evaluation reward: 2.43\n",
      "episode: 923   score: 0.0   memory length: 173449   epsilon: 0.8545690000031572    steps: 123    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 924   score: 2.0   memory length: 173667   epsilon: 0.8541373600031665    steps: 218    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 925   score: 3.0   memory length: 173919   epsilon: 0.8536384000031774    steps: 252    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 926   score: 3.0   memory length: 174169   epsilon: 0.8531434000031881    steps: 250    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 927   score: 1.0   memory length: 174337   epsilon: 0.8528107600031953    steps: 168    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 928   score: 0.0   memory length: 174460   epsilon: 0.8525672200032006    steps: 123    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 929   score: 1.0   memory length: 174629   epsilon: 0.8522326000032079    steps: 169    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 930   score: 2.0   memory length: 174827   epsilon: 0.8518405600032164    steps: 198    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 931   score: 2.0   memory length: 175010   epsilon: 0.8514782200032243    steps: 183    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 932   score: 3.0   memory length: 175240   epsilon: 0.8510228200032341    steps: 230    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 933   score: 2.0   memory length: 175438   epsilon: 0.8506307800032427    steps: 198    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 934   score: 2.0   memory length: 175636   epsilon: 0.8502387400032512    steps: 198    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 935   score: 5.0   memory length: 175961   epsilon: 0.8495952400032651    steps: 325    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 936   score: 6.0   memory length: 176326   epsilon: 0.8488725400032808    steps: 365    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 937   score: 5.0   memory length: 176657   epsilon: 0.8482171600032951    steps: 331    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 938   score: 4.0   memory length: 176951   epsilon: 0.8476350400033077    steps: 294    lr: 0.0001     evaluation reward: 2.46\n",
      "episode: 939   score: 0.0   memory length: 177073   epsilon: 0.8473934800033129    steps: 122    lr: 0.0001     evaluation reward: 2.43\n",
      "episode: 940   score: 6.0   memory length: 177433   epsilon: 0.8466806800033284    steps: 360    lr: 0.0001     evaluation reward: 2.49\n",
      "episode: 941   score: 1.0   memory length: 177602   epsilon: 0.8463460600033357    steps: 169    lr: 0.0001     evaluation reward: 2.49\n",
      "episode: 942   score: 2.0   memory length: 177800   epsilon: 0.8459540200033442    steps: 198    lr: 0.0001     evaluation reward: 2.45\n",
      "episode: 943   score: 2.0   memory length: 177980   epsilon: 0.8455976200033519    steps: 180    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 944   score: 2.0   memory length: 178198   epsilon: 0.8451659800033613    steps: 218    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 945   score: 6.0   memory length: 178590   epsilon: 0.8443898200033781    steps: 392    lr: 0.0001     evaluation reward: 2.43\n",
      "episode: 946   score: 3.0   memory length: 178818   epsilon: 0.843938380003388    steps: 228    lr: 0.0001     evaluation reward: 2.45\n",
      "episode: 947   score: 4.0   memory length: 179092   epsilon: 0.8433958600033997    steps: 274    lr: 0.0001     evaluation reward: 2.48\n",
      "episode: 948   score: 2.0   memory length: 179272   epsilon: 0.8430394600034075    steps: 180    lr: 0.0001     evaluation reward: 2.47\n",
      "episode: 949   score: 1.0   memory length: 179440   epsilon: 0.8427068200034147    steps: 168    lr: 0.0001     evaluation reward: 2.45\n",
      "episode: 950   score: 1.0   memory length: 179591   epsilon: 0.8424078400034212    steps: 151    lr: 0.0001     evaluation reward: 2.45\n",
      "episode: 951   score: 4.0   memory length: 179888   epsilon: 0.8418197800034339    steps: 297    lr: 0.0001     evaluation reward: 2.47\n",
      "episode: 952   score: 5.0   memory length: 180213   epsilon: 0.8411762800034479    steps: 325    lr: 0.0001     evaluation reward: 2.48\n",
      "episode: 953   score: 0.0   memory length: 180336   epsilon: 0.8409327400034532    steps: 123    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 954   score: 1.0   memory length: 180508   epsilon: 0.8405921800034606    steps: 172    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 955   score: 0.0   memory length: 180630   epsilon: 0.8403506200034658    steps: 122    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 956   score: 5.0   memory length: 180955   epsilon: 0.8397071200034798    steps: 325    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 957   score: 2.0   memory length: 181153   epsilon: 0.8393150800034883    steps: 198    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 958   score: 0.0   memory length: 181275   epsilon: 0.8390735200034936    steps: 122    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 959   score: 3.0   memory length: 181522   epsilon: 0.8385844600035042    steps: 247    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 960   score: 3.0   memory length: 181747   epsilon: 0.8381389600035138    steps: 225    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 961   score: 1.0   memory length: 181919   epsilon: 0.8377984000035212    steps: 172    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 962   score: 2.0   memory length: 182099   epsilon: 0.837442000003529    steps: 180    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 963   score: 2.0   memory length: 182316   epsilon: 0.8370123400035383    steps: 217    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 964   score: 1.0   memory length: 182485   epsilon: 0.8366777200035456    steps: 169    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 965   score: 2.0   memory length: 182682   epsilon: 0.836287660003554    steps: 197    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 966   score: 3.0   memory length: 182908   epsilon: 0.8358401800035637    steps: 226    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 967   score: 0.0   memory length: 183031   epsilon: 0.835596640003569    steps: 123    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 968   score: 4.0   memory length: 183299   epsilon: 0.8350660000035806    steps: 268    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 969   score: 1.0   memory length: 183471   epsilon: 0.834725440003588    steps: 172    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 970   score: 2.0   memory length: 183688   epsilon: 0.8342957800035973    steps: 217    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 971   score: 5.0   memory length: 184036   epsilon: 0.8336067400036122    steps: 348    lr: 0.0001     evaluation reward: 2.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 972   score: 0.0   memory length: 184159   epsilon: 0.8333632000036175    steps: 123    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 973   score: 2.0   memory length: 184359   epsilon: 0.8329672000036261    steps: 200    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 974   score: 3.0   memory length: 184584   epsilon: 0.8325217000036358    steps: 225    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 975   score: 1.0   memory length: 184753   epsilon: 0.832187080003643    steps: 169    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 976   score: 6.0   memory length: 185144   epsilon: 0.8314129000036599    steps: 391    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 977   score: 3.0   memory length: 185370   epsilon: 0.8309654200036696    steps: 226    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 978   score: 4.0   memory length: 185648   epsilon: 0.8304149800036815    steps: 278    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 979   score: 3.0   memory length: 185897   epsilon: 0.8299219600036922    steps: 249    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 980   score: 4.0   memory length: 186196   epsilon: 0.8293299400037051    steps: 299    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 981   score: 4.0   memory length: 186513   epsilon: 0.8287022800037187    steps: 317    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 982   score: 5.0   memory length: 186861   epsilon: 0.8280132400037337    steps: 348    lr: 0.0001     evaluation reward: 2.46\n",
      "episode: 983   score: 3.0   memory length: 187093   epsilon: 0.8275538800037436    steps: 232    lr: 0.0001     evaluation reward: 2.49\n",
      "episode: 984   score: 5.0   memory length: 187421   epsilon: 0.8269044400037577    steps: 328    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 985   score: 3.0   memory length: 187669   epsilon: 0.8264134000037684    steps: 248    lr: 0.0001     evaluation reward: 2.51\n",
      "episode: 986   score: 4.0   memory length: 187963   epsilon: 0.825831280003781    steps: 294    lr: 0.0001     evaluation reward: 2.52\n",
      "episode: 987   score: 2.0   memory length: 188161   epsilon: 0.8254392400037895    steps: 198    lr: 0.0001     evaluation reward: 2.51\n",
      "episode: 988   score: 6.0   memory length: 188553   epsilon: 0.8246630800038064    steps: 392    lr: 0.0001     evaluation reward: 2.56\n",
      "episode: 989   score: 2.0   memory length: 188771   epsilon: 0.8242314400038158    steps: 218    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 990   score: 3.0   memory length: 188997   epsilon: 0.8237839600038255    steps: 226    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 991   score: 1.0   memory length: 189167   epsilon: 0.8234473600038328    steps: 170    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 992   score: 3.0   memory length: 189401   epsilon: 0.8229840400038428    steps: 234    lr: 0.0001     evaluation reward: 2.56\n",
      "episode: 993   score: 5.0   memory length: 189704   epsilon: 0.8223841000038559    steps: 303    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 994   score: 3.0   memory length: 189933   epsilon: 0.8219306800038657    steps: 229    lr: 0.0001     evaluation reward: 2.56\n",
      "episode: 995   score: 2.0   memory length: 190134   epsilon: 0.8215327000038743    steps: 201    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 996   score: 2.0   memory length: 190332   epsilon: 0.8211406600038829    steps: 198    lr: 0.0001     evaluation reward: 2.59\n",
      "episode: 997   score: 5.0   memory length: 190654   epsilon: 0.8205031000038967    steps: 322    lr: 0.0001     evaluation reward: 2.61\n",
      "episode: 998   score: 0.0   memory length: 190777   epsilon: 0.820259560003902    steps: 123    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 999   score: 3.0   memory length: 191022   epsilon: 0.8197744600039125    steps: 245    lr: 0.0001     evaluation reward: 2.54\n",
      "episode: 1000   score: 4.0   memory length: 191319   epsilon: 0.8191864000039253    steps: 297    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 1001   score: 0.0   memory length: 191442   epsilon: 0.8189428600039306    steps: 123    lr: 0.0001     evaluation reward: 2.54\n",
      "episode: 1002   score: 1.0   memory length: 191592   epsilon: 0.818645860003937    steps: 150    lr: 0.0001     evaluation reward: 2.52\n",
      "episode: 1003   score: 3.0   memory length: 191835   epsilon: 0.8181647200039475    steps: 243    lr: 0.0001     evaluation reward: 2.53\n",
      "episode: 1004   score: 3.0   memory length: 192047   epsilon: 0.8177449600039566    steps: 212    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 1005   score: 8.0   memory length: 192496   epsilon: 0.8168559400039759    steps: 449    lr: 0.0001     evaluation reward: 2.63\n",
      "episode: 1006   score: 1.0   memory length: 192647   epsilon: 0.8165569600039824    steps: 151    lr: 0.0001     evaluation reward: 2.61\n",
      "episode: 1007   score: 2.0   memory length: 192864   epsilon: 0.8161273000039917    steps: 217    lr: 0.0001     evaluation reward: 2.62\n",
      "episode: 1008   score: 3.0   memory length: 193091   epsilon: 0.8156778400040015    steps: 227    lr: 0.0001     evaluation reward: 2.63\n",
      "episode: 1009   score: 0.0   memory length: 193213   epsilon: 0.8154362800040067    steps: 122    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 1010   score: 3.0   memory length: 193444   epsilon: 0.8149789000040166    steps: 231    lr: 0.0001     evaluation reward: 2.62\n",
      "episode: 1011   score: 4.0   memory length: 193705   epsilon: 0.8144621200040278    steps: 261    lr: 0.0001     evaluation reward: 2.66\n",
      "episode: 1012   score: 2.0   memory length: 193903   epsilon: 0.8140700800040364    steps: 198    lr: 0.0001     evaluation reward: 2.66\n",
      "episode: 1013   score: 4.0   memory length: 194144   epsilon: 0.8135929000040467    steps: 241    lr: 0.0001     evaluation reward: 2.69\n",
      "episode: 1014   score: 3.0   memory length: 194373   epsilon: 0.8131394800040566    steps: 229    lr: 0.0001     evaluation reward: 2.7\n",
      "episode: 1015   score: 3.0   memory length: 194640   epsilon: 0.812610820004068    steps: 267    lr: 0.0001     evaluation reward: 2.69\n",
      "episode: 1016   score: 3.0   memory length: 194867   epsilon: 0.8121613600040778    steps: 227    lr: 0.0001     evaluation reward: 2.69\n",
      "episode: 1017   score: 3.0   memory length: 195136   epsilon: 0.8116287400040894    steps: 269    lr: 0.0001     evaluation reward: 2.68\n",
      "episode: 1018   score: 3.0   memory length: 195399   epsilon: 0.8111080000041007    steps: 263    lr: 0.0001     evaluation reward: 2.66\n",
      "episode: 1019   score: 4.0   memory length: 195657   epsilon: 0.8105971600041117    steps: 258    lr: 0.0001     evaluation reward: 2.7\n",
      "episode: 1020   score: 2.0   memory length: 195875   epsilon: 0.8101655200041211    steps: 218    lr: 0.0001     evaluation reward: 2.69\n",
      "episode: 1021   score: 5.0   memory length: 196203   epsilon: 0.8095160800041352    steps: 328    lr: 0.0001     evaluation reward: 2.7\n",
      "episode: 1022   score: 2.0   memory length: 196401   epsilon: 0.8091240400041437    steps: 198    lr: 0.0001     evaluation reward: 2.7\n",
      "episode: 1023   score: 3.0   memory length: 196645   epsilon: 0.8086409200041542    steps: 244    lr: 0.0001     evaluation reward: 2.73\n",
      "episode: 1024   score: 3.0   memory length: 196871   epsilon: 0.8081934400041639    steps: 226    lr: 0.0001     evaluation reward: 2.74\n",
      "episode: 1025   score: 2.0   memory length: 197069   epsilon: 0.8078014000041724    steps: 198    lr: 0.0001     evaluation reward: 2.73\n",
      "episode: 1026   score: 2.0   memory length: 197269   epsilon: 0.807405400004181    steps: 200    lr: 0.0001     evaluation reward: 2.72\n",
      "episode: 1027   score: 3.0   memory length: 197494   epsilon: 0.8069599000041907    steps: 225    lr: 0.0001     evaluation reward: 2.74\n",
      "episode: 1028   score: 2.0   memory length: 197692   epsilon: 0.8065678600041992    steps: 198    lr: 0.0001     evaluation reward: 2.76\n",
      "episode: 1029   score: 7.0   memory length: 198088   epsilon: 0.8057837800042162    steps: 396    lr: 0.0001     evaluation reward: 2.82\n",
      "episode: 1030   score: 2.0   memory length: 198288   epsilon: 0.8053877800042248    steps: 200    lr: 0.0001     evaluation reward: 2.82\n",
      "episode: 1031   score: 2.0   memory length: 198486   epsilon: 0.8049957400042334    steps: 198    lr: 0.0001     evaluation reward: 2.82\n",
      "episode: 1032   score: 2.0   memory length: 198688   epsilon: 0.804595780004242    steps: 202    lr: 0.0001     evaluation reward: 2.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1033   score: 4.0   memory length: 199004   epsilon: 0.8039701000042556    steps: 316    lr: 0.0001     evaluation reward: 2.83\n",
      "episode: 1034   score: 4.0   memory length: 199284   epsilon: 0.8034157000042677    steps: 280    lr: 0.0001     evaluation reward: 2.85\n",
      "episode: 1035   score: 4.0   memory length: 199541   epsilon: 0.8029068400042787    steps: 257    lr: 0.0001     evaluation reward: 2.84\n",
      "episode: 1036   score: 4.0   memory length: 199801   epsilon: 0.8023920400042899    steps: 260    lr: 0.0001     evaluation reward: 2.82\n",
      "episode: 1037   score: 1.0   memory length: 199970   epsilon: 0.8020574200042971    steps: 169    lr: 0.0001     evaluation reward: 2.78\n",
      "episode: 1038   score: 3.0   memory length: 200199   epsilon: 0.801604000004307    steps: 229    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1039   score: 2.0   memory length: 200381   epsilon: 0.8012436400043148    steps: 182    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1040   score: 4.0   memory length: 200678   epsilon: 0.8006555800043276    steps: 297    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1041   score: 0.0   memory length: 200800   epsilon: 0.8004140200043328    steps: 122    lr: 4e-05     evaluation reward: 2.76\n",
      "episode: 1042   score: 10.0   memory length: 201249   epsilon: 0.7995250000043521    steps: 449    lr: 4e-05     evaluation reward: 2.84\n",
      "episode: 1043   score: 6.0   memory length: 201623   epsilon: 0.7987844800043682    steps: 374    lr: 4e-05     evaluation reward: 2.88\n",
      "episode: 1044   score: 2.0   memory length: 201821   epsilon: 0.7983924400043767    steps: 198    lr: 4e-05     evaluation reward: 2.88\n",
      "episode: 1045   score: 4.0   memory length: 202101   epsilon: 0.7978380400043887    steps: 280    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1046   score: 2.0   memory length: 202281   epsilon: 0.7974816400043965    steps: 180    lr: 4e-05     evaluation reward: 2.85\n",
      "episode: 1047   score: 4.0   memory length: 202557   epsilon: 0.7969351600044083    steps: 276    lr: 4e-05     evaluation reward: 2.85\n",
      "episode: 1048   score: 2.0   memory length: 202756   epsilon: 0.7965411400044169    steps: 199    lr: 4e-05     evaluation reward: 2.85\n",
      "episode: 1049   score: 4.0   memory length: 203031   epsilon: 0.7959966400044287    steps: 275    lr: 4e-05     evaluation reward: 2.88\n",
      "episode: 1050   score: 3.0   memory length: 203296   epsilon: 0.7954719400044401    steps: 265    lr: 4e-05     evaluation reward: 2.9\n",
      "episode: 1051   score: 3.0   memory length: 203545   epsilon: 0.7949789200044508    steps: 249    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1052   score: 4.0   memory length: 203820   epsilon: 0.7944344200044626    steps: 275    lr: 4e-05     evaluation reward: 2.88\n",
      "episode: 1053   score: 5.0   memory length: 204109   epsilon: 0.793862200004475    steps: 289    lr: 4e-05     evaluation reward: 2.93\n",
      "episode: 1054   score: 1.0   memory length: 204262   epsilon: 0.7935592600044816    steps: 153    lr: 4e-05     evaluation reward: 2.93\n",
      "episode: 1055   score: 6.0   memory length: 204621   epsilon: 0.7928484400044971    steps: 359    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1056   score: 5.0   memory length: 204969   epsilon: 0.792159400004512    steps: 348    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1057   score: 3.0   memory length: 205218   epsilon: 0.7916663800045227    steps: 249    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1058   score: 3.0   memory length: 205465   epsilon: 0.7911773200045333    steps: 247    lr: 4e-05     evaluation reward: 3.03\n",
      "episode: 1059   score: 3.0   memory length: 205695   epsilon: 0.7907219200045432    steps: 230    lr: 4e-05     evaluation reward: 3.03\n",
      "episode: 1060   score: 2.0   memory length: 205893   epsilon: 0.7903298800045517    steps: 198    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1061   score: 2.0   memory length: 206092   epsilon: 0.7899358600045603    steps: 199    lr: 4e-05     evaluation reward: 3.03\n",
      "episode: 1062   score: 3.0   memory length: 206344   epsilon: 0.7894369000045711    steps: 252    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1063   score: 2.0   memory length: 206526   epsilon: 0.7890765400045789    steps: 182    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1064   score: 3.0   memory length: 206753   epsilon: 0.7886270800045887    steps: 227    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1065   score: 3.0   memory length: 206978   epsilon: 0.7881815800045984    steps: 225    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1066   score: 7.0   memory length: 207371   epsilon: 0.7874034400046153    steps: 393    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1067   score: 0.0   memory length: 207494   epsilon: 0.7871599000046205    steps: 123    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1068   score: 1.0   memory length: 207645   epsilon: 0.786860920004627    steps: 151    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1069   score: 6.0   memory length: 208018   epsilon: 0.7861223800046431    steps: 373    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1070   score: 5.0   memory length: 208343   epsilon: 0.785478880004657    steps: 325    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1071   score: 7.0   memory length: 208754   epsilon: 0.7846651000046747    steps: 411    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1072   score: 6.0   memory length: 209111   epsilon: 0.78395824000469    steps: 357    lr: 4e-05     evaluation reward: 3.24\n",
      "episode: 1073   score: 2.0   memory length: 209313   epsilon: 0.7835582800046987    steps: 202    lr: 4e-05     evaluation reward: 3.24\n",
      "episode: 1074   score: 6.0   memory length: 209619   epsilon: 0.7829524000047119    steps: 306    lr: 4e-05     evaluation reward: 3.27\n",
      "episode: 1075   score: 4.0   memory length: 209897   epsilon: 0.7824019600047238    steps: 278    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1076   score: 2.0   memory length: 210095   epsilon: 0.7820099200047324    steps: 198    lr: 4e-05     evaluation reward: 3.26\n",
      "episode: 1077   score: 7.0   memory length: 210489   epsilon: 0.7812298000047493    steps: 394    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1078   score: 4.0   memory length: 210766   epsilon: 0.7806813400047612    steps: 277    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1079   score: 4.0   memory length: 211043   epsilon: 0.7801328800047731    steps: 277    lr: 4e-05     evaluation reward: 3.31\n",
      "episode: 1080   score: 1.0   memory length: 211214   epsilon: 0.7797943000047804    steps: 171    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1081   score: 0.0   memory length: 211337   epsilon: 0.7795507600047857    steps: 123    lr: 4e-05     evaluation reward: 3.24\n",
      "episode: 1082   score: 4.0   memory length: 211612   epsilon: 0.7790062600047976    steps: 275    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1083   score: 8.0   memory length: 212065   epsilon: 0.778109320004817    steps: 453    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1084   score: 3.0   memory length: 212332   epsilon: 0.7775806600048285    steps: 267    lr: 4e-05     evaluation reward: 3.26\n",
      "episode: 1085   score: 3.0   memory length: 212562   epsilon: 0.7771252600048384    steps: 230    lr: 4e-05     evaluation reward: 3.26\n",
      "episode: 1086   score: 3.0   memory length: 212772   epsilon: 0.7767094600048474    steps: 210    lr: 4e-05     evaluation reward: 3.25\n",
      "episode: 1087   score: 5.0   memory length: 213079   epsilon: 0.7761016000048606    steps: 307    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1088   score: 1.0   memory length: 213250   epsilon: 0.775763020004868    steps: 171    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1089   score: 4.0   memory length: 213545   epsilon: 0.7751789200048806    steps: 295    lr: 4e-05     evaluation reward: 3.25\n",
      "episode: 1090   score: 5.0   memory length: 213867   epsilon: 0.7745413600048945    steps: 322    lr: 4e-05     evaluation reward: 3.27\n",
      "episode: 1091   score: 1.0   memory length: 214017   epsilon: 0.7742443600049009    steps: 150    lr: 4e-05     evaluation reward: 3.27\n",
      "episode: 1092   score: 5.0   memory length: 214362   epsilon: 0.7735612600049158    steps: 345    lr: 4e-05     evaluation reward: 3.29\n",
      "episode: 1093   score: 3.0   memory length: 214629   epsilon: 0.7730326000049272    steps: 267    lr: 4e-05     evaluation reward: 3.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1094   score: 4.0   memory length: 214924   epsilon: 0.7724485000049399    steps: 295    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1095   score: 5.0   memory length: 215266   epsilon: 0.7717713400049546    steps: 342    lr: 4e-05     evaluation reward: 3.31\n",
      "episode: 1096   score: 7.0   memory length: 215712   epsilon: 0.7708882600049738    steps: 446    lr: 4e-05     evaluation reward: 3.36\n",
      "episode: 1097   score: 4.0   memory length: 216006   epsilon: 0.7703061400049864    steps: 294    lr: 4e-05     evaluation reward: 3.35\n",
      "episode: 1098   score: 8.0   memory length: 216463   epsilon: 0.7694012800050061    steps: 457    lr: 4e-05     evaluation reward: 3.43\n",
      "episode: 1099   score: 3.0   memory length: 216713   epsilon: 0.7689062800050168    steps: 250    lr: 4e-05     evaluation reward: 3.43\n",
      "episode: 1100   score: 2.0   memory length: 216929   epsilon: 0.7684786000050261    steps: 216    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1101   score: 4.0   memory length: 217203   epsilon: 0.7679360800050379    steps: 274    lr: 4e-05     evaluation reward: 3.45\n",
      "episode: 1102   score: 4.0   memory length: 217497   epsilon: 0.7673539600050505    steps: 294    lr: 4e-05     evaluation reward: 3.48\n",
      "episode: 1103   score: 3.0   memory length: 217742   epsilon: 0.766868860005061    steps: 245    lr: 4e-05     evaluation reward: 3.48\n",
      "episode: 1104   score: 5.0   memory length: 218087   epsilon: 0.7661857600050759    steps: 345    lr: 4e-05     evaluation reward: 3.5\n",
      "episode: 1105   score: 3.0   memory length: 218354   epsilon: 0.7656571000050874    steps: 267    lr: 4e-05     evaluation reward: 3.45\n",
      "episode: 1106   score: 3.0   memory length: 218581   epsilon: 0.7652076400050971    steps: 227    lr: 4e-05     evaluation reward: 3.47\n",
      "episode: 1107   score: 3.0   memory length: 218791   epsilon: 0.7647918400051061    steps: 210    lr: 4e-05     evaluation reward: 3.48\n",
      "episode: 1108   score: 1.0   memory length: 218942   epsilon: 0.7644928600051126    steps: 151    lr: 4e-05     evaluation reward: 3.46\n",
      "episode: 1109   score: 3.0   memory length: 219170   epsilon: 0.7640414200051224    steps: 228    lr: 4e-05     evaluation reward: 3.49\n",
      "episode: 1110   score: 2.0   memory length: 219386   epsilon: 0.7636137400051317    steps: 216    lr: 4e-05     evaluation reward: 3.48\n",
      "episode: 1111   score: 3.0   memory length: 219613   epsilon: 0.7631642800051415    steps: 227    lr: 4e-05     evaluation reward: 3.47\n",
      "episode: 1112   score: 3.0   memory length: 219860   epsilon: 0.7626752200051521    steps: 247    lr: 4e-05     evaluation reward: 3.48\n",
      "episode: 1113   score: 4.0   memory length: 220153   epsilon: 0.7620950800051647    steps: 293    lr: 4e-05     evaluation reward: 3.48\n",
      "episode: 1114   score: 5.0   memory length: 220440   epsilon: 0.761526820005177    steps: 287    lr: 4e-05     evaluation reward: 3.5\n",
      "episode: 1115   score: 4.0   memory length: 220698   epsilon: 0.7610159800051881    steps: 258    lr: 4e-05     evaluation reward: 3.51\n",
      "episode: 1116   score: 2.0   memory length: 220880   epsilon: 0.7606556200051959    steps: 182    lr: 4e-05     evaluation reward: 3.5\n",
      "episode: 1117   score: 4.0   memory length: 221159   epsilon: 0.7601032000052079    steps: 279    lr: 4e-05     evaluation reward: 3.51\n",
      "episode: 1118   score: 2.0   memory length: 221340   epsilon: 0.7597448200052157    steps: 181    lr: 4e-05     evaluation reward: 3.5\n",
      "episode: 1119   score: 1.0   memory length: 221491   epsilon: 0.7594458400052222    steps: 151    lr: 4e-05     evaluation reward: 3.47\n",
      "episode: 1120   score: 3.0   memory length: 221701   epsilon: 0.7590300400052312    steps: 210    lr: 4e-05     evaluation reward: 3.48\n",
      "episode: 1121   score: 5.0   memory length: 222028   epsilon: 0.7583825800052453    steps: 327    lr: 4e-05     evaluation reward: 3.48\n",
      "episode: 1122   score: 3.0   memory length: 222257   epsilon: 0.7579291600052551    steps: 229    lr: 4e-05     evaluation reward: 3.49\n",
      "episode: 1123   score: 4.0   memory length: 222557   epsilon: 0.757335160005268    steps: 300    lr: 4e-05     evaluation reward: 3.5\n",
      "episode: 1124   score: 4.0   memory length: 222834   epsilon: 0.7567867000052799    steps: 277    lr: 4e-05     evaluation reward: 3.51\n",
      "episode: 1125   score: 5.0   memory length: 223140   epsilon: 0.7561808200052931    steps: 306    lr: 4e-05     evaluation reward: 3.54\n",
      "episode: 1126   score: 2.0   memory length: 223320   epsilon: 0.7558244200053008    steps: 180    lr: 4e-05     evaluation reward: 3.54\n",
      "episode: 1127   score: 2.0   memory length: 223540   epsilon: 0.7553888200053103    steps: 220    lr: 4e-05     evaluation reward: 3.53\n",
      "episode: 1128   score: 3.0   memory length: 223753   epsilon: 0.7549670800053194    steps: 213    lr: 4e-05     evaluation reward: 3.54\n",
      "episode: 1129   score: 1.0   memory length: 223903   epsilon: 0.7546700800053259    steps: 150    lr: 4e-05     evaluation reward: 3.48\n",
      "episode: 1130   score: 4.0   memory length: 224181   epsilon: 0.7541196400053378    steps: 278    lr: 4e-05     evaluation reward: 3.5\n",
      "episode: 1131   score: 5.0   memory length: 224526   epsilon: 0.7534365400053527    steps: 345    lr: 4e-05     evaluation reward: 3.53\n",
      "episode: 1132   score: 3.0   memory length: 224769   epsilon: 0.7529554000053631    steps: 243    lr: 4e-05     evaluation reward: 3.54\n",
      "episode: 1133   score: 2.0   memory length: 224968   epsilon: 0.7525613800053716    steps: 199    lr: 4e-05     evaluation reward: 3.52\n",
      "episode: 1134   score: 3.0   memory length: 225194   epsilon: 0.7521139000053814    steps: 226    lr: 4e-05     evaluation reward: 3.51\n",
      "episode: 1135   score: 8.0   memory length: 225534   epsilon: 0.751440700005396    steps: 340    lr: 4e-05     evaluation reward: 3.55\n",
      "episode: 1136   score: 3.0   memory length: 225780   epsilon: 0.7509536200054066    steps: 246    lr: 4e-05     evaluation reward: 3.54\n",
      "episode: 1137   score: 2.0   memory length: 225979   epsilon: 0.7505596000054151    steps: 199    lr: 4e-05     evaluation reward: 3.55\n",
      "episode: 1138   score: 7.0   memory length: 226403   epsilon: 0.7497200800054333    steps: 424    lr: 4e-05     evaluation reward: 3.59\n",
      "episode: 1139   score: 8.0   memory length: 226853   epsilon: 0.7488290800054527    steps: 450    lr: 4e-05     evaluation reward: 3.65\n",
      "episode: 1140   score: 4.0   memory length: 227129   epsilon: 0.7482826000054645    steps: 276    lr: 4e-05     evaluation reward: 3.65\n",
      "episode: 1141   score: 4.0   memory length: 227391   epsilon: 0.7477638400054758    steps: 262    lr: 4e-05     evaluation reward: 3.69\n",
      "episode: 1142   score: 2.0   memory length: 227571   epsilon: 0.7474074400054835    steps: 180    lr: 4e-05     evaluation reward: 3.61\n",
      "episode: 1143   score: 6.0   memory length: 227909   epsilon: 0.7467382000054981    steps: 338    lr: 4e-05     evaluation reward: 3.61\n",
      "episode: 1144   score: 2.0   memory length: 228108   epsilon: 0.7463441800055066    steps: 199    lr: 4e-05     evaluation reward: 3.61\n",
      "episode: 1145   score: 5.0   memory length: 228447   epsilon: 0.7456729600055212    steps: 339    lr: 4e-05     evaluation reward: 3.62\n",
      "episode: 1146   score: 4.0   memory length: 228707   epsilon: 0.7451581600055324    steps: 260    lr: 4e-05     evaluation reward: 3.64\n",
      "episode: 1147   score: 2.0   memory length: 228888   epsilon: 0.7447997800055401    steps: 181    lr: 4e-05     evaluation reward: 3.62\n",
      "episode: 1148   score: 6.0   memory length: 229284   epsilon: 0.7440157000055572    steps: 396    lr: 4e-05     evaluation reward: 3.66\n",
      "episode: 1149   score: 2.0   memory length: 229483   epsilon: 0.7436216800055657    steps: 199    lr: 4e-05     evaluation reward: 3.64\n",
      "episode: 1150   score: 0.0   memory length: 229606   epsilon: 0.743378140005571    steps: 123    lr: 4e-05     evaluation reward: 3.61\n",
      "episode: 1151   score: 7.0   memory length: 229999   epsilon: 0.7426000000055879    steps: 393    lr: 4e-05     evaluation reward: 3.65\n",
      "episode: 1152   score: 4.0   memory length: 230272   epsilon: 0.7420594600055996    steps: 273    lr: 4e-05     evaluation reward: 3.65\n",
      "episode: 1153   score: 6.0   memory length: 230635   epsilon: 0.7413407200056152    steps: 363    lr: 4e-05     evaluation reward: 3.66\n",
      "episode: 1154   score: 2.0   memory length: 230817   epsilon: 0.7409803600056231    steps: 182    lr: 4e-05     evaluation reward: 3.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1155   score: 0.0   memory length: 230939   epsilon: 0.7407388000056283    steps: 122    lr: 4e-05     evaluation reward: 3.61\n",
      "episode: 1156   score: 3.0   memory length: 231171   epsilon: 0.7402794400056383    steps: 232    lr: 4e-05     evaluation reward: 3.59\n",
      "episode: 1157   score: 4.0   memory length: 231433   epsilon: 0.7397606800056495    steps: 262    lr: 4e-05     evaluation reward: 3.6\n",
      "episode: 1158   score: 1.0   memory length: 231602   epsilon: 0.7394260600056568    steps: 169    lr: 4e-05     evaluation reward: 3.58\n",
      "episode: 1159   score: 4.0   memory length: 231898   epsilon: 0.7388399800056695    steps: 296    lr: 4e-05     evaluation reward: 3.59\n",
      "episode: 1160   score: 4.0   memory length: 232179   epsilon: 0.7382836000056816    steps: 281    lr: 4e-05     evaluation reward: 3.61\n",
      "episode: 1161   score: 7.0   memory length: 232590   epsilon: 0.7374698200056993    steps: 411    lr: 4e-05     evaluation reward: 3.66\n",
      "episode: 1162   score: 1.0   memory length: 232759   epsilon: 0.7371352000057065    steps: 169    lr: 4e-05     evaluation reward: 3.64\n",
      "episode: 1163   score: 0.0   memory length: 232881   epsilon: 0.7368936400057118    steps: 122    lr: 4e-05     evaluation reward: 3.62\n",
      "episode: 1164   score: 4.0   memory length: 233174   epsilon: 0.7363135000057244    steps: 293    lr: 4e-05     evaluation reward: 3.63\n",
      "episode: 1165   score: 2.0   memory length: 233353   epsilon: 0.7359590800057321    steps: 179    lr: 4e-05     evaluation reward: 3.62\n",
      "episode: 1166   score: 4.0   memory length: 233629   epsilon: 0.7354126000057439    steps: 276    lr: 4e-05     evaluation reward: 3.59\n",
      "episode: 1167   score: 3.0   memory length: 233898   epsilon: 0.7348799800057555    steps: 269    lr: 4e-05     evaluation reward: 3.62\n",
      "episode: 1168   score: 4.0   memory length: 234174   epsilon: 0.7343335000057674    steps: 276    lr: 4e-05     evaluation reward: 3.65\n",
      "episode: 1169   score: 4.0   memory length: 234431   epsilon: 0.7338246400057784    steps: 257    lr: 4e-05     evaluation reward: 3.63\n",
      "episode: 1170   score: 3.0   memory length: 234679   epsilon: 0.7333336000057891    steps: 248    lr: 4e-05     evaluation reward: 3.61\n",
      "episode: 1171   score: 0.0   memory length: 234801   epsilon: 0.7330920400057943    steps: 122    lr: 4e-05     evaluation reward: 3.54\n",
      "episode: 1172   score: 4.0   memory length: 235060   epsilon: 0.7325792200058054    steps: 259    lr: 4e-05     evaluation reward: 3.52\n",
      "episode: 1173   score: 4.0   memory length: 235352   epsilon: 0.732001060005818    steps: 292    lr: 4e-05     evaluation reward: 3.54\n",
      "episode: 1174   score: 6.0   memory length: 235729   epsilon: 0.7312546000058342    steps: 377    lr: 4e-05     evaluation reward: 3.54\n",
      "episode: 1175   score: 3.0   memory length: 235994   epsilon: 0.7307299000058456    steps: 265    lr: 4e-05     evaluation reward: 3.53\n",
      "episode: 1176   score: 3.0   memory length: 236261   epsilon: 0.7302012400058571    steps: 267    lr: 4e-05     evaluation reward: 3.54\n",
      "episode: 1177   score: 6.0   memory length: 236588   epsilon: 0.7295537800058711    steps: 327    lr: 4e-05     evaluation reward: 3.53\n",
      "episode: 1178   score: 2.0   memory length: 236806   epsilon: 0.7291221400058805    steps: 218    lr: 4e-05     evaluation reward: 3.51\n",
      "episode: 1179   score: 6.0   memory length: 237172   epsilon: 0.7283974600058962    steps: 366    lr: 4e-05     evaluation reward: 3.53\n",
      "episode: 1180   score: 3.0   memory length: 237397   epsilon: 0.7279519600059059    steps: 225    lr: 4e-05     evaluation reward: 3.55\n",
      "episode: 1181   score: 3.0   memory length: 237641   epsilon: 0.7274688400059164    steps: 244    lr: 4e-05     evaluation reward: 3.58\n",
      "episode: 1182   score: 4.0   memory length: 237896   epsilon: 0.7269639400059273    steps: 255    lr: 4e-05     evaluation reward: 3.58\n",
      "episode: 1183   score: 2.0   memory length: 238094   epsilon: 0.7265719000059359    steps: 198    lr: 4e-05     evaluation reward: 3.52\n",
      "episode: 1184   score: 3.0   memory length: 238324   epsilon: 0.7261165000059457    steps: 230    lr: 4e-05     evaluation reward: 3.52\n",
      "episode: 1185   score: 8.0   memory length: 238719   epsilon: 0.7253344000059627    steps: 395    lr: 4e-05     evaluation reward: 3.57\n",
      "episode: 1186   score: 8.0   memory length: 239165   epsilon: 0.7244513200059819    steps: 446    lr: 4e-05     evaluation reward: 3.62\n",
      "episode: 1187   score: 3.0   memory length: 239409   epsilon: 0.7239682000059924    steps: 244    lr: 4e-05     evaluation reward: 3.6\n",
      "episode: 1188   score: 4.0   memory length: 239683   epsilon: 0.7234256800060042    steps: 274    lr: 4e-05     evaluation reward: 3.63\n",
      "episode: 1189   score: 3.0   memory length: 239892   epsilon: 0.7230118600060131    steps: 209    lr: 4e-05     evaluation reward: 3.62\n",
      "episode: 1190   score: 6.0   memory length: 240230   epsilon: 0.7223426200060277    steps: 338    lr: 4e-05     evaluation reward: 3.63\n",
      "episode: 1191   score: 5.0   memory length: 240516   epsilon: 0.72177634000604    steps: 286    lr: 4e-05     evaluation reward: 3.67\n",
      "episode: 1192   score: 3.0   memory length: 240743   epsilon: 0.7213268800060497    steps: 227    lr: 4e-05     evaluation reward: 3.65\n",
      "episode: 1193   score: 4.0   memory length: 241019   epsilon: 0.7207804000060616    steps: 276    lr: 4e-05     evaluation reward: 3.66\n",
      "episode: 1194   score: 2.0   memory length: 241217   epsilon: 0.7203883600060701    steps: 198    lr: 4e-05     evaluation reward: 3.64\n",
      "episode: 1195   score: 3.0   memory length: 241483   epsilon: 0.7198616800060815    steps: 266    lr: 4e-05     evaluation reward: 3.62\n",
      "episode: 1196   score: 5.0   memory length: 241828   epsilon: 0.7191785800060964    steps: 345    lr: 4e-05     evaluation reward: 3.6\n",
      "episode: 1197   score: 7.0   memory length: 242231   epsilon: 0.7183806400061137    steps: 403    lr: 4e-05     evaluation reward: 3.63\n",
      "episode: 1198   score: 2.0   memory length: 242430   epsilon: 0.7179866200061222    steps: 199    lr: 4e-05     evaluation reward: 3.57\n",
      "episode: 1199   score: 8.0   memory length: 242901   epsilon: 0.7170540400061425    steps: 471    lr: 4e-05     evaluation reward: 3.62\n",
      "episode: 1200   score: 3.0   memory length: 243165   epsilon: 0.7165313200061538    steps: 264    lr: 4e-05     evaluation reward: 3.63\n",
      "episode: 1201   score: 3.0   memory length: 243392   epsilon: 0.7160818600061636    steps: 227    lr: 4e-05     evaluation reward: 3.62\n",
      "episode: 1202   score: 7.0   memory length: 243778   epsilon: 0.7153175800061802    steps: 386    lr: 4e-05     evaluation reward: 3.65\n",
      "episode: 1203   score: 4.0   memory length: 244033   epsilon: 0.7148126800061911    steps: 255    lr: 4e-05     evaluation reward: 3.66\n",
      "episode: 1204   score: 3.0   memory length: 244262   epsilon: 0.714359260006201    steps: 229    lr: 4e-05     evaluation reward: 3.64\n",
      "episode: 1205   score: 1.0   memory length: 244430   epsilon: 0.7140266200062082    steps: 168    lr: 4e-05     evaluation reward: 3.62\n",
      "episode: 1206   score: 1.0   memory length: 244599   epsilon: 0.7136920000062155    steps: 169    lr: 4e-05     evaluation reward: 3.6\n",
      "episode: 1207   score: 3.0   memory length: 244825   epsilon: 0.7132445200062252    steps: 226    lr: 4e-05     evaluation reward: 3.6\n",
      "episode: 1208   score: 1.0   memory length: 244976   epsilon: 0.7129455400062317    steps: 151    lr: 4e-05     evaluation reward: 3.6\n",
      "episode: 1209   score: 5.0   memory length: 245301   epsilon: 0.7123020400062456    steps: 325    lr: 4e-05     evaluation reward: 3.62\n",
      "episode: 1210   score: 9.0   memory length: 245785   epsilon: 0.7113437200062664    steps: 484    lr: 4e-05     evaluation reward: 3.69\n",
      "episode: 1211   score: 5.0   memory length: 246110   epsilon: 0.7107002200062804    steps: 325    lr: 4e-05     evaluation reward: 3.71\n",
      "episode: 1212   score: 7.0   memory length: 246482   epsilon: 0.7099636600062964    steps: 372    lr: 4e-05     evaluation reward: 3.75\n",
      "episode: 1213   score: 3.0   memory length: 246708   epsilon: 0.7095161800063061    steps: 226    lr: 4e-05     evaluation reward: 3.74\n",
      "episode: 1214   score: 2.0   memory length: 246905   epsilon: 0.7091261200063146    steps: 197    lr: 4e-05     evaluation reward: 3.71\n",
      "episode: 1215   score: 4.0   memory length: 247200   epsilon: 0.7085420200063273    steps: 295    lr: 4e-05     evaluation reward: 3.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1216   score: 4.0   memory length: 247473   epsilon: 0.708001480006339    steps: 273    lr: 4e-05     evaluation reward: 3.73\n",
      "episode: 1217   score: 3.0   memory length: 247741   epsilon: 0.7074708400063505    steps: 268    lr: 4e-05     evaluation reward: 3.72\n",
      "episode: 1218   score: 7.0   memory length: 248142   epsilon: 0.7066768600063678    steps: 401    lr: 4e-05     evaluation reward: 3.77\n",
      "episode: 1219   score: 3.0   memory length: 248388   epsilon: 0.7061897800063783    steps: 246    lr: 4e-05     evaluation reward: 3.79\n",
      "episode: 1220   score: 4.0   memory length: 248686   epsilon: 0.7055997400063911    steps: 298    lr: 4e-05     evaluation reward: 3.8\n",
      "episode: 1221   score: 10.0   memory length: 249208   epsilon: 0.7045661800064136    steps: 522    lr: 4e-05     evaluation reward: 3.85\n",
      "episode: 1222   score: 6.0   memory length: 249543   epsilon: 0.703902880006428    steps: 335    lr: 4e-05     evaluation reward: 3.88\n",
      "episode: 1223   score: 3.0   memory length: 249771   epsilon: 0.7034514400064378    steps: 228    lr: 4e-05     evaluation reward: 3.87\n",
      "episode: 1224   score: 5.0   memory length: 250059   epsilon: 0.7028812000064502    steps: 288    lr: 4e-05     evaluation reward: 3.88\n",
      "episode: 1225   score: 4.0   memory length: 250319   epsilon: 0.7023664000064613    steps: 260    lr: 4e-05     evaluation reward: 3.87\n",
      "episode: 1226   score: 5.0   memory length: 250646   epsilon: 0.7017189400064754    steps: 327    lr: 4e-05     evaluation reward: 3.9\n",
      "episode: 1227   score: 2.0   memory length: 250863   epsilon: 0.7012892800064847    steps: 217    lr: 4e-05     evaluation reward: 3.9\n",
      "episode: 1228   score: 4.0   memory length: 251121   epsilon: 0.7007784400064958    steps: 258    lr: 4e-05     evaluation reward: 3.91\n",
      "episode: 1229   score: 8.0   memory length: 251586   epsilon: 0.6998577400065158    steps: 465    lr: 4e-05     evaluation reward: 3.98\n",
      "episode: 1230   score: 3.0   memory length: 251852   epsilon: 0.6993310600065272    steps: 266    lr: 4e-05     evaluation reward: 3.97\n",
      "episode: 1231   score: 5.0   memory length: 252173   epsilon: 0.698695480006541    steps: 321    lr: 4e-05     evaluation reward: 3.97\n",
      "episode: 1232   score: 6.0   memory length: 252543   epsilon: 0.6979628800065569    steps: 370    lr: 4e-05     evaluation reward: 4.0\n",
      "episode: 1233   score: 2.0   memory length: 252740   epsilon: 0.6975728200065654    steps: 197    lr: 4e-05     evaluation reward: 4.0\n",
      "episode: 1234   score: 4.0   memory length: 253057   epsilon: 0.696945160006579    steps: 317    lr: 4e-05     evaluation reward: 4.01\n",
      "episode: 1235   score: 1.0   memory length: 253207   epsilon: 0.6966481600065855    steps: 150    lr: 4e-05     evaluation reward: 3.94\n",
      "episode: 1236   score: 2.0   memory length: 253388   epsilon: 0.6962897800065933    steps: 181    lr: 4e-05     evaluation reward: 3.93\n",
      "episode: 1237   score: 5.0   memory length: 253716   epsilon: 0.6956403400066073    steps: 328    lr: 4e-05     evaluation reward: 3.96\n",
      "episode: 1238   score: 4.0   memory length: 254015   epsilon: 0.6950483200066202    steps: 299    lr: 4e-05     evaluation reward: 3.93\n",
      "episode: 1239   score: 2.0   memory length: 254213   epsilon: 0.6946562800066287    steps: 198    lr: 4e-05     evaluation reward: 3.87\n",
      "episode: 1240   score: 6.0   memory length: 254553   epsilon: 0.6939830800066433    steps: 340    lr: 4e-05     evaluation reward: 3.89\n",
      "episode: 1241   score: 2.0   memory length: 254768   epsilon: 0.6935573800066526    steps: 215    lr: 4e-05     evaluation reward: 3.87\n",
      "episode: 1242   score: 8.0   memory length: 255193   epsilon: 0.6927158800066708    steps: 425    lr: 4e-05     evaluation reward: 3.93\n",
      "episode: 1243   score: 4.0   memory length: 255484   epsilon: 0.6921397000066833    steps: 291    lr: 4e-05     evaluation reward: 3.91\n",
      "episode: 1244   score: 1.0   memory length: 255635   epsilon: 0.6918407200066898    steps: 151    lr: 4e-05     evaluation reward: 3.9\n",
      "episode: 1245   score: 3.0   memory length: 255845   epsilon: 0.6914249200066989    steps: 210    lr: 4e-05     evaluation reward: 3.88\n",
      "episode: 1246   score: 6.0   memory length: 256237   epsilon: 0.6906487600067157    steps: 392    lr: 4e-05     evaluation reward: 3.9\n",
      "episode: 1247   score: 2.0   memory length: 256434   epsilon: 0.6902587000067242    steps: 197    lr: 4e-05     evaluation reward: 3.9\n",
      "episode: 1248   score: 4.0   memory length: 256732   epsilon: 0.689668660006737    steps: 298    lr: 4e-05     evaluation reward: 3.88\n",
      "episode: 1249   score: 4.0   memory length: 257008   epsilon: 0.6891221800067489    steps: 276    lr: 4e-05     evaluation reward: 3.9\n",
      "episode: 1250   score: 4.0   memory length: 257306   epsilon: 0.6885321400067617    steps: 298    lr: 4e-05     evaluation reward: 3.94\n",
      "episode: 1251   score: 4.0   memory length: 257583   epsilon: 0.6879836800067736    steps: 277    lr: 4e-05     evaluation reward: 3.91\n",
      "episode: 1252   score: 7.0   memory length: 257973   epsilon: 0.6872114800067903    steps: 390    lr: 4e-05     evaluation reward: 3.94\n",
      "episode: 1253   score: 5.0   memory length: 258316   epsilon: 0.6865323400068051    steps: 343    lr: 4e-05     evaluation reward: 3.93\n",
      "episode: 1254   score: 5.0   memory length: 258661   epsilon: 0.6858492400068199    steps: 345    lr: 4e-05     evaluation reward: 3.96\n",
      "episode: 1255   score: 5.0   memory length: 258990   epsilon: 0.685197820006834    steps: 329    lr: 4e-05     evaluation reward: 4.01\n",
      "episode: 1256   score: 4.0   memory length: 259290   epsilon: 0.6846038200068469    steps: 300    lr: 4e-05     evaluation reward: 4.02\n",
      "episode: 1257   score: 4.0   memory length: 259587   epsilon: 0.6840157600068597    steps: 297    lr: 4e-05     evaluation reward: 4.02\n",
      "episode: 1258   score: 2.0   memory length: 259785   epsilon: 0.6836237200068682    steps: 198    lr: 4e-05     evaluation reward: 4.03\n",
      "episode: 1259   score: 2.0   memory length: 260000   epsilon: 0.6831980200068775    steps: 215    lr: 4e-05     evaluation reward: 4.01\n",
      "episode: 1260   score: 6.0   memory length: 260414   epsilon: 0.6823783000068953    steps: 414    lr: 4e-05     evaluation reward: 4.03\n",
      "episode: 1261   score: 3.0   memory length: 260660   epsilon: 0.6818912200069058    steps: 246    lr: 4e-05     evaluation reward: 3.99\n",
      "episode: 1262   score: 6.0   memory length: 260979   epsilon: 0.6812596000069195    steps: 319    lr: 4e-05     evaluation reward: 4.04\n",
      "episode: 1263   score: 7.0   memory length: 261360   epsilon: 0.6805052200069359    steps: 381    lr: 4e-05     evaluation reward: 4.11\n",
      "episode: 1264   score: 5.0   memory length: 261667   epsilon: 0.6798973600069491    steps: 307    lr: 4e-05     evaluation reward: 4.12\n",
      "episode: 1265   score: 5.0   memory length: 262014   epsilon: 0.679210300006964    steps: 347    lr: 4e-05     evaluation reward: 4.15\n",
      "episode: 1266   score: 6.0   memory length: 262396   epsilon: 0.6784539400069804    steps: 382    lr: 4e-05     evaluation reward: 4.17\n",
      "episode: 1267   score: 2.0   memory length: 262594   epsilon: 0.678061900006989    steps: 198    lr: 4e-05     evaluation reward: 4.16\n",
      "episode: 1268   score: 3.0   memory length: 262805   epsilon: 0.677644120006998    steps: 211    lr: 4e-05     evaluation reward: 4.15\n",
      "episode: 1269   score: 3.0   memory length: 263035   epsilon: 0.6771887200070079    steps: 230    lr: 4e-05     evaluation reward: 4.14\n",
      "episode: 1270   score: 4.0   memory length: 263310   epsilon: 0.6766442200070197    steps: 275    lr: 4e-05     evaluation reward: 4.15\n",
      "episode: 1271   score: 6.0   memory length: 263684   epsilon: 0.6759037000070358    steps: 374    lr: 4e-05     evaluation reward: 4.21\n",
      "episode: 1272   score: 4.0   memory length: 263970   epsilon: 0.6753374200070481    steps: 286    lr: 4e-05     evaluation reward: 4.21\n",
      "episode: 1273   score: 12.0   memory length: 264545   epsilon: 0.6741989200070728    steps: 575    lr: 4e-05     evaluation reward: 4.29\n",
      "episode: 1274   score: 3.0   memory length: 264793   epsilon: 0.6737078800070835    steps: 248    lr: 4e-05     evaluation reward: 4.26\n",
      "episode: 1275   score: 5.0   memory length: 265132   epsilon: 0.673036660007098    steps: 339    lr: 4e-05     evaluation reward: 4.28\n",
      "episode: 1276   score: 2.0   memory length: 265314   epsilon: 0.6726763000071059    steps: 182    lr: 4e-05     evaluation reward: 4.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1277   score: 5.0   memory length: 265678   epsilon: 0.6719555800071215    steps: 364    lr: 4e-05     evaluation reward: 4.26\n",
      "episode: 1278   score: 4.0   memory length: 265971   epsilon: 0.6713754400071341    steps: 293    lr: 4e-05     evaluation reward: 4.28\n",
      "episode: 1279   score: 11.0   memory length: 266472   epsilon: 0.6703834600071557    steps: 501    lr: 4e-05     evaluation reward: 4.33\n",
      "episode: 1280   score: 2.0   memory length: 266652   epsilon: 0.6700270600071634    steps: 180    lr: 4e-05     evaluation reward: 4.32\n",
      "episode: 1281   score: 5.0   memory length: 266971   epsilon: 0.6693954400071771    steps: 319    lr: 4e-05     evaluation reward: 4.34\n",
      "episode: 1282   score: 3.0   memory length: 267221   epsilon: 0.6689004400071878    steps: 250    lr: 4e-05     evaluation reward: 4.33\n",
      "episode: 1283   score: 4.0   memory length: 267479   epsilon: 0.6683896000071989    steps: 258    lr: 4e-05     evaluation reward: 4.35\n",
      "episode: 1284   score: 6.0   memory length: 267837   epsilon: 0.6676807600072143    steps: 358    lr: 4e-05     evaluation reward: 4.38\n",
      "episode: 1285   score: 2.0   memory length: 268034   epsilon: 0.6672907000072228    steps: 197    lr: 4e-05     evaluation reward: 4.32\n",
      "episode: 1286   score: 3.0   memory length: 268260   epsilon: 0.6668432200072325    steps: 226    lr: 4e-05     evaluation reward: 4.27\n",
      "episode: 1287   score: 7.0   memory length: 268682   epsilon: 0.6660076600072506    steps: 422    lr: 4e-05     evaluation reward: 4.31\n",
      "episode: 1288   score: 3.0   memory length: 268895   epsilon: 0.6655859200072598    steps: 213    lr: 4e-05     evaluation reward: 4.3\n",
      "episode: 1289   score: 6.0   memory length: 269250   epsilon: 0.6648830200072751    steps: 355    lr: 4e-05     evaluation reward: 4.33\n",
      "episode: 1290   score: 8.0   memory length: 269693   epsilon: 0.6640058800072941    steps: 443    lr: 4e-05     evaluation reward: 4.35\n",
      "episode: 1291   score: 3.0   memory length: 269957   epsilon: 0.6634831600073055    steps: 264    lr: 4e-05     evaluation reward: 4.33\n",
      "episode: 1292   score: 5.0   memory length: 270263   epsilon: 0.6628772800073186    steps: 306    lr: 4e-05     evaluation reward: 4.35\n",
      "episode: 1293   score: 3.0   memory length: 270509   epsilon: 0.6623902000073292    steps: 246    lr: 4e-05     evaluation reward: 4.34\n",
      "episode: 1294   score: 4.0   memory length: 270785   epsilon: 0.661843720007341    steps: 276    lr: 4e-05     evaluation reward: 4.36\n",
      "episode: 1295   score: 5.0   memory length: 271109   epsilon: 0.661202200007355    steps: 324    lr: 4e-05     evaluation reward: 4.38\n",
      "episode: 1296   score: 5.0   memory length: 271412   epsilon: 0.660602260007368    steps: 303    lr: 4e-05     evaluation reward: 4.38\n",
      "episode: 1297   score: 4.0   memory length: 271705   epsilon: 0.6600221200073806    steps: 293    lr: 4e-05     evaluation reward: 4.35\n",
      "episode: 1298   score: 5.0   memory length: 272050   epsilon: 0.6593390200073954    steps: 345    lr: 4e-05     evaluation reward: 4.38\n",
      "episode: 1299   score: 3.0   memory length: 272278   epsilon: 0.6588875800074052    steps: 228    lr: 4e-05     evaluation reward: 4.33\n",
      "episode: 1300   score: 5.0   memory length: 272583   epsilon: 0.6582836800074183    steps: 305    lr: 4e-05     evaluation reward: 4.35\n",
      "episode: 1301   score: 5.0   memory length: 272891   epsilon: 0.6576738400074316    steps: 308    lr: 4e-05     evaluation reward: 4.37\n",
      "episode: 1302   score: 2.0   memory length: 273089   epsilon: 0.6572818000074401    steps: 198    lr: 4e-05     evaluation reward: 4.32\n",
      "episode: 1303   score: 3.0   memory length: 273334   epsilon: 0.6567967000074506    steps: 245    lr: 4e-05     evaluation reward: 4.31\n",
      "episode: 1304   score: 3.0   memory length: 273546   epsilon: 0.6563769400074597    steps: 212    lr: 4e-05     evaluation reward: 4.31\n",
      "episode: 1305   score: 3.0   memory length: 273791   epsilon: 0.6558918400074703    steps: 245    lr: 4e-05     evaluation reward: 4.33\n",
      "episode: 1306   score: 2.0   memory length: 274006   epsilon: 0.6554661400074795    steps: 215    lr: 4e-05     evaluation reward: 4.34\n",
      "episode: 1307   score: 6.0   memory length: 274360   epsilon: 0.6547652200074947    steps: 354    lr: 4e-05     evaluation reward: 4.37\n",
      "episode: 1308   score: 4.0   memory length: 274637   epsilon: 0.6542167600075066    steps: 277    lr: 4e-05     evaluation reward: 4.4\n",
      "episode: 1309   score: 6.0   memory length: 275033   epsilon: 0.6534326800075236    steps: 396    lr: 4e-05     evaluation reward: 4.41\n",
      "episode: 1310   score: 4.0   memory length: 275329   epsilon: 0.6528466000075364    steps: 296    lr: 4e-05     evaluation reward: 4.36\n",
      "episode: 1311   score: 6.0   memory length: 275682   epsilon: 0.6521476600075515    steps: 353    lr: 4e-05     evaluation reward: 4.37\n",
      "episode: 1312   score: 5.0   memory length: 275988   epsilon: 0.6515417800075647    steps: 306    lr: 4e-05     evaluation reward: 4.35\n",
      "episode: 1313   score: 0.0   memory length: 276111   epsilon: 0.65129824000757    steps: 123    lr: 4e-05     evaluation reward: 4.32\n",
      "episode: 1314   score: 4.0   memory length: 276386   epsilon: 0.6507537400075818    steps: 275    lr: 4e-05     evaluation reward: 4.34\n",
      "episode: 1315   score: 1.0   memory length: 276555   epsilon: 0.6504191200075891    steps: 169    lr: 4e-05     evaluation reward: 4.31\n",
      "episode: 1316   score: 2.0   memory length: 276737   epsilon: 0.6500587600075969    steps: 182    lr: 4e-05     evaluation reward: 4.29\n",
      "episode: 1317   score: 3.0   memory length: 276985   epsilon: 0.6495677200076075    steps: 248    lr: 4e-05     evaluation reward: 4.29\n",
      "episode: 1318   score: 4.0   memory length: 277302   epsilon: 0.6489400600076212    steps: 317    lr: 4e-05     evaluation reward: 4.26\n",
      "episode: 1319   score: 2.0   memory length: 277482   epsilon: 0.6485836600076289    steps: 180    lr: 4e-05     evaluation reward: 4.25\n",
      "episode: 1320   score: 2.0   memory length: 277664   epsilon: 0.6482233000076367    steps: 182    lr: 4e-05     evaluation reward: 4.23\n",
      "episode: 1321   score: 3.0   memory length: 277909   epsilon: 0.6477382000076473    steps: 245    lr: 4e-05     evaluation reward: 4.16\n",
      "episode: 1322   score: 3.0   memory length: 278119   epsilon: 0.6473224000076563    steps: 210    lr: 4e-05     evaluation reward: 4.13\n",
      "episode: 1323   score: 6.0   memory length: 278458   epsilon: 0.6466511800076709    steps: 339    lr: 4e-05     evaluation reward: 4.16\n",
      "episode: 1324   score: 2.0   memory length: 278640   epsilon: 0.6462908200076787    steps: 182    lr: 4e-05     evaluation reward: 4.13\n",
      "episode: 1325   score: 6.0   memory length: 279017   epsilon: 0.6455443600076949    steps: 377    lr: 4e-05     evaluation reward: 4.15\n",
      "episode: 1326   score: 2.0   memory length: 279197   epsilon: 0.6451879600077026    steps: 180    lr: 4e-05     evaluation reward: 4.12\n",
      "episode: 1327   score: 3.0   memory length: 279424   epsilon: 0.6447385000077124    steps: 227    lr: 4e-05     evaluation reward: 4.13\n",
      "episode: 1328   score: 9.0   memory length: 279918   epsilon: 0.6437603800077336    steps: 494    lr: 4e-05     evaluation reward: 4.18\n",
      "episode: 1329   score: 4.0   memory length: 280197   epsilon: 0.6432079600077456    steps: 279    lr: 4e-05     evaluation reward: 4.14\n",
      "episode: 1330   score: 2.0   memory length: 280394   epsilon: 0.6428179000077541    steps: 197    lr: 4e-05     evaluation reward: 4.13\n",
      "episode: 1331   score: 2.0   memory length: 280573   epsilon: 0.6424634800077618    steps: 179    lr: 4e-05     evaluation reward: 4.1\n",
      "episode: 1332   score: 8.0   memory length: 281013   epsilon: 0.6415922800077807    steps: 440    lr: 4e-05     evaluation reward: 4.12\n",
      "episode: 1333   score: 4.0   memory length: 281331   epsilon: 0.6409626400077943    steps: 318    lr: 4e-05     evaluation reward: 4.14\n",
      "episode: 1334   score: 9.0   memory length: 281832   epsilon: 0.6399706600078159    steps: 501    lr: 4e-05     evaluation reward: 4.19\n",
      "episode: 1335   score: 6.0   memory length: 282180   epsilon: 0.6392816200078308    steps: 348    lr: 4e-05     evaluation reward: 4.24\n",
      "episode: 1336   score: 8.0   memory length: 282655   epsilon: 0.6383411200078513    steps: 475    lr: 4e-05     evaluation reward: 4.3\n",
      "episode: 1337   score: 4.0   memory length: 282933   epsilon: 0.6377906800078632    steps: 278    lr: 4e-05     evaluation reward: 4.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1338   score: 6.0   memory length: 283310   epsilon: 0.6370442200078794    steps: 377    lr: 4e-05     evaluation reward: 4.31\n",
      "episode: 1339   score: 4.0   memory length: 283586   epsilon: 0.6364977400078913    steps: 276    lr: 4e-05     evaluation reward: 4.33\n",
      "episode: 1340   score: 3.0   memory length: 283812   epsilon: 0.636050260007901    steps: 226    lr: 4e-05     evaluation reward: 4.3\n",
      "episode: 1341   score: 5.0   memory length: 284139   epsilon: 0.635402800007915    steps: 327    lr: 4e-05     evaluation reward: 4.33\n",
      "episode: 1342   score: 5.0   memory length: 284463   epsilon: 0.634761280007929    steps: 324    lr: 4e-05     evaluation reward: 4.3\n",
      "episode: 1343   score: 6.0   memory length: 284823   epsilon: 0.6340484800079444    steps: 360    lr: 4e-05     evaluation reward: 4.32\n",
      "episode: 1344   score: 3.0   memory length: 285052   epsilon: 0.6335950600079543    steps: 229    lr: 4e-05     evaluation reward: 4.34\n",
      "episode: 1345   score: 1.0   memory length: 285202   epsilon: 0.6332980600079607    steps: 150    lr: 4e-05     evaluation reward: 4.32\n",
      "episode: 1346   score: 3.0   memory length: 285415   epsilon: 0.6328763200079699    steps: 213    lr: 4e-05     evaluation reward: 4.29\n",
      "episode: 1347   score: 8.0   memory length: 285855   epsilon: 0.6320051200079888    steps: 440    lr: 4e-05     evaluation reward: 4.35\n",
      "episode: 1348   score: 4.0   memory length: 286134   epsilon: 0.6314527000080008    steps: 279    lr: 4e-05     evaluation reward: 4.35\n",
      "episode: 1349   score: 7.0   memory length: 286517   epsilon: 0.6306943600080173    steps: 383    lr: 4e-05     evaluation reward: 4.38\n",
      "episode: 1350   score: 3.0   memory length: 286745   epsilon: 0.6302429200080271    steps: 228    lr: 4e-05     evaluation reward: 4.37\n",
      "episode: 1351   score: 5.0   memory length: 287036   epsilon: 0.6296667400080396    steps: 291    lr: 4e-05     evaluation reward: 4.38\n",
      "episode: 1352   score: 5.0   memory length: 287322   epsilon: 0.6291004600080519    steps: 286    lr: 4e-05     evaluation reward: 4.36\n",
      "episode: 1353   score: 6.0   memory length: 287696   epsilon: 0.6283599400080679    steps: 374    lr: 4e-05     evaluation reward: 4.37\n",
      "episode: 1354   score: 5.0   memory length: 288044   epsilon: 0.6276709000080829    steps: 348    lr: 4e-05     evaluation reward: 4.37\n",
      "episode: 1355   score: 2.0   memory length: 288266   epsilon: 0.6272313400080924    steps: 222    lr: 4e-05     evaluation reward: 4.34\n",
      "episode: 1356   score: 6.0   memory length: 288584   epsilon: 0.6266017000081061    steps: 318    lr: 4e-05     evaluation reward: 4.36\n",
      "episode: 1357   score: 6.0   memory length: 288937   epsilon: 0.6259027600081213    steps: 353    lr: 4e-05     evaluation reward: 4.38\n",
      "episode: 1358   score: 5.0   memory length: 289257   epsilon: 0.625269160008135    steps: 320    lr: 4e-05     evaluation reward: 4.41\n",
      "episode: 1359   score: 1.0   memory length: 289407   epsilon: 0.6249721600081415    steps: 150    lr: 4e-05     evaluation reward: 4.4\n",
      "episode: 1360   score: 5.0   memory length: 289733   epsilon: 0.6243266800081555    steps: 326    lr: 4e-05     evaluation reward: 4.39\n",
      "episode: 1361   score: 6.0   memory length: 290089   epsilon: 0.6236218000081708    steps: 356    lr: 4e-05     evaluation reward: 4.42\n",
      "episode: 1362   score: 6.0   memory length: 290410   epsilon: 0.6229862200081846    steps: 321    lr: 4e-05     evaluation reward: 4.42\n",
      "episode: 1363   score: 5.0   memory length: 290713   epsilon: 0.6223862800081976    steps: 303    lr: 4e-05     evaluation reward: 4.4\n",
      "episode: 1364   score: 8.0   memory length: 291163   epsilon: 0.621495280008217    steps: 450    lr: 4e-05     evaluation reward: 4.43\n",
      "episode: 1365   score: 5.0   memory length: 291511   epsilon: 0.6208062400082319    steps: 348    lr: 4e-05     evaluation reward: 4.43\n",
      "episode: 1366   score: 4.0   memory length: 291789   epsilon: 0.6202558000082439    steps: 278    lr: 4e-05     evaluation reward: 4.41\n",
      "episode: 1367   score: 1.0   memory length: 291961   epsilon: 0.6199152400082513    steps: 172    lr: 4e-05     evaluation reward: 4.4\n",
      "episode: 1368   score: 6.0   memory length: 292299   epsilon: 0.6192460000082658    steps: 338    lr: 4e-05     evaluation reward: 4.43\n",
      "episode: 1369   score: 6.0   memory length: 292656   epsilon: 0.6185391400082811    steps: 357    lr: 4e-05     evaluation reward: 4.46\n",
      "episode: 1370   score: 6.0   memory length: 293009   epsilon: 0.6178402000082963    steps: 353    lr: 4e-05     evaluation reward: 4.48\n",
      "episode: 1371   score: 3.0   memory length: 293259   epsilon: 0.6173452000083071    steps: 250    lr: 4e-05     evaluation reward: 4.45\n",
      "episode: 1372   score: 4.0   memory length: 293516   epsilon: 0.6168363400083181    steps: 257    lr: 4e-05     evaluation reward: 4.45\n",
      "episode: 1373   score: 3.0   memory length: 293744   epsilon: 0.6163849000083279    steps: 228    lr: 4e-05     evaluation reward: 4.36\n",
      "episode: 1374   score: 4.0   memory length: 294023   epsilon: 0.6158324800083399    steps: 279    lr: 4e-05     evaluation reward: 4.37\n",
      "episode: 1375   score: 3.0   memory length: 294249   epsilon: 0.6153850000083496    steps: 226    lr: 4e-05     evaluation reward: 4.35\n",
      "episode: 1376   score: 4.0   memory length: 294527   epsilon: 0.6148345600083616    steps: 278    lr: 4e-05     evaluation reward: 4.37\n",
      "episode: 1377   score: 9.0   memory length: 295019   epsilon: 0.6138604000083827    steps: 492    lr: 4e-05     evaluation reward: 4.41\n",
      "episode: 1378   score: 2.0   memory length: 295235   epsilon: 0.613432720008392    steps: 216    lr: 4e-05     evaluation reward: 4.39\n",
      "episode: 1379   score: 3.0   memory length: 295481   epsilon: 0.6129456400084026    steps: 246    lr: 4e-05     evaluation reward: 4.31\n",
      "episode: 1380   score: 7.0   memory length: 295856   epsilon: 0.6122031400084187    steps: 375    lr: 4e-05     evaluation reward: 4.36\n",
      "episode: 1381   score: 13.0   memory length: 296334   epsilon: 0.6112567000084392    steps: 478    lr: 4e-05     evaluation reward: 4.44\n",
      "episode: 1382   score: 4.0   memory length: 296610   epsilon: 0.6107102200084511    steps: 276    lr: 4e-05     evaluation reward: 4.45\n",
      "episode: 1383   score: 2.0   memory length: 296828   epsilon: 0.6102785800084605    steps: 218    lr: 4e-05     evaluation reward: 4.43\n",
      "episode: 1384   score: 4.0   memory length: 297106   epsilon: 0.6097281400084724    steps: 278    lr: 4e-05     evaluation reward: 4.41\n",
      "episode: 1385   score: 1.0   memory length: 297257   epsilon: 0.6094291600084789    steps: 151    lr: 4e-05     evaluation reward: 4.4\n",
      "episode: 1386   score: 5.0   memory length: 297567   epsilon: 0.6088153600084922    steps: 310    lr: 4e-05     evaluation reward: 4.42\n",
      "episode: 1387   score: 3.0   memory length: 297794   epsilon: 0.608365900008502    steps: 227    lr: 4e-05     evaluation reward: 4.38\n",
      "episode: 1388   score: 5.0   memory length: 298101   epsilon: 0.6077580400085152    steps: 307    lr: 4e-05     evaluation reward: 4.4\n",
      "episode: 1389   score: 4.0   memory length: 298363   epsilon: 0.6072392800085264    steps: 262    lr: 4e-05     evaluation reward: 4.38\n",
      "episode: 1390   score: 6.0   memory length: 298706   epsilon: 0.6065601400085412    steps: 343    lr: 4e-05     evaluation reward: 4.36\n",
      "episode: 1391   score: 2.0   memory length: 298904   epsilon: 0.6061681000085497    steps: 198    lr: 4e-05     evaluation reward: 4.35\n",
      "episode: 1392   score: 24.0   memory length: 299442   epsilon: 0.6051028600085728    steps: 538    lr: 4e-05     evaluation reward: 4.54\n",
      "episode: 1393   score: 8.0   memory length: 299886   epsilon: 0.6042237400085919    steps: 444    lr: 4e-05     evaluation reward: 4.59\n",
      "episode: 1394   score: 5.0   memory length: 300184   epsilon: 0.6036337000086047    steps: 298    lr: 1.6000000000000003e-05     evaluation reward: 4.6\n",
      "episode: 1395   score: 4.0   memory length: 300478   epsilon: 0.6030515800086174    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 4.59\n",
      "episode: 1396   score: 7.0   memory length: 300922   epsilon: 0.6021724600086364    steps: 444    lr: 1.6000000000000003e-05     evaluation reward: 4.61\n",
      "episode: 1397   score: 8.0   memory length: 301412   epsilon: 0.6012022600086575    steps: 490    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1398   score: 7.0   memory length: 301831   epsilon: 0.6003726400086755    steps: 419    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
      "episode: 1399   score: 3.0   memory length: 302041   epsilon: 0.5999568400086845    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
      "episode: 1400   score: 6.0   memory length: 302431   epsilon: 0.5991846400087013    steps: 390    lr: 1.6000000000000003e-05     evaluation reward: 4.68\n",
      "episode: 1401   score: 7.0   memory length: 302874   epsilon: 0.5983075000087203    steps: 443    lr: 1.6000000000000003e-05     evaluation reward: 4.7\n",
      "episode: 1402   score: 7.0   memory length: 303276   epsilon: 0.5975115400087376    steps: 402    lr: 1.6000000000000003e-05     evaluation reward: 4.75\n",
      "episode: 1403   score: 4.0   memory length: 303526   epsilon: 0.5970165400087484    steps: 250    lr: 1.6000000000000003e-05     evaluation reward: 4.76\n",
      "episode: 1404   score: 7.0   memory length: 303955   epsilon: 0.5961671200087668    steps: 429    lr: 1.6000000000000003e-05     evaluation reward: 4.8\n",
      "episode: 1405   score: 8.0   memory length: 304394   epsilon: 0.5952979000087857    steps: 439    lr: 1.6000000000000003e-05     evaluation reward: 4.85\n",
      "episode: 1406   score: 3.0   memory length: 304605   epsilon: 0.5948801200087948    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 4.86\n",
      "episode: 1407   score: 5.0   memory length: 304929   epsilon: 0.5942386000088087    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 4.85\n",
      "episode: 1408   score: 6.0   memory length: 305302   epsilon: 0.5935000600088247    steps: 373    lr: 1.6000000000000003e-05     evaluation reward: 4.87\n",
      "episode: 1409   score: 6.0   memory length: 305634   epsilon: 0.592842700008839    steps: 332    lr: 1.6000000000000003e-05     evaluation reward: 4.87\n",
      "episode: 1410   score: 6.0   memory length: 306007   epsilon: 0.592104160008855    steps: 373    lr: 1.6000000000000003e-05     evaluation reward: 4.89\n",
      "episode: 1411   score: 7.0   memory length: 306410   epsilon: 0.5913062200088723    steps: 403    lr: 1.6000000000000003e-05     evaluation reward: 4.9\n",
      "episode: 1412   score: 9.0   memory length: 306942   epsilon: 0.5902528600088952    steps: 532    lr: 1.6000000000000003e-05     evaluation reward: 4.94\n",
      "episode: 1413   score: 2.0   memory length: 307140   epsilon: 0.5898608200089037    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 4.96\n",
      "episode: 1414   score: 3.0   memory length: 307368   epsilon: 0.5894093800089135    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 4.95\n",
      "episode: 1415   score: 5.0   memory length: 307674   epsilon: 0.5888035000089267    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 4.99\n",
      "episode: 1416   score: 11.0   memory length: 308103   epsilon: 0.5879540800089451    steps: 429    lr: 1.6000000000000003e-05     evaluation reward: 5.08\n",
      "episode: 1417   score: 6.0   memory length: 308446   epsilon: 0.5872749400089599    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 5.11\n",
      "episode: 1418   score: 6.0   memory length: 308798   epsilon: 0.586577980008975    steps: 352    lr: 1.6000000000000003e-05     evaluation reward: 5.13\n",
      "episode: 1419   score: 7.0   memory length: 309192   epsilon: 0.5857978600089919    steps: 394    lr: 1.6000000000000003e-05     evaluation reward: 5.18\n",
      "episode: 1420   score: 4.0   memory length: 309510   epsilon: 0.5851682200090056    steps: 318    lr: 1.6000000000000003e-05     evaluation reward: 5.2\n",
      "episode: 1421   score: 4.0   memory length: 309754   epsilon: 0.5846851000090161    steps: 244    lr: 1.6000000000000003e-05     evaluation reward: 5.21\n",
      "episode: 1422   score: 7.0   memory length: 310141   epsilon: 0.5839188400090327    steps: 387    lr: 1.6000000000000003e-05     evaluation reward: 5.25\n",
      "episode: 1423   score: 3.0   memory length: 310370   epsilon: 0.5834654200090426    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 5.22\n",
      "episode: 1424   score: 10.0   memory length: 310715   epsilon: 0.5827823200090574    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 5.3\n",
      "episode: 1425   score: 3.0   memory length: 310923   epsilon: 0.5823704800090663    steps: 208    lr: 1.6000000000000003e-05     evaluation reward: 5.27\n",
      "episode: 1426   score: 6.0   memory length: 311286   epsilon: 0.5816517400090819    steps: 363    lr: 1.6000000000000003e-05     evaluation reward: 5.31\n",
      "episode: 1427   score: 7.0   memory length: 311661   epsilon: 0.580909240009098    steps: 375    lr: 1.6000000000000003e-05     evaluation reward: 5.35\n",
      "episode: 1428   score: 8.0   memory length: 312090   epsilon: 0.5800598200091165    steps: 429    lr: 1.6000000000000003e-05     evaluation reward: 5.34\n",
      "episode: 1429   score: 3.0   memory length: 312316   epsilon: 0.5796123400091262    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 5.33\n",
      "episode: 1430   score: 5.0   memory length: 312644   epsilon: 0.5789629000091403    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 5.36\n",
      "episode: 1431   score: 6.0   memory length: 313016   epsilon: 0.5782263400091563    steps: 372    lr: 1.6000000000000003e-05     evaluation reward: 5.4\n",
      "episode: 1432   score: 6.0   memory length: 313359   epsilon: 0.577547200009171    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 5.38\n",
      "episode: 1433   score: 4.0   memory length: 313640   epsilon: 0.5769908200091831    steps: 281    lr: 1.6000000000000003e-05     evaluation reward: 5.38\n",
      "episode: 1434   score: 8.0   memory length: 313988   epsilon: 0.5763017800091981    steps: 348    lr: 1.6000000000000003e-05     evaluation reward: 5.37\n",
      "episode: 1435   score: 8.0   memory length: 314440   epsilon: 0.5754068200092175    steps: 452    lr: 1.6000000000000003e-05     evaluation reward: 5.39\n",
      "episode: 1436   score: 5.0   memory length: 314725   epsilon: 0.5748425200092298    steps: 285    lr: 1.6000000000000003e-05     evaluation reward: 5.36\n",
      "episode: 1437   score: 7.0   memory length: 315168   epsilon: 0.5739653800092488    steps: 443    lr: 1.6000000000000003e-05     evaluation reward: 5.39\n",
      "episode: 1438   score: 5.0   memory length: 315513   epsilon: 0.5732822800092636    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 5.38\n",
      "episode: 1439   score: 4.0   memory length: 315773   epsilon: 0.5727674800092748    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 5.38\n",
      "episode: 1440   score: 5.0   memory length: 316100   epsilon: 0.5721200200092889    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 5.4\n",
      "episode: 1441   score: 5.0   memory length: 316403   epsilon: 0.5715200800093019    steps: 303    lr: 1.6000000000000003e-05     evaluation reward: 5.4\n",
      "episode: 1442   score: 4.0   memory length: 316695   epsilon: 0.5709419200093144    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 5.39\n",
      "episode: 1443   score: 9.0   memory length: 317174   epsilon: 0.569993500009335    steps: 479    lr: 1.6000000000000003e-05     evaluation reward: 5.42\n",
      "episode: 1444   score: 4.0   memory length: 317470   epsilon: 0.5694074200093477    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 5.43\n",
      "episode: 1445   score: 4.0   memory length: 317784   epsilon: 0.5687857000093612    steps: 314    lr: 1.6000000000000003e-05     evaluation reward: 5.46\n",
      "episode: 1446   score: 6.0   memory length: 318138   epsilon: 0.5680847800093765    steps: 354    lr: 1.6000000000000003e-05     evaluation reward: 5.49\n",
      "episode: 1447   score: 5.0   memory length: 318463   epsilon: 0.5674412800093904    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 5.46\n",
      "episode: 1448   score: 6.0   memory length: 318800   epsilon: 0.5667740200094049    steps: 337    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1449   score: 5.0   memory length: 319145   epsilon: 0.5660909200094197    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 5.46\n",
      "episode: 1450   score: 4.0   memory length: 319423   epsilon: 0.5655404800094317    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 5.47\n",
      "episode: 1451   score: 7.0   memory length: 319813   epsilon: 0.5647682800094485    steps: 390    lr: 1.6000000000000003e-05     evaluation reward: 5.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1452   score: 4.0   memory length: 320070   epsilon: 0.5642594200094595    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1453   score: 7.0   memory length: 320448   epsilon: 0.5635109800094757    steps: 378    lr: 1.6000000000000003e-05     evaluation reward: 5.49\n",
      "episode: 1454   score: 1.0   memory length: 320618   epsilon: 0.563174380009483    steps: 170    lr: 1.6000000000000003e-05     evaluation reward: 5.45\n",
      "episode: 1455   score: 4.0   memory length: 320892   epsilon: 0.5626318600094948    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 5.47\n",
      "episode: 1456   score: 8.0   memory length: 321332   epsilon: 0.5617606600095137    steps: 440    lr: 1.6000000000000003e-05     evaluation reward: 5.49\n",
      "episode: 1457   score: 3.0   memory length: 321544   epsilon: 0.5613409000095229    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 5.46\n",
      "episode: 1458   score: 10.0   memory length: 322074   epsilon: 0.5602915000095456    steps: 530    lr: 1.6000000000000003e-05     evaluation reward: 5.51\n",
      "episode: 1459   score: 6.0   memory length: 322414   epsilon: 0.5596183000095603    steps: 340    lr: 1.6000000000000003e-05     evaluation reward: 5.56\n",
      "episode: 1460   score: 3.0   memory length: 322663   epsilon: 0.559125280009571    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 5.54\n",
      "episode: 1461   score: 3.0   memory length: 322889   epsilon: 0.5586778000095807    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 5.51\n",
      "episode: 1462   score: 4.0   memory length: 323163   epsilon: 0.5581352800095924    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 5.49\n",
      "episode: 1463   score: 7.0   memory length: 323554   epsilon: 0.5573611000096093    steps: 391    lr: 1.6000000000000003e-05     evaluation reward: 5.51\n",
      "episode: 1464   score: 6.0   memory length: 323912   epsilon: 0.5566522600096246    steps: 358    lr: 1.6000000000000003e-05     evaluation reward: 5.49\n",
      "episode: 1465   score: 3.0   memory length: 324120   epsilon: 0.5562404200096336    steps: 208    lr: 1.6000000000000003e-05     evaluation reward: 5.47\n",
      "episode: 1466   score: 4.0   memory length: 324395   epsilon: 0.5556959200096454    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 5.47\n",
      "episode: 1467   score: 2.0   memory length: 324577   epsilon: 0.5553355600096532    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1468   score: 11.0   memory length: 324995   epsilon: 0.5545079200096712    steps: 418    lr: 1.6000000000000003e-05     evaluation reward: 5.53\n",
      "episode: 1469   score: 8.0   memory length: 325372   epsilon: 0.5537614600096874    steps: 377    lr: 1.6000000000000003e-05     evaluation reward: 5.55\n",
      "episode: 1470   score: 3.0   memory length: 325602   epsilon: 0.5533060600096973    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 5.52\n",
      "episode: 1471   score: 5.0   memory length: 325890   epsilon: 0.5527358200097097    steps: 288    lr: 1.6000000000000003e-05     evaluation reward: 5.54\n",
      "episode: 1472   score: 7.0   memory length: 326293   epsilon: 0.551937880009727    steps: 403    lr: 1.6000000000000003e-05     evaluation reward: 5.57\n",
      "episode: 1473   score: 9.0   memory length: 326787   epsilon: 0.5509597600097482    steps: 494    lr: 1.6000000000000003e-05     evaluation reward: 5.63\n",
      "episode: 1474   score: 4.0   memory length: 327027   epsilon: 0.5504845600097585    steps: 240    lr: 1.6000000000000003e-05     evaluation reward: 5.63\n",
      "episode: 1475   score: 5.0   memory length: 327347   epsilon: 0.5498509600097723    steps: 320    lr: 1.6000000000000003e-05     evaluation reward: 5.65\n",
      "episode: 1476   score: 6.0   memory length: 327720   epsilon: 0.5491124200097883    steps: 373    lr: 1.6000000000000003e-05     evaluation reward: 5.67\n",
      "episode: 1477   score: 5.0   memory length: 328027   epsilon: 0.5485045600098015    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 5.63\n",
      "episode: 1478   score: 3.0   memory length: 328256   epsilon: 0.5480511400098114    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 5.64\n",
      "episode: 1479   score: 6.0   memory length: 328619   epsilon: 0.547332400009827    steps: 363    lr: 1.6000000000000003e-05     evaluation reward: 5.67\n",
      "episode: 1480   score: 5.0   memory length: 328941   epsilon: 0.5466948400098408    steps: 322    lr: 1.6000000000000003e-05     evaluation reward: 5.65\n",
      "episode: 1481   score: 10.0   memory length: 329446   epsilon: 0.5456949400098625    steps: 505    lr: 1.6000000000000003e-05     evaluation reward: 5.62\n",
      "episode: 1482   score: 6.0   memory length: 329768   epsilon: 0.5450573800098764    steps: 322    lr: 1.6000000000000003e-05     evaluation reward: 5.64\n",
      "episode: 1483   score: 4.0   memory length: 330025   epsilon: 0.5445485200098874    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 5.66\n",
      "episode: 1484   score: 11.0   memory length: 330667   epsilon: 0.543277360009915    steps: 642    lr: 1.6000000000000003e-05     evaluation reward: 5.73\n",
      "episode: 1485   score: 9.0   memory length: 331173   epsilon: 0.5422754800099368    steps: 506    lr: 1.6000000000000003e-05     evaluation reward: 5.81\n",
      "episode: 1486   score: 6.0   memory length: 331538   epsilon: 0.5415527800099524    steps: 365    lr: 1.6000000000000003e-05     evaluation reward: 5.82\n",
      "episode: 1487   score: 8.0   memory length: 332000   epsilon: 0.5406380200099723    steps: 462    lr: 1.6000000000000003e-05     evaluation reward: 5.87\n",
      "episode: 1488   score: 5.0   memory length: 332325   epsilon: 0.5399945200099863    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 5.87\n",
      "episode: 1489   score: 5.0   memory length: 332634   epsilon: 0.5393827000099995    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 5.88\n",
      "episode: 1490   score: 5.0   memory length: 332924   epsilon: 0.538808500010012    steps: 290    lr: 1.6000000000000003e-05     evaluation reward: 5.87\n",
      "episode: 1491   score: 6.0   memory length: 333245   epsilon: 0.5381729200100258    steps: 321    lr: 1.6000000000000003e-05     evaluation reward: 5.91\n",
      "episode: 1492   score: 6.0   memory length: 333598   epsilon: 0.537473980010041    steps: 353    lr: 1.6000000000000003e-05     evaluation reward: 5.73\n",
      "episode: 1493   score: 4.0   memory length: 333877   epsilon: 0.536921560010053    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 5.69\n",
      "episode: 1494   score: 9.0   memory length: 334401   epsilon: 0.5358840400100755    steps: 524    lr: 1.6000000000000003e-05     evaluation reward: 5.73\n",
      "episode: 1495   score: 10.0   memory length: 334874   epsilon: 0.5349475000100958    steps: 473    lr: 1.6000000000000003e-05     evaluation reward: 5.79\n",
      "episode: 1496   score: 9.0   memory length: 335329   epsilon: 0.5340466000101154    steps: 455    lr: 1.6000000000000003e-05     evaluation reward: 5.81\n",
      "episode: 1497   score: 8.0   memory length: 335766   epsilon: 0.5331813400101342    steps: 437    lr: 1.6000000000000003e-05     evaluation reward: 5.81\n",
      "episode: 1498   score: 9.0   memory length: 336248   epsilon: 0.5322269800101549    steps: 482    lr: 1.6000000000000003e-05     evaluation reward: 5.83\n",
      "episode: 1499   score: 1.0   memory length: 336399   epsilon: 0.5319280000101614    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 5.81\n",
      "episode: 1500   score: 3.0   memory length: 336611   epsilon: 0.5315082400101705    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 5.78\n",
      "episode: 1501   score: 9.0   memory length: 337044   epsilon: 0.5306509000101891    steps: 433    lr: 1.6000000000000003e-05     evaluation reward: 5.8\n",
      "episode: 1502   score: 7.0   memory length: 337444   epsilon: 0.5298589000102063    steps: 400    lr: 1.6000000000000003e-05     evaluation reward: 5.8\n",
      "episode: 1503   score: 7.0   memory length: 337850   epsilon: 0.5290550200102238    steps: 406    lr: 1.6000000000000003e-05     evaluation reward: 5.83\n",
      "episode: 1504   score: 17.0   memory length: 338552   epsilon: 0.5276650600102539    steps: 702    lr: 1.6000000000000003e-05     evaluation reward: 5.93\n",
      "episode: 1505   score: 8.0   memory length: 338849   epsilon: 0.5270770000102667    steps: 297    lr: 1.6000000000000003e-05     evaluation reward: 5.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1506   score: 10.0   memory length: 339349   epsilon: 0.5260870000102882    steps: 500    lr: 1.6000000000000003e-05     evaluation reward: 6.0\n",
      "episode: 1507   score: 8.0   memory length: 339808   epsilon: 0.5251781800103079    steps: 459    lr: 1.6000000000000003e-05     evaluation reward: 6.03\n",
      "episode: 1508   score: 4.0   memory length: 340068   epsilon: 0.5246633800103191    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 6.01\n",
      "episode: 1509   score: 6.0   memory length: 340401   epsilon: 0.5240040400103334    steps: 333    lr: 1.6000000000000003e-05     evaluation reward: 6.01\n",
      "episode: 1510   score: 7.0   memory length: 340828   epsilon: 0.5231585800103518    steps: 427    lr: 1.6000000000000003e-05     evaluation reward: 6.02\n",
      "episode: 1511   score: 6.0   memory length: 341183   epsilon: 0.522455680010367    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 6.01\n",
      "episode: 1512   score: 4.0   memory length: 341441   epsilon: 0.5219448400103781    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 5.96\n",
      "episode: 1513   score: 7.0   memory length: 341848   epsilon: 0.5211389800103956    steps: 407    lr: 1.6000000000000003e-05     evaluation reward: 6.01\n",
      "episode: 1514   score: 9.0   memory length: 342351   epsilon: 0.5201430400104172    steps: 503    lr: 1.6000000000000003e-05     evaluation reward: 6.07\n",
      "episode: 1515   score: 11.0   memory length: 342841   epsilon: 0.5191728400104383    steps: 490    lr: 1.6000000000000003e-05     evaluation reward: 6.13\n",
      "episode: 1516   score: 9.0   memory length: 343300   epsilon: 0.518264020010458    steps: 459    lr: 1.6000000000000003e-05     evaluation reward: 6.11\n",
      "episode: 1517   score: 7.0   memory length: 343709   epsilon: 0.5174542000104756    steps: 409    lr: 1.6000000000000003e-05     evaluation reward: 6.12\n",
      "episode: 1518   score: 6.0   memory length: 344059   epsilon: 0.5167612000104906    steps: 350    lr: 1.6000000000000003e-05     evaluation reward: 6.12\n",
      "episode: 1519   score: 9.0   memory length: 344540   epsilon: 0.5158088200105113    steps: 481    lr: 1.6000000000000003e-05     evaluation reward: 6.14\n",
      "episode: 1520   score: 5.0   memory length: 344862   epsilon: 0.5151712600105252    steps: 322    lr: 1.6000000000000003e-05     evaluation reward: 6.15\n",
      "episode: 1521   score: 5.0   memory length: 345190   epsilon: 0.5145218200105393    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 6.16\n",
      "episode: 1522   score: 7.0   memory length: 345617   epsilon: 0.5136763600105576    steps: 427    lr: 1.6000000000000003e-05     evaluation reward: 6.16\n",
      "episode: 1523   score: 4.0   memory length: 345856   epsilon: 0.5132031400105679    steps: 239    lr: 1.6000000000000003e-05     evaluation reward: 6.17\n",
      "episode: 1524   score: 8.0   memory length: 346276   epsilon: 0.5123715400105859    steps: 420    lr: 1.6000000000000003e-05     evaluation reward: 6.15\n",
      "episode: 1525   score: 13.0   memory length: 346751   epsilon: 0.5114310400106064    steps: 475    lr: 1.6000000000000003e-05     evaluation reward: 6.25\n",
      "episode: 1526   score: 7.0   memory length: 347169   epsilon: 0.5106034000106243    steps: 418    lr: 1.6000000000000003e-05     evaluation reward: 6.26\n",
      "episode: 1527   score: 3.0   memory length: 347420   epsilon: 0.5101064200106351    steps: 251    lr: 1.6000000000000003e-05     evaluation reward: 6.22\n",
      "episode: 1528   score: 7.0   memory length: 347796   epsilon: 0.5093619400106513    steps: 376    lr: 1.6000000000000003e-05     evaluation reward: 6.21\n",
      "episode: 1529   score: 5.0   memory length: 348103   epsilon: 0.5087540800106645    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 6.23\n",
      "episode: 1530   score: 8.0   memory length: 348562   epsilon: 0.5078452600106842    steps: 459    lr: 1.6000000000000003e-05     evaluation reward: 6.26\n",
      "episode: 1531   score: 3.0   memory length: 348809   epsilon: 0.5073562000106948    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 6.23\n",
      "episode: 1532   score: 1.0   memory length: 348960   epsilon: 0.5070572200107013    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 6.18\n",
      "episode: 1533   score: 5.0   memory length: 349286   epsilon: 0.5064117400107153    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 6.19\n",
      "episode: 1534   score: 4.0   memory length: 349543   epsilon: 0.5059028800107264    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 6.15\n",
      "episode: 1535   score: 6.0   memory length: 349902   epsilon: 0.5051920600107418    steps: 359    lr: 1.6000000000000003e-05     evaluation reward: 6.13\n",
      "episode: 1536   score: 5.0   memory length: 350249   epsilon: 0.5045050000107567    steps: 347    lr: 1.6000000000000003e-05     evaluation reward: 6.13\n",
      "episode: 1537   score: 4.0   memory length: 350523   epsilon: 0.5039624800107685    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 6.1\n",
      "episode: 1538   score: 4.0   memory length: 350800   epsilon: 0.5034140200107804    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 6.09\n",
      "episode: 1539   score: 3.0   memory length: 351049   epsilon: 0.5029210000107911    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 6.08\n",
      "episode: 1540   score: 4.0   memory length: 351327   epsilon: 0.502370560010803    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 6.07\n",
      "episode: 1541   score: 16.0   memory length: 351914   epsilon: 0.5012083000108283    steps: 587    lr: 1.6000000000000003e-05     evaluation reward: 6.18\n",
      "episode: 1542   score: 11.0   memory length: 352458   epsilon: 0.5001311800108517    steps: 544    lr: 1.6000000000000003e-05     evaluation reward: 6.25\n",
      "episode: 1543   score: 7.0   memory length: 352866   epsilon: 0.4993233400108502    steps: 408    lr: 1.6000000000000003e-05     evaluation reward: 6.23\n",
      "episode: 1544   score: 5.0   memory length: 353152   epsilon: 0.49875706001084663    steps: 286    lr: 1.6000000000000003e-05     evaluation reward: 6.24\n",
      "episode: 1545   score: 6.0   memory length: 353518   epsilon: 0.49803238001084205    steps: 366    lr: 1.6000000000000003e-05     evaluation reward: 6.26\n",
      "episode: 1546   score: 5.0   memory length: 353823   epsilon: 0.4974284800108382    steps: 305    lr: 1.6000000000000003e-05     evaluation reward: 6.25\n",
      "episode: 1547   score: 6.0   memory length: 354163   epsilon: 0.49675528001083397    steps: 340    lr: 1.6000000000000003e-05     evaluation reward: 6.26\n",
      "episode: 1548   score: 3.0   memory length: 354373   epsilon: 0.49633948001083134    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 6.23\n",
      "episode: 1549   score: 9.0   memory length: 354842   epsilon: 0.49541086001082546    steps: 469    lr: 1.6000000000000003e-05     evaluation reward: 6.27\n",
      "episode: 1550   score: 8.0   memory length: 355298   epsilon: 0.49450798001081975    steps: 456    lr: 1.6000000000000003e-05     evaluation reward: 6.31\n",
      "episode: 1551   score: 7.0   memory length: 355719   epsilon: 0.4936744000108145    steps: 421    lr: 1.6000000000000003e-05     evaluation reward: 6.31\n",
      "episode: 1552   score: 8.0   memory length: 356155   epsilon: 0.492811120010809    steps: 436    lr: 1.6000000000000003e-05     evaluation reward: 6.35\n",
      "episode: 1553   score: 6.0   memory length: 356495   epsilon: 0.49213792001080475    steps: 340    lr: 1.6000000000000003e-05     evaluation reward: 6.34\n",
      "episode: 1554   score: 4.0   memory length: 356753   epsilon: 0.4916270800108015    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 6.37\n",
      "episode: 1555   score: 6.0   memory length: 357091   epsilon: 0.4909578400107973    steps: 338    lr: 1.6000000000000003e-05     evaluation reward: 6.39\n",
      "episode: 1556   score: 8.0   memory length: 357489   epsilon: 0.4901698000107923    steps: 398    lr: 1.6000000000000003e-05     evaluation reward: 6.39\n",
      "episode: 1557   score: 10.0   memory length: 357947   epsilon: 0.48926296001078656    steps: 458    lr: 1.6000000000000003e-05     evaluation reward: 6.46\n",
      "episode: 1558   score: 4.0   memory length: 358202   epsilon: 0.48875806001078337    steps: 255    lr: 1.6000000000000003e-05     evaluation reward: 6.4\n",
      "episode: 1559   score: 7.0   memory length: 358546   epsilon: 0.48807694001077906    steps: 344    lr: 1.6000000000000003e-05     evaluation reward: 6.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1560   score: 5.0   memory length: 358854   epsilon: 0.4874671000107752    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 6.43\n",
      "episode: 1561   score: 2.0   memory length: 359034   epsilon: 0.48711070001077295    steps: 180    lr: 1.6000000000000003e-05     evaluation reward: 6.42\n",
      "episode: 1562   score: 7.0   memory length: 359437   epsilon: 0.4863127600107679    steps: 403    lr: 1.6000000000000003e-05     evaluation reward: 6.45\n",
      "episode: 1563   score: 6.0   memory length: 359739   epsilon: 0.4857148000107641    steps: 302    lr: 1.6000000000000003e-05     evaluation reward: 6.44\n",
      "episode: 1564   score: 6.0   memory length: 360094   epsilon: 0.48501190001075967    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 6.44\n",
      "episode: 1565   score: 3.0   memory length: 360303   epsilon: 0.48459808001075705    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 6.44\n",
      "episode: 1566   score: 4.0   memory length: 360566   epsilon: 0.48407734001075375    steps: 263    lr: 1.6000000000000003e-05     evaluation reward: 6.44\n",
      "episode: 1567   score: 4.0   memory length: 360844   epsilon: 0.48352690001075027    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 6.46\n",
      "episode: 1568   score: 7.0   memory length: 361228   epsilon: 0.48276658001074546    steps: 384    lr: 1.6000000000000003e-05     evaluation reward: 6.42\n",
      "episode: 1569   score: 6.0   memory length: 361546   epsilon: 0.4821369400107415    steps: 318    lr: 1.6000000000000003e-05     evaluation reward: 6.4\n",
      "episode: 1570   score: 7.0   memory length: 361932   epsilon: 0.48137266001073664    steps: 386    lr: 1.6000000000000003e-05     evaluation reward: 6.44\n",
      "episode: 1571   score: 16.0   memory length: 362601   epsilon: 0.48004804001072826    steps: 669    lr: 1.6000000000000003e-05     evaluation reward: 6.55\n",
      "episode: 1572   score: 3.0   memory length: 362814   epsilon: 0.4796263000107256    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 6.51\n",
      "episode: 1573   score: 6.0   memory length: 363152   epsilon: 0.47895706001072136    steps: 338    lr: 1.6000000000000003e-05     evaluation reward: 6.48\n",
      "episode: 1574   score: 4.0   memory length: 363415   epsilon: 0.47843632001071806    steps: 263    lr: 1.6000000000000003e-05     evaluation reward: 6.48\n",
      "episode: 1575   score: 9.0   memory length: 363900   epsilon: 0.477476020010712    steps: 485    lr: 1.6000000000000003e-05     evaluation reward: 6.52\n",
      "episode: 1576   score: 7.0   memory length: 364295   epsilon: 0.47669392001070704    steps: 395    lr: 1.6000000000000003e-05     evaluation reward: 6.53\n",
      "episode: 1577   score: 6.0   memory length: 364651   epsilon: 0.4759890400107026    steps: 356    lr: 1.6000000000000003e-05     evaluation reward: 6.54\n",
      "episode: 1578   score: 10.0   memory length: 365169   epsilon: 0.4749634000106961    steps: 518    lr: 1.6000000000000003e-05     evaluation reward: 6.61\n",
      "episode: 1579   score: 7.0   memory length: 365559   epsilon: 0.4741912000106912    steps: 390    lr: 1.6000000000000003e-05     evaluation reward: 6.62\n",
      "episode: 1580   score: 4.0   memory length: 365819   epsilon: 0.47367640001068795    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 6.61\n",
      "episode: 1581   score: 3.0   memory length: 366050   epsilon: 0.47321902001068505    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 6.54\n",
      "episode: 1582   score: 5.0   memory length: 366358   epsilon: 0.4726091800106812    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 6.53\n",
      "episode: 1583   score: 7.0   memory length: 366741   epsilon: 0.4718508400106764    steps: 383    lr: 1.6000000000000003e-05     evaluation reward: 6.56\n",
      "episode: 1584   score: 8.0   memory length: 367192   epsilon: 0.47095786001067075    steps: 451    lr: 1.6000000000000003e-05     evaluation reward: 6.53\n",
      "episode: 1585   score: 7.0   memory length: 367560   epsilon: 0.47022922001066614    steps: 368    lr: 1.6000000000000003e-05     evaluation reward: 6.51\n",
      "episode: 1586   score: 13.0   memory length: 368116   epsilon: 0.4691283400106592    steps: 556    lr: 1.6000000000000003e-05     evaluation reward: 6.58\n",
      "episode: 1587   score: 4.0   memory length: 368395   epsilon: 0.4685759200106557    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 6.54\n",
      "episode: 1588   score: 7.0   memory length: 368787   epsilon: 0.46779976001065077    steps: 392    lr: 1.6000000000000003e-05     evaluation reward: 6.56\n",
      "episode: 1589   score: 7.0   memory length: 369150   epsilon: 0.4670810200106462    steps: 363    lr: 1.6000000000000003e-05     evaluation reward: 6.58\n",
      "episode: 1590   score: 4.0   memory length: 369408   epsilon: 0.466570180010643    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 6.57\n",
      "episode: 1591   score: 5.0   memory length: 369749   epsilon: 0.4658950000106387    steps: 341    lr: 1.6000000000000003e-05     evaluation reward: 6.56\n",
      "episode: 1592   score: 5.0   memory length: 370058   epsilon: 0.46528318001063484    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 6.55\n",
      "episode: 1593   score: 9.0   memory length: 370496   epsilon: 0.46441594001062936    steps: 438    lr: 1.6000000000000003e-05     evaluation reward: 6.6\n",
      "episode: 1594   score: 3.0   memory length: 370727   epsilon: 0.46395856001062646    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 6.54\n",
      "episode: 1595   score: 11.0   memory length: 371289   epsilon: 0.4628458000106194    steps: 562    lr: 1.6000000000000003e-05     evaluation reward: 6.55\n",
      "episode: 1596   score: 4.0   memory length: 371550   epsilon: 0.46232902001061615    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 6.5\n",
      "episode: 1597   score: 3.0   memory length: 371760   epsilon: 0.4619132200106135    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 6.45\n",
      "episode: 1598   score: 2.0   memory length: 371940   epsilon: 0.46155682001061127    steps: 180    lr: 1.6000000000000003e-05     evaluation reward: 6.38\n",
      "episode: 1599   score: 3.0   memory length: 372188   epsilon: 0.46106578001060816    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 6.4\n",
      "episode: 1600   score: 6.0   memory length: 372527   epsilon: 0.4603945600106039    steps: 339    lr: 1.6000000000000003e-05     evaluation reward: 6.43\n",
      "episode: 1601   score: 6.0   memory length: 372862   epsilon: 0.4597312600105997    steps: 335    lr: 1.6000000000000003e-05     evaluation reward: 6.4\n",
      "episode: 1602   score: 6.0   memory length: 373184   epsilon: 0.4590937000105957    steps: 322    lr: 1.6000000000000003e-05     evaluation reward: 6.39\n",
      "episode: 1603   score: 11.0   memory length: 373755   epsilon: 0.45796312001058853    steps: 571    lr: 1.6000000000000003e-05     evaluation reward: 6.43\n",
      "episode: 1604   score: 3.0   memory length: 373986   epsilon: 0.45750574001058564    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 6.29\n",
      "episode: 1605   score: 10.0   memory length: 374543   epsilon: 0.45640288001057866    steps: 557    lr: 1.6000000000000003e-05     evaluation reward: 6.31\n",
      "episode: 1606   score: 5.0   memory length: 374849   epsilon: 0.4557970000105748    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 6.26\n",
      "episode: 1607   score: 5.0   memory length: 375174   epsilon: 0.45515350001057076    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 6.23\n",
      "episode: 1608   score: 5.0   memory length: 375500   epsilon: 0.45450802001056667    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 6.24\n",
      "episode: 1609   score: 8.0   memory length: 375920   epsilon: 0.4536764200105614    steps: 420    lr: 1.6000000000000003e-05     evaluation reward: 6.26\n",
      "episode: 1610   score: 13.0   memory length: 376402   epsilon: 0.45272206001055537    steps: 482    lr: 1.6000000000000003e-05     evaluation reward: 6.32\n",
      "episode: 1611   score: 7.0   memory length: 376768   epsilon: 0.4519973800105508    steps: 366    lr: 1.6000000000000003e-05     evaluation reward: 6.33\n",
      "episode: 1612   score: 12.0   memory length: 377339   epsilon: 0.45086680001054363    steps: 571    lr: 1.6000000000000003e-05     evaluation reward: 6.41\n",
      "episode: 1613   score: 4.0   memory length: 377578   epsilon: 0.45039358001054064    steps: 239    lr: 1.6000000000000003e-05     evaluation reward: 6.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1614   score: 10.0   memory length: 378061   epsilon: 0.4494372400105346    steps: 483    lr: 1.6000000000000003e-05     evaluation reward: 6.39\n",
      "episode: 1615   score: 9.0   memory length: 378563   epsilon: 0.4484432800105283    steps: 502    lr: 1.6000000000000003e-05     evaluation reward: 6.37\n",
      "episode: 1616   score: 8.0   memory length: 378979   epsilon: 0.4476196000105231    steps: 416    lr: 1.6000000000000003e-05     evaluation reward: 6.36\n",
      "episode: 1617   score: 5.0   memory length: 379321   epsilon: 0.4469424400105188    steps: 342    lr: 1.6000000000000003e-05     evaluation reward: 6.34\n",
      "episode: 1618   score: 5.0   memory length: 379612   epsilon: 0.44636626001051516    steps: 291    lr: 1.6000000000000003e-05     evaluation reward: 6.33\n",
      "episode: 1619   score: 4.0   memory length: 379907   epsilon: 0.44578216001051146    steps: 295    lr: 1.6000000000000003e-05     evaluation reward: 6.28\n",
      "episode: 1620   score: 5.0   memory length: 380235   epsilon: 0.44513272001050735    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 6.28\n",
      "episode: 1621   score: 2.0   memory length: 380434   epsilon: 0.44473870001050486    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 6.25\n",
      "episode: 1622   score: 6.0   memory length: 380755   epsilon: 0.44410312001050084    steps: 321    lr: 1.6000000000000003e-05     evaluation reward: 6.24\n",
      "episode: 1623   score: 8.0   memory length: 381162   epsilon: 0.44329726001049574    steps: 407    lr: 1.6000000000000003e-05     evaluation reward: 6.28\n",
      "episode: 1624   score: 7.0   memory length: 381541   epsilon: 0.442546840010491    steps: 379    lr: 1.6000000000000003e-05     evaluation reward: 6.27\n",
      "episode: 1625   score: 8.0   memory length: 381941   epsilon: 0.441754840010486    steps: 400    lr: 1.6000000000000003e-05     evaluation reward: 6.22\n",
      "episode: 1626   score: 8.0   memory length: 382348   epsilon: 0.4409489800104809    steps: 407    lr: 1.6000000000000003e-05     evaluation reward: 6.23\n",
      "episode: 1627   score: 7.0   memory length: 382767   epsilon: 0.44011936001047564    steps: 419    lr: 1.6000000000000003e-05     evaluation reward: 6.27\n",
      "episode: 1628   score: 7.0   memory length: 383151   epsilon: 0.4393590400104708    steps: 384    lr: 1.6000000000000003e-05     evaluation reward: 6.27\n",
      "episode: 1629   score: 15.0   memory length: 383665   epsilon: 0.4383413200104644    steps: 514    lr: 1.6000000000000003e-05     evaluation reward: 6.37\n",
      "episode: 1630   score: 4.0   memory length: 383907   epsilon: 0.43786216001046135    steps: 242    lr: 1.6000000000000003e-05     evaluation reward: 6.33\n",
      "episode: 1631   score: 5.0   memory length: 384235   epsilon: 0.43721272001045725    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 6.35\n",
      "episode: 1632   score: 5.0   memory length: 384547   epsilon: 0.43659496001045334    steps: 312    lr: 1.6000000000000003e-05     evaluation reward: 6.39\n",
      "episode: 1633   score: 10.0   memory length: 385051   epsilon: 0.435597040010447    steps: 504    lr: 1.6000000000000003e-05     evaluation reward: 6.44\n",
      "episode: 1634   score: 8.0   memory length: 385493   epsilon: 0.4347218800104415    steps: 442    lr: 1.6000000000000003e-05     evaluation reward: 6.48\n",
      "episode: 1635   score: 5.0   memory length: 385819   epsilon: 0.4340764000104374    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 6.47\n",
      "episode: 1636   score: 11.0   memory length: 386387   epsilon: 0.4329517600104303    steps: 568    lr: 1.6000000000000003e-05     evaluation reward: 6.53\n",
      "episode: 1637   score: 9.0   memory length: 386879   epsilon: 0.4319776000104241    steps: 492    lr: 1.6000000000000003e-05     evaluation reward: 6.58\n",
      "episode: 1638   score: 7.0   memory length: 387284   epsilon: 0.43117570001041905    steps: 405    lr: 1.6000000000000003e-05     evaluation reward: 6.61\n",
      "episode: 1639   score: 6.0   memory length: 387641   epsilon: 0.4304688400104146    steps: 357    lr: 1.6000000000000003e-05     evaluation reward: 6.64\n",
      "episode: 1640   score: 4.0   memory length: 387918   epsilon: 0.4299203800104111    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 6.64\n",
      "episode: 1641   score: 4.0   memory length: 388159   epsilon: 0.4294432000104081    steps: 241    lr: 1.6000000000000003e-05     evaluation reward: 6.52\n",
      "episode: 1642   score: 3.0   memory length: 388405   epsilon: 0.428956120010405    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 6.44\n",
      "episode: 1643   score: 7.0   memory length: 388810   epsilon: 0.42815422001039993    steps: 405    lr: 1.6000000000000003e-05     evaluation reward: 6.44\n",
      "episode: 1644   score: 4.0   memory length: 389085   epsilon: 0.4276097200103965    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 6.43\n",
      "episode: 1645   score: 5.0   memory length: 389412   epsilon: 0.4269622600103924    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 6.42\n",
      "episode: 1646   score: 5.0   memory length: 389703   epsilon: 0.42638608001038875    steps: 291    lr: 1.6000000000000003e-05     evaluation reward: 6.42\n",
      "episode: 1647   score: 8.0   memory length: 390095   epsilon: 0.42560992001038384    steps: 392    lr: 1.6000000000000003e-05     evaluation reward: 6.44\n",
      "episode: 1648   score: 8.0   memory length: 390518   epsilon: 0.42477238001037854    steps: 423    lr: 1.6000000000000003e-05     evaluation reward: 6.49\n",
      "episode: 1649   score: 6.0   memory length: 390856   epsilon: 0.4241031400103743    steps: 338    lr: 1.6000000000000003e-05     evaluation reward: 6.46\n",
      "episode: 1650   score: 7.0   memory length: 391262   epsilon: 0.4232992600103692    steps: 406    lr: 1.6000000000000003e-05     evaluation reward: 6.45\n",
      "episode: 1651   score: 16.0   memory length: 391719   epsilon: 0.4223944000103635    steps: 457    lr: 1.6000000000000003e-05     evaluation reward: 6.54\n",
      "episode: 1652   score: 3.0   memory length: 391929   epsilon: 0.42197860001036086    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 6.49\n",
      "episode: 1653   score: 5.0   memory length: 392235   epsilon: 0.421372720010357    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 6.48\n",
      "episode: 1654   score: 10.0   memory length: 392775   epsilon: 0.42030352001035026    steps: 540    lr: 1.6000000000000003e-05     evaluation reward: 6.54\n",
      "episode: 1655   score: 3.0   memory length: 393005   epsilon: 0.4198481200103474    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 6.51\n",
      "episode: 1656   score: 5.0   memory length: 393314   epsilon: 0.4192363000103435    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 6.48\n",
      "episode: 1657   score: 6.0   memory length: 393660   epsilon: 0.4185512200103392    steps: 346    lr: 1.6000000000000003e-05     evaluation reward: 6.44\n",
      "episode: 1658   score: 15.0   memory length: 394264   epsilon: 0.4173553000103316    steps: 604    lr: 1.6000000000000003e-05     evaluation reward: 6.55\n",
      "episode: 1659   score: 8.0   memory length: 394716   epsilon: 0.41646034001032595    steps: 452    lr: 1.6000000000000003e-05     evaluation reward: 6.56\n",
      "episode: 1660   score: 4.0   memory length: 394976   epsilon: 0.4159455400103227    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 6.55\n",
      "episode: 1661   score: 6.0   memory length: 395316   epsilon: 0.41527234001031843    steps: 340    lr: 1.6000000000000003e-05     evaluation reward: 6.59\n",
      "episode: 1662   score: 6.0   memory length: 395706   epsilon: 0.41450014001031354    steps: 390    lr: 1.6000000000000003e-05     evaluation reward: 6.58\n",
      "episode: 1663   score: 5.0   memory length: 395993   epsilon: 0.41393188001030995    steps: 287    lr: 1.6000000000000003e-05     evaluation reward: 6.57\n",
      "episode: 1664   score: 7.0   memory length: 396416   epsilon: 0.41309434001030465    steps: 423    lr: 1.6000000000000003e-05     evaluation reward: 6.58\n",
      "episode: 1665   score: 10.0   memory length: 396902   epsilon: 0.41213206001029856    steps: 486    lr: 1.6000000000000003e-05     evaluation reward: 6.65\n",
      "episode: 1666   score: 7.0   memory length: 397306   epsilon: 0.4113321400102935    steps: 404    lr: 1.6000000000000003e-05     evaluation reward: 6.68\n",
      "episode: 1667   score: 4.0   memory length: 397566   epsilon: 0.41081734001029024    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 6.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1668   score: 4.0   memory length: 397808   epsilon: 0.4103381800102872    steps: 242    lr: 1.6000000000000003e-05     evaluation reward: 6.65\n",
      "episode: 1669   score: 3.0   memory length: 398019   epsilon: 0.40992040001028457    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 6.62\n",
      "episode: 1670   score: 5.0   memory length: 398326   epsilon: 0.4093125400102807    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 6.6\n",
      "episode: 1671   score: 10.0   memory length: 398867   epsilon: 0.40824136001027395    steps: 541    lr: 1.6000000000000003e-05     evaluation reward: 6.54\n",
      "episode: 1672   score: 6.0   memory length: 399204   epsilon: 0.4075741000102697    steps: 337    lr: 1.6000000000000003e-05     evaluation reward: 6.57\n",
      "episode: 1673   score: 11.0   memory length: 399732   epsilon: 0.4065286600102631    steps: 528    lr: 1.6000000000000003e-05     evaluation reward: 6.62\n",
      "episode: 1674   score: 4.0   memory length: 399974   epsilon: 0.4060495000102601    steps: 242    lr: 1.6000000000000003e-05     evaluation reward: 6.62\n",
      "episode: 1675   score: 6.0   memory length: 400320   epsilon: 0.40536442001025574    steps: 346    lr: 6.400000000000001e-06     evaluation reward: 6.59\n",
      "episode: 1676   score: 10.0   memory length: 400897   epsilon: 0.4042219600102485    steps: 577    lr: 6.400000000000001e-06     evaluation reward: 6.62\n",
      "episode: 1677   score: 10.0   memory length: 401389   epsilon: 0.40324780001024235    steps: 492    lr: 6.400000000000001e-06     evaluation reward: 6.66\n",
      "episode: 1678   score: 8.0   memory length: 401762   epsilon: 0.4025092600102377    steps: 373    lr: 6.400000000000001e-06     evaluation reward: 6.64\n",
      "episode: 1679   score: 4.0   memory length: 402043   epsilon: 0.40195288001023416    steps: 281    lr: 6.400000000000001e-06     evaluation reward: 6.61\n",
      "episode: 1680   score: 7.0   memory length: 402452   epsilon: 0.40114306001022904    steps: 409    lr: 6.400000000000001e-06     evaluation reward: 6.64\n",
      "episode: 1681   score: 11.0   memory length: 402943   epsilon: 0.4001708800102229    steps: 491    lr: 6.400000000000001e-06     evaluation reward: 6.72\n",
      "episode: 1682   score: 14.0   memory length: 403484   epsilon: 0.3990997000102161    steps: 541    lr: 6.400000000000001e-06     evaluation reward: 6.81\n",
      "episode: 1683   score: 8.0   memory length: 403939   epsilon: 0.3981988000102104    steps: 455    lr: 6.400000000000001e-06     evaluation reward: 6.82\n",
      "episode: 1684   score: 5.0   memory length: 404244   epsilon: 0.3975949000102066    steps: 305    lr: 6.400000000000001e-06     evaluation reward: 6.79\n",
      "episode: 1685   score: 8.0   memory length: 404694   epsilon: 0.39670390001020095    steps: 450    lr: 6.400000000000001e-06     evaluation reward: 6.8\n",
      "episode: 1686   score: 9.0   memory length: 405151   epsilon: 0.3957990400101952    steps: 457    lr: 6.400000000000001e-06     evaluation reward: 6.76\n",
      "episode: 1687   score: 17.0   memory length: 405839   epsilon: 0.3944368000101866    steps: 688    lr: 6.400000000000001e-06     evaluation reward: 6.89\n",
      "episode: 1688   score: 4.0   memory length: 406080   epsilon: 0.3939596200101836    steps: 241    lr: 6.400000000000001e-06     evaluation reward: 6.86\n",
      "episode: 1689   score: 12.0   memory length: 406637   epsilon: 0.3928567600101766    steps: 557    lr: 6.400000000000001e-06     evaluation reward: 6.91\n",
      "episode: 1690   score: 3.0   memory length: 406866   epsilon: 0.39240334001017374    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 6.9\n",
      "episode: 1691   score: 3.0   memory length: 407076   epsilon: 0.3919875400101711    steps: 210    lr: 6.400000000000001e-06     evaluation reward: 6.88\n",
      "episode: 1692   score: 3.0   memory length: 407286   epsilon: 0.3915717400101685    steps: 210    lr: 6.400000000000001e-06     evaluation reward: 6.86\n",
      "episode: 1693   score: 5.0   memory length: 407613   epsilon: 0.3909242800101644    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 6.82\n",
      "episode: 1694   score: 8.0   memory length: 408083   epsilon: 0.3899936800101585    steps: 470    lr: 6.400000000000001e-06     evaluation reward: 6.87\n",
      "episode: 1695   score: 5.0   memory length: 408412   epsilon: 0.3893422600101544    steps: 329    lr: 6.400000000000001e-06     evaluation reward: 6.81\n",
      "episode: 1696   score: 17.0   memory length: 409102   epsilon: 0.38797606001014573    steps: 690    lr: 6.400000000000001e-06     evaluation reward: 6.94\n",
      "episode: 1697   score: 8.0   memory length: 409573   epsilon: 0.38704348001013983    steps: 471    lr: 6.400000000000001e-06     evaluation reward: 6.99\n",
      "episode: 1698   score: 10.0   memory length: 410054   epsilon: 0.3860911000101338    steps: 481    lr: 6.400000000000001e-06     evaluation reward: 7.07\n",
      "episode: 1699   score: 9.0   memory length: 410548   epsilon: 0.3851129800101276    steps: 494    lr: 6.400000000000001e-06     evaluation reward: 7.13\n",
      "episode: 1700   score: 6.0   memory length: 410911   epsilon: 0.38439424001012307    steps: 363    lr: 6.400000000000001e-06     evaluation reward: 7.13\n",
      "episode: 1701   score: 7.0   memory length: 411322   epsilon: 0.3835804600101179    steps: 411    lr: 6.400000000000001e-06     evaluation reward: 7.14\n",
      "episode: 1702   score: 9.0   memory length: 411825   epsilon: 0.3825845200101116    steps: 503    lr: 6.400000000000001e-06     evaluation reward: 7.17\n",
      "episode: 1703   score: 6.0   memory length: 412201   epsilon: 0.3818400400101069    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 7.12\n",
      "episode: 1704   score: 6.0   memory length: 412547   epsilon: 0.3811549600101026    steps: 346    lr: 6.400000000000001e-06     evaluation reward: 7.15\n",
      "episode: 1705   score: 11.0   memory length: 413017   epsilon: 0.3802243600100967    steps: 470    lr: 6.400000000000001e-06     evaluation reward: 7.16\n",
      "episode: 1706   score: 3.0   memory length: 413265   epsilon: 0.3797333200100936    steps: 248    lr: 6.400000000000001e-06     evaluation reward: 7.14\n",
      "episode: 1707   score: 4.0   memory length: 413524   epsilon: 0.37922050001009033    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 7.13\n",
      "episode: 1708   score: 7.0   memory length: 413930   epsilon: 0.37841662001008525    steps: 406    lr: 6.400000000000001e-06     evaluation reward: 7.15\n",
      "episode: 1709   score: 9.0   memory length: 414388   epsilon: 0.3775097800100795    steps: 458    lr: 6.400000000000001e-06     evaluation reward: 7.16\n",
      "episode: 1710   score: 2.0   memory length: 414588   epsilon: 0.377113780010077    steps: 200    lr: 6.400000000000001e-06     evaluation reward: 7.05\n",
      "episode: 1711   score: 6.0   memory length: 414921   epsilon: 0.37645444001007283    steps: 333    lr: 6.400000000000001e-06     evaluation reward: 7.04\n",
      "episode: 1712   score: 9.0   memory length: 415393   epsilon: 0.3755198800100669    steps: 472    lr: 6.400000000000001e-06     evaluation reward: 7.01\n",
      "episode: 1713   score: 10.0   memory length: 415784   epsilon: 0.374745700010062    steps: 391    lr: 6.400000000000001e-06     evaluation reward: 7.07\n",
      "episode: 1714   score: 8.0   memory length: 416206   epsilon: 0.37391014001005674    steps: 422    lr: 6.400000000000001e-06     evaluation reward: 7.05\n",
      "episode: 1715   score: 6.0   memory length: 416560   epsilon: 0.3732092200100523    steps: 354    lr: 6.400000000000001e-06     evaluation reward: 7.02\n",
      "episode: 1716   score: 8.0   memory length: 417016   epsilon: 0.3723063400100466    steps: 456    lr: 6.400000000000001e-06     evaluation reward: 7.02\n",
      "episode: 1717   score: 6.0   memory length: 417394   epsilon: 0.37155790001004185    steps: 378    lr: 6.400000000000001e-06     evaluation reward: 7.03\n",
      "episode: 1718   score: 8.0   memory length: 417817   epsilon: 0.37072036001003655    steps: 423    lr: 6.400000000000001e-06     evaluation reward: 7.06\n",
      "episode: 1719   score: 10.0   memory length: 418273   epsilon: 0.36981748001003084    steps: 456    lr: 6.400000000000001e-06     evaluation reward: 7.12\n",
      "episode: 1720   score: 11.0   memory length: 418803   epsilon: 0.3687680800100242    steps: 530    lr: 6.400000000000001e-06     evaluation reward: 7.18\n",
      "episode: 1721   score: 11.0   memory length: 419363   epsilon: 0.3676592800100172    steps: 560    lr: 6.400000000000001e-06     evaluation reward: 7.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1722   score: 10.0   memory length: 419890   epsilon: 0.3666158200100106    steps: 527    lr: 6.400000000000001e-06     evaluation reward: 7.31\n",
      "episode: 1723   score: 11.0   memory length: 420413   epsilon: 0.36558028001000403    steps: 523    lr: 6.400000000000001e-06     evaluation reward: 7.34\n",
      "episode: 1724   score: 5.0   memory length: 420688   epsilon: 0.3650357800100006    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 7.32\n",
      "episode: 1725   score: 5.0   memory length: 420980   epsilon: 0.36445762000999693    steps: 292    lr: 6.400000000000001e-06     evaluation reward: 7.29\n",
      "episode: 1726   score: 7.0   memory length: 421364   epsilon: 0.3636973000099921    steps: 384    lr: 6.400000000000001e-06     evaluation reward: 7.28\n",
      "episode: 1727   score: 9.0   memory length: 421810   epsilon: 0.36281422000998653    steps: 446    lr: 6.400000000000001e-06     evaluation reward: 7.3\n",
      "episode: 1728   score: 7.0   memory length: 422195   epsilon: 0.3620519200099817    steps: 385    lr: 6.400000000000001e-06     evaluation reward: 7.3\n",
      "episode: 1729   score: 7.0   memory length: 422644   epsilon: 0.3611629000099761    steps: 449    lr: 6.400000000000001e-06     evaluation reward: 7.22\n",
      "episode: 1730   score: 4.0   memory length: 422903   epsilon: 0.36065008000997284    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 7.22\n",
      "episode: 1731   score: 11.0   memory length: 423422   epsilon: 0.35962246000996634    steps: 519    lr: 6.400000000000001e-06     evaluation reward: 7.28\n",
      "episode: 1732   score: 8.0   memory length: 423833   epsilon: 0.3588086800099612    steps: 411    lr: 6.400000000000001e-06     evaluation reward: 7.31\n",
      "episode: 1733   score: 9.0   memory length: 424290   epsilon: 0.35790382000995546    steps: 457    lr: 6.400000000000001e-06     evaluation reward: 7.3\n",
      "episode: 1734   score: 13.0   memory length: 424851   epsilon: 0.35679304000994844    steps: 561    lr: 6.400000000000001e-06     evaluation reward: 7.35\n",
      "episode: 1735   score: 10.0   memory length: 425307   epsilon: 0.3558901600099427    steps: 456    lr: 6.400000000000001e-06     evaluation reward: 7.4\n",
      "episode: 1736   score: 5.0   memory length: 425597   epsilon: 0.3553159600099391    steps: 290    lr: 6.400000000000001e-06     evaluation reward: 7.34\n",
      "episode: 1737   score: 11.0   memory length: 426098   epsilon: 0.3543239800099328    steps: 501    lr: 6.400000000000001e-06     evaluation reward: 7.36\n",
      "episode: 1738   score: 6.0   memory length: 426441   epsilon: 0.3536448400099285    steps: 343    lr: 6.400000000000001e-06     evaluation reward: 7.35\n",
      "episode: 1739   score: 3.0   memory length: 426650   epsilon: 0.3532310200099259    steps: 209    lr: 6.400000000000001e-06     evaluation reward: 7.32\n",
      "episode: 1740   score: 9.0   memory length: 427096   epsilon: 0.3523479400099203    steps: 446    lr: 6.400000000000001e-06     evaluation reward: 7.37\n",
      "episode: 1741   score: 6.0   memory length: 427434   epsilon: 0.3516787000099161    steps: 338    lr: 6.400000000000001e-06     evaluation reward: 7.39\n",
      "episode: 1742   score: 4.0   memory length: 427691   epsilon: 0.35116984000991286    steps: 257    lr: 6.400000000000001e-06     evaluation reward: 7.4\n",
      "episode: 1743   score: 7.0   memory length: 428044   epsilon: 0.35047090000990844    steps: 353    lr: 6.400000000000001e-06     evaluation reward: 7.4\n",
      "episode: 1744   score: 5.0   memory length: 428318   epsilon: 0.349928380009905    steps: 274    lr: 6.400000000000001e-06     evaluation reward: 7.41\n",
      "episode: 1745   score: 5.0   memory length: 428627   epsilon: 0.34931656000990113    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 7.41\n",
      "episode: 1746   score: 5.0   memory length: 428915   epsilon: 0.3487463200098975    steps: 288    lr: 6.400000000000001e-06     evaluation reward: 7.41\n",
      "episode: 1747   score: 5.0   memory length: 429228   epsilon: 0.3481265800098936    steps: 313    lr: 6.400000000000001e-06     evaluation reward: 7.38\n",
      "episode: 1748   score: 6.0   memory length: 429569   epsilon: 0.34745140000988933    steps: 341    lr: 6.400000000000001e-06     evaluation reward: 7.36\n",
      "episode: 1749   score: 6.0   memory length: 429944   epsilon: 0.34670890000988464    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 7.36\n",
      "episode: 1750   score: 10.0   memory length: 430418   epsilon: 0.3457703800098787    steps: 474    lr: 6.400000000000001e-06     evaluation reward: 7.39\n",
      "episode: 1751   score: 7.0   memory length: 430784   epsilon: 0.3450457000098741    steps: 366    lr: 6.400000000000001e-06     evaluation reward: 7.3\n",
      "episode: 1752   score: 9.0   memory length: 431252   epsilon: 0.34411906000986825    steps: 468    lr: 6.400000000000001e-06     evaluation reward: 7.36\n",
      "episode: 1753   score: 7.0   memory length: 431676   epsilon: 0.34327954000986294    steps: 424    lr: 6.400000000000001e-06     evaluation reward: 7.38\n",
      "episode: 1754   score: 8.0   memory length: 432130   epsilon: 0.34238062000985725    steps: 454    lr: 6.400000000000001e-06     evaluation reward: 7.36\n",
      "episode: 1755   score: 7.0   memory length: 432532   epsilon: 0.3415846600098522    steps: 402    lr: 6.400000000000001e-06     evaluation reward: 7.4\n",
      "episode: 1756   score: 9.0   memory length: 432994   epsilon: 0.3406699000098464    steps: 462    lr: 6.400000000000001e-06     evaluation reward: 7.44\n",
      "episode: 1757   score: 4.0   memory length: 433236   epsilon: 0.3401907400098434    steps: 242    lr: 6.400000000000001e-06     evaluation reward: 7.42\n",
      "episode: 1758   score: 3.0   memory length: 433449   epsilon: 0.3397690000098407    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 7.3\n",
      "episode: 1759   score: 11.0   memory length: 434018   epsilon: 0.3386423800098336    steps: 569    lr: 6.400000000000001e-06     evaluation reward: 7.33\n",
      "episode: 1760   score: 8.0   memory length: 434492   epsilon: 0.33770386000982766    steps: 474    lr: 6.400000000000001e-06     evaluation reward: 7.37\n",
      "episode: 1761   score: 7.0   memory length: 434873   epsilon: 0.3369494800098229    steps: 381    lr: 6.400000000000001e-06     evaluation reward: 7.38\n",
      "episode: 1762   score: 7.0   memory length: 435282   epsilon: 0.33613966000981776    steps: 409    lr: 6.400000000000001e-06     evaluation reward: 7.39\n",
      "episode: 1763   score: 3.0   memory length: 435509   epsilon: 0.3356902000098149    steps: 227    lr: 6.400000000000001e-06     evaluation reward: 7.37\n",
      "episode: 1764   score: 3.0   memory length: 435739   epsilon: 0.33523480000981204    steps: 230    lr: 6.400000000000001e-06     evaluation reward: 7.33\n",
      "episode: 1765   score: 7.0   memory length: 436131   epsilon: 0.33445864000980713    steps: 392    lr: 6.400000000000001e-06     evaluation reward: 7.3\n",
      "episode: 1766   score: 5.0   memory length: 436421   epsilon: 0.3338844400098035    steps: 290    lr: 6.400000000000001e-06     evaluation reward: 7.28\n",
      "episode: 1767   score: 6.0   memory length: 436797   epsilon: 0.3331399600097988    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 7.3\n",
      "episode: 1768   score: 6.0   memory length: 437128   epsilon: 0.33248458000979464    steps: 331    lr: 6.400000000000001e-06     evaluation reward: 7.32\n",
      "episode: 1769   score: 9.0   memory length: 437617   epsilon: 0.3315163600097885    steps: 489    lr: 6.400000000000001e-06     evaluation reward: 7.38\n",
      "episode: 1770   score: 10.0   memory length: 438104   epsilon: 0.3305521000097824    steps: 487    lr: 6.400000000000001e-06     evaluation reward: 7.43\n",
      "episode: 1771   score: 8.0   memory length: 438529   epsilon: 0.3297106000097771    steps: 425    lr: 6.400000000000001e-06     evaluation reward: 7.41\n",
      "episode: 1772   score: 10.0   memory length: 439018   epsilon: 0.32874238000977096    steps: 489    lr: 6.400000000000001e-06     evaluation reward: 7.45\n",
      "episode: 1773   score: 7.0   memory length: 439371   epsilon: 0.32804344000976654    steps: 353    lr: 6.400000000000001e-06     evaluation reward: 7.41\n",
      "episode: 1774   score: 4.0   memory length: 439630   epsilon: 0.3275306200097633    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 7.41\n",
      "episode: 1775   score: 8.0   memory length: 440076   epsilon: 0.3266475400097577    steps: 446    lr: 6.400000000000001e-06     evaluation reward: 7.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1776   score: 12.0   memory length: 440689   epsilon: 0.32543380000975003    steps: 613    lr: 6.400000000000001e-06     evaluation reward: 7.45\n",
      "episode: 1777   score: 11.0   memory length: 441211   epsilon: 0.3244002400097435    steps: 522    lr: 6.400000000000001e-06     evaluation reward: 7.46\n",
      "episode: 1778   score: 6.0   memory length: 441583   epsilon: 0.32366368000973883    steps: 372    lr: 6.400000000000001e-06     evaluation reward: 7.44\n",
      "episode: 1779   score: 11.0   memory length: 442118   epsilon: 0.32260438000973213    steps: 535    lr: 6.400000000000001e-06     evaluation reward: 7.51\n",
      "episode: 1780   score: 6.0   memory length: 442475   epsilon: 0.32189752000972766    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 7.5\n",
      "episode: 1781   score: 9.0   memory length: 442992   epsilon: 0.3208738600097212    steps: 517    lr: 6.400000000000001e-06     evaluation reward: 7.48\n",
      "episode: 1782   score: 12.0   memory length: 443447   epsilon: 0.3199729600097155    steps: 455    lr: 6.400000000000001e-06     evaluation reward: 7.46\n",
      "episode: 1783   score: 5.0   memory length: 443774   epsilon: 0.3193255000097114    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 7.43\n",
      "episode: 1784   score: 4.0   memory length: 444018   epsilon: 0.3188423800097083    steps: 244    lr: 6.400000000000001e-06     evaluation reward: 7.42\n",
      "episode: 1785   score: 6.0   memory length: 444371   epsilon: 0.3181434400097039    steps: 353    lr: 6.400000000000001e-06     evaluation reward: 7.4\n",
      "episode: 1786   score: 7.0   memory length: 444754   epsilon: 0.3173851000096991    steps: 383    lr: 6.400000000000001e-06     evaluation reward: 7.38\n",
      "episode: 1787   score: 10.0   memory length: 445270   epsilon: 0.31636342000969264    steps: 516    lr: 6.400000000000001e-06     evaluation reward: 7.31\n",
      "episode: 1788   score: 12.0   memory length: 445845   epsilon: 0.31522492000968544    steps: 575    lr: 6.400000000000001e-06     evaluation reward: 7.39\n",
      "episode: 1789   score: 8.0   memory length: 446249   epsilon: 0.3144250000096804    steps: 404    lr: 6.400000000000001e-06     evaluation reward: 7.35\n",
      "episode: 1790   score: 5.0   memory length: 446536   epsilon: 0.3138567400096768    steps: 287    lr: 6.400000000000001e-06     evaluation reward: 7.37\n",
      "episode: 1791   score: 14.0   memory length: 447161   epsilon: 0.31261924000966895    steps: 625    lr: 6.400000000000001e-06     evaluation reward: 7.48\n",
      "episode: 1792   score: 12.0   memory length: 447601   epsilon: 0.31174804000966344    steps: 440    lr: 6.400000000000001e-06     evaluation reward: 7.57\n",
      "episode: 1793   score: 10.0   memory length: 448153   epsilon: 0.3106550800096565    steps: 552    lr: 6.400000000000001e-06     evaluation reward: 7.62\n",
      "episode: 1794   score: 6.0   memory length: 448475   epsilon: 0.3100175200096525    steps: 322    lr: 6.400000000000001e-06     evaluation reward: 7.6\n",
      "episode: 1795   score: 12.0   memory length: 449054   epsilon: 0.30887110000964524    steps: 579    lr: 6.400000000000001e-06     evaluation reward: 7.67\n",
      "episode: 1796   score: 9.0   memory length: 449471   epsilon: 0.30804544000964    steps: 417    lr: 6.400000000000001e-06     evaluation reward: 7.59\n",
      "episode: 1797   score: 8.0   memory length: 449924   epsilon: 0.30714850000963434    steps: 453    lr: 6.400000000000001e-06     evaluation reward: 7.59\n",
      "episode: 1798   score: 16.0   memory length: 450514   epsilon: 0.30598030000962695    steps: 590    lr: 6.400000000000001e-06     evaluation reward: 7.65\n",
      "episode: 1799   score: 5.0   memory length: 450820   epsilon: 0.3053744200096231    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 7.61\n",
      "episode: 1800   score: 8.0   memory length: 451219   epsilon: 0.3045844000096181    steps: 399    lr: 6.400000000000001e-06     evaluation reward: 7.63\n",
      "episode: 1801   score: 11.0   memory length: 451734   epsilon: 0.30356470000961167    steps: 515    lr: 6.400000000000001e-06     evaluation reward: 7.67\n",
      "episode: 1802   score: 4.0   memory length: 451991   epsilon: 0.30305584000960845    steps: 257    lr: 6.400000000000001e-06     evaluation reward: 7.62\n",
      "episode: 1803   score: 6.0   memory length: 452352   epsilon: 0.3023410600096039    steps: 361    lr: 6.400000000000001e-06     evaluation reward: 7.62\n",
      "episode: 1804   score: 8.0   memory length: 452805   epsilon: 0.30144412000959825    steps: 453    lr: 6.400000000000001e-06     evaluation reward: 7.64\n",
      "episode: 1805   score: 10.0   memory length: 453287   epsilon: 0.3004897600095922    steps: 482    lr: 6.400000000000001e-06     evaluation reward: 7.63\n",
      "episode: 1806   score: 9.0   memory length: 453757   epsilon: 0.2995591600095863    steps: 470    lr: 6.400000000000001e-06     evaluation reward: 7.69\n",
      "episode: 1807   score: 6.0   memory length: 454078   epsilon: 0.2989235800095823    steps: 321    lr: 6.400000000000001e-06     evaluation reward: 7.71\n",
      "episode: 1808   score: 7.0   memory length: 454487   epsilon: 0.2981137600095772    steps: 409    lr: 6.400000000000001e-06     evaluation reward: 7.71\n",
      "episode: 1809   score: 6.0   memory length: 454861   epsilon: 0.2973732400095725    steps: 374    lr: 6.400000000000001e-06     evaluation reward: 7.68\n",
      "episode: 1810   score: 6.0   memory length: 455196   epsilon: 0.2967099400095683    steps: 335    lr: 6.400000000000001e-06     evaluation reward: 7.72\n",
      "episode: 1811   score: 3.0   memory length: 455446   epsilon: 0.29621494000956516    steps: 250    lr: 6.400000000000001e-06     evaluation reward: 7.69\n",
      "episode: 1812   score: 11.0   memory length: 455964   epsilon: 0.2951893000095587    steps: 518    lr: 6.400000000000001e-06     evaluation reward: 7.71\n",
      "episode: 1813   score: 4.0   memory length: 456243   epsilon: 0.2946368800095552    steps: 279    lr: 6.400000000000001e-06     evaluation reward: 7.65\n",
      "episode: 1814   score: 7.0   memory length: 456602   epsilon: 0.2939260600095507    steps: 359    lr: 6.400000000000001e-06     evaluation reward: 7.64\n",
      "episode: 1815   score: 4.0   memory length: 456864   epsilon: 0.2934073000095474    steps: 262    lr: 6.400000000000001e-06     evaluation reward: 7.62\n",
      "episode: 1816   score: 5.0   memory length: 457135   epsilon: 0.292870720009544    steps: 271    lr: 6.400000000000001e-06     evaluation reward: 7.59\n",
      "episode: 1817   score: 8.0   memory length: 457564   epsilon: 0.29202130000953863    steps: 429    lr: 6.400000000000001e-06     evaluation reward: 7.61\n",
      "episode: 1818   score: 9.0   memory length: 458039   epsilon: 0.2910808000095327    steps: 475    lr: 6.400000000000001e-06     evaluation reward: 7.62\n",
      "episode: 1819   score: 9.0   memory length: 458549   epsilon: 0.2900710000095263    steps: 510    lr: 6.400000000000001e-06     evaluation reward: 7.61\n",
      "episode: 1820   score: 4.0   memory length: 458809   epsilon: 0.28955620000952303    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 7.54\n",
      "episode: 1821   score: 5.0   memory length: 459132   epsilon: 0.288916660009519    steps: 323    lr: 6.400000000000001e-06     evaluation reward: 7.48\n",
      "episode: 1822   score: 4.0   memory length: 459374   epsilon: 0.28843750000951596    steps: 242    lr: 6.400000000000001e-06     evaluation reward: 7.42\n",
      "episode: 1823   score: 10.0   memory length: 459870   epsilon: 0.28745542000950974    steps: 496    lr: 6.400000000000001e-06     evaluation reward: 7.41\n",
      "episode: 1824   score: 6.0   memory length: 460213   epsilon: 0.28677628000950545    steps: 343    lr: 6.400000000000001e-06     evaluation reward: 7.42\n",
      "episode: 1825   score: 5.0   memory length: 460503   epsilon: 0.2862020800095018    steps: 290    lr: 6.400000000000001e-06     evaluation reward: 7.42\n",
      "episode: 1826   score: 6.0   memory length: 460880   epsilon: 0.2854556200094971    steps: 377    lr: 6.400000000000001e-06     evaluation reward: 7.41\n",
      "episode: 1827   score: 11.0   memory length: 461448   epsilon: 0.28433098000949    steps: 568    lr: 6.400000000000001e-06     evaluation reward: 7.43\n",
      "episode: 1828   score: 3.0   memory length: 461661   epsilon: 0.2839092400094873    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 7.39\n",
      "episode: 1829   score: 12.0   memory length: 462258   epsilon: 0.28272718000947983    steps: 597    lr: 6.400000000000001e-06     evaluation reward: 7.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1830   score: 6.0   memory length: 462572   epsilon: 0.2821054600094759    steps: 314    lr: 6.400000000000001e-06     evaluation reward: 7.46\n",
      "episode: 1831   score: 9.0   memory length: 462979   epsilon: 0.2812996000094708    steps: 407    lr: 6.400000000000001e-06     evaluation reward: 7.44\n",
      "episode: 1832   score: 10.0   memory length: 463372   epsilon: 0.2805214600094659    steps: 393    lr: 6.400000000000001e-06     evaluation reward: 7.46\n",
      "episode: 1833   score: 11.0   memory length: 463922   epsilon: 0.279432460009459    steps: 550    lr: 6.400000000000001e-06     evaluation reward: 7.48\n",
      "episode: 1834   score: 4.0   memory length: 464181   epsilon: 0.27891964000945574    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 7.39\n",
      "episode: 1835   score: 4.0   memory length: 464462   epsilon: 0.2783632600094522    steps: 281    lr: 6.400000000000001e-06     evaluation reward: 7.33\n",
      "episode: 1836   score: 5.0   memory length: 464785   epsilon: 0.27772372000944817    steps: 323    lr: 6.400000000000001e-06     evaluation reward: 7.33\n",
      "episode: 1837   score: 5.0   memory length: 465094   epsilon: 0.2771119000094443    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 7.27\n",
      "episode: 1838   score: 4.0   memory length: 465334   epsilon: 0.2766367000094413    steps: 240    lr: 6.400000000000001e-06     evaluation reward: 7.25\n",
      "episode: 1839   score: 4.0   memory length: 465612   epsilon: 0.2760862600094378    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 7.26\n",
      "episode: 1840   score: 10.0   memory length: 466101   epsilon: 0.2751180400094317    steps: 489    lr: 6.400000000000001e-06     evaluation reward: 7.27\n",
      "episode: 1841   score: 5.0   memory length: 466389   epsilon: 0.2745478000094281    steps: 288    lr: 6.400000000000001e-06     evaluation reward: 7.26\n",
      "episode: 1842   score: 5.0   memory length: 466715   epsilon: 0.273902320009424    steps: 326    lr: 6.400000000000001e-06     evaluation reward: 7.27\n",
      "episode: 1843   score: 3.0   memory length: 466928   epsilon: 0.2734805800094213    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 7.23\n",
      "episode: 1844   score: 5.0   memory length: 467203   epsilon: 0.2729360800094179    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 7.23\n",
      "episode: 1845   score: 14.0   memory length: 467683   epsilon: 0.27198568000941187    steps: 480    lr: 6.400000000000001e-06     evaluation reward: 7.32\n",
      "episode: 1846   score: 11.0   memory length: 468237   epsilon: 0.2708887600094049    steps: 554    lr: 6.400000000000001e-06     evaluation reward: 7.38\n",
      "episode: 1847   score: 3.0   memory length: 468449   epsilon: 0.27046900000940227    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 7.36\n",
      "episode: 1848   score: 9.0   memory length: 468901   epsilon: 0.2695740400093966    steps: 452    lr: 6.400000000000001e-06     evaluation reward: 7.39\n",
      "episode: 1849   score: 4.0   memory length: 469143   epsilon: 0.2690948800093936    steps: 242    lr: 6.400000000000001e-06     evaluation reward: 7.37\n",
      "episode: 1850   score: 4.0   memory length: 469403   epsilon: 0.2685800800093903    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 7.31\n",
      "episode: 1851   score: 4.0   memory length: 469663   epsilon: 0.26806528000938706    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 7.28\n",
      "episode: 1852   score: 10.0   memory length: 470129   epsilon: 0.2671426000093812    steps: 466    lr: 6.400000000000001e-06     evaluation reward: 7.29\n",
      "episode: 1853   score: 5.0   memory length: 470439   epsilon: 0.26652880000937734    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 7.27\n",
      "episode: 1854   score: 8.0   memory length: 470851   epsilon: 0.2657130400093722    steps: 412    lr: 6.400000000000001e-06     evaluation reward: 7.27\n",
      "episode: 1855   score: 16.0   memory length: 471566   epsilon: 0.2642973400093632    steps: 715    lr: 6.400000000000001e-06     evaluation reward: 7.36\n",
      "episode: 1856   score: 13.0   memory length: 472187   epsilon: 0.26306776000935544    steps: 621    lr: 6.400000000000001e-06     evaluation reward: 7.4\n",
      "episode: 1857   score: 14.0   memory length: 472767   epsilon: 0.2619193600093482    steps: 580    lr: 6.400000000000001e-06     evaluation reward: 7.5\n",
      "episode: 1858   score: 8.0   memory length: 473160   epsilon: 0.26114122000934326    steps: 393    lr: 6.400000000000001e-06     evaluation reward: 7.55\n",
      "episode: 1859   score: 8.0   memory length: 473592   epsilon: 0.26028586000933784    steps: 432    lr: 6.400000000000001e-06     evaluation reward: 7.52\n",
      "episode: 1860   score: 8.0   memory length: 474014   epsilon: 0.25945030000933256    steps: 422    lr: 6.400000000000001e-06     evaluation reward: 7.52\n",
      "episode: 1861   score: 12.0   memory length: 474518   epsilon: 0.25845238000932624    steps: 504    lr: 6.400000000000001e-06     evaluation reward: 7.57\n",
      "episode: 1862   score: 8.0   memory length: 474959   epsilon: 0.2575792000093207    steps: 441    lr: 6.400000000000001e-06     evaluation reward: 7.58\n",
      "episode: 1863   score: 6.0   memory length: 475296   epsilon: 0.2569119400093165    steps: 337    lr: 6.400000000000001e-06     evaluation reward: 7.61\n",
      "episode: 1864   score: 6.0   memory length: 475634   epsilon: 0.25624270000931226    steps: 338    lr: 6.400000000000001e-06     evaluation reward: 7.64\n",
      "episode: 1865   score: 6.0   memory length: 475972   epsilon: 0.25557346000930803    steps: 338    lr: 6.400000000000001e-06     evaluation reward: 7.63\n",
      "episode: 1866   score: 9.0   memory length: 476403   epsilon: 0.25472008000930263    steps: 431    lr: 6.400000000000001e-06     evaluation reward: 7.67\n",
      "episode: 1867   score: 4.0   memory length: 476645   epsilon: 0.2542409200092996    steps: 242    lr: 6.400000000000001e-06     evaluation reward: 7.65\n",
      "episode: 1868   score: 10.0   memory length: 477150   epsilon: 0.25324102000929327    steps: 505    lr: 6.400000000000001e-06     evaluation reward: 7.69\n",
      "episode: 1869   score: 12.0   memory length: 477622   epsilon: 0.25230646000928736    steps: 472    lr: 6.400000000000001e-06     evaluation reward: 7.72\n",
      "episode: 1870   score: 11.0   memory length: 478084   epsilon: 0.25139170000928157    steps: 462    lr: 6.400000000000001e-06     evaluation reward: 7.73\n",
      "episode: 1871   score: 3.0   memory length: 478295   epsilon: 0.25097392000927893    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 7.68\n",
      "episode: 1872   score: 11.0   memory length: 478721   epsilon: 0.2501304400092736    steps: 426    lr: 6.400000000000001e-06     evaluation reward: 7.69\n",
      "episode: 1873   score: 9.0   memory length: 479173   epsilon: 0.24923548000926793    steps: 452    lr: 6.400000000000001e-06     evaluation reward: 7.71\n",
      "episode: 1874   score: 5.0   memory length: 479461   epsilon: 0.24866524000926432    steps: 288    lr: 6.400000000000001e-06     evaluation reward: 7.72\n",
      "episode: 1875   score: 6.0   memory length: 479785   epsilon: 0.24802372000926026    steps: 324    lr: 6.400000000000001e-06     evaluation reward: 7.7\n",
      "episode: 1876   score: 13.0   memory length: 480425   epsilon: 0.24675652000925224    steps: 640    lr: 6.400000000000001e-06     evaluation reward: 7.71\n",
      "episode: 1877   score: 12.0   memory length: 480998   epsilon: 0.24562198000924507    steps: 573    lr: 6.400000000000001e-06     evaluation reward: 7.72\n",
      "episode: 1878   score: 9.0   memory length: 481434   epsilon: 0.2447587000092396    steps: 436    lr: 6.400000000000001e-06     evaluation reward: 7.75\n",
      "episode: 1879   score: 11.0   memory length: 481924   epsilon: 0.24378850000923347    steps: 490    lr: 6.400000000000001e-06     evaluation reward: 7.75\n",
      "episode: 1880   score: 3.0   memory length: 482135   epsilon: 0.24337072000923082    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 7.72\n",
      "episode: 1881   score: 13.0   memory length: 482748   epsilon: 0.24215698000922314    steps: 613    lr: 6.400000000000001e-06     evaluation reward: 7.76\n",
      "episode: 1882   score: 6.0   memory length: 483054   epsilon: 0.2415511000092193    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 7.7\n",
      "episode: 1883   score: 5.0   memory length: 483362   epsilon: 0.24094126000921545    steps: 308    lr: 6.400000000000001e-06     evaluation reward: 7.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1884   score: 9.0   memory length: 483809   epsilon: 0.24005620000920985    steps: 447    lr: 6.400000000000001e-06     evaluation reward: 7.75\n",
      "episode: 1885   score: 5.0   memory length: 484114   epsilon: 0.23945230000920603    steps: 305    lr: 6.400000000000001e-06     evaluation reward: 7.74\n",
      "episode: 1886   score: 4.0   memory length: 484356   epsilon: 0.238973140009203    steps: 242    lr: 6.400000000000001e-06     evaluation reward: 7.71\n",
      "episode: 1887   score: 5.0   memory length: 484665   epsilon: 0.23836132000919913    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 7.66\n",
      "episode: 1888   score: 16.0   memory length: 485186   epsilon: 0.2373297400091926    steps: 521    lr: 6.400000000000001e-06     evaluation reward: 7.7\n",
      "episode: 1889   score: 11.0   memory length: 485624   epsilon: 0.23646250000918712    steps: 438    lr: 6.400000000000001e-06     evaluation reward: 7.73\n",
      "episode: 1890   score: 4.0   memory length: 485903   epsilon: 0.23591008000918362    steps: 279    lr: 6.400000000000001e-06     evaluation reward: 7.72\n",
      "episode: 1891   score: 9.0   memory length: 486377   epsilon: 0.23497156000917768    steps: 474    lr: 6.400000000000001e-06     evaluation reward: 7.67\n",
      "episode: 1892   score: 11.0   memory length: 486900   epsilon: 0.23393602000917113    steps: 523    lr: 6.400000000000001e-06     evaluation reward: 7.66\n",
      "episode: 1893   score: 11.0   memory length: 487380   epsilon: 0.23298562000916512    steps: 480    lr: 6.400000000000001e-06     evaluation reward: 7.67\n",
      "episode: 1894   score: 8.0   memory length: 487818   epsilon: 0.23211838000915963    steps: 438    lr: 6.400000000000001e-06     evaluation reward: 7.69\n",
      "episode: 1895   score: 4.0   memory length: 488057   epsilon: 0.23164516000915664    steps: 239    lr: 6.400000000000001e-06     evaluation reward: 7.61\n",
      "episode: 1896   score: 16.0   memory length: 488646   epsilon: 0.23047894000914926    steps: 589    lr: 6.400000000000001e-06     evaluation reward: 7.68\n",
      "episode: 1897   score: 9.0   memory length: 489106   epsilon: 0.2295681400091435    steps: 460    lr: 6.400000000000001e-06     evaluation reward: 7.69\n",
      "episode: 1898   score: 9.0   memory length: 489560   epsilon: 0.2286692200091378    steps: 454    lr: 6.400000000000001e-06     evaluation reward: 7.62\n",
      "episode: 1899   score: 9.0   memory length: 490031   epsilon: 0.2277366400091319    steps: 471    lr: 6.400000000000001e-06     evaluation reward: 7.66\n",
      "episode: 1900   score: 9.0   memory length: 490496   epsilon: 0.22681594000912608    steps: 465    lr: 6.400000000000001e-06     evaluation reward: 7.67\n",
      "episode: 1901   score: 3.0   memory length: 490708   epsilon: 0.22639618000912343    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 7.59\n",
      "episode: 1902   score: 9.0   memory length: 491146   epsilon: 0.22552894000911794    steps: 438    lr: 6.400000000000001e-06     evaluation reward: 7.64\n",
      "episode: 1903   score: 11.0   memory length: 491710   epsilon: 0.22441222000911087    steps: 564    lr: 6.400000000000001e-06     evaluation reward: 7.69\n",
      "episode: 1904   score: 12.0   memory length: 492182   epsilon: 0.22347766000910496    steps: 472    lr: 6.400000000000001e-06     evaluation reward: 7.73\n",
      "episode: 1905   score: 7.0   memory length: 492573   epsilon: 0.22270348000910006    steps: 391    lr: 6.400000000000001e-06     evaluation reward: 7.7\n",
      "episode: 1906   score: 6.0   memory length: 492911   epsilon: 0.22203424000909583    steps: 338    lr: 6.400000000000001e-06     evaluation reward: 7.67\n",
      "episode: 1907   score: 11.0   memory length: 493456   epsilon: 0.220955140009089    steps: 545    lr: 6.400000000000001e-06     evaluation reward: 7.72\n",
      "episode: 1908   score: 7.0   memory length: 493863   epsilon: 0.2201492800090839    steps: 407    lr: 6.400000000000001e-06     evaluation reward: 7.72\n",
      "episode: 1909   score: 10.0   memory length: 494376   epsilon: 0.21913354000907748    steps: 513    lr: 6.400000000000001e-06     evaluation reward: 7.76\n",
      "episode: 1910   score: 10.0   memory length: 494829   epsilon: 0.2182366000090718    steps: 453    lr: 6.400000000000001e-06     evaluation reward: 7.8\n",
      "episode: 1911   score: 9.0   memory length: 495300   epsilon: 0.2173040200090659    steps: 471    lr: 6.400000000000001e-06     evaluation reward: 7.86\n",
      "episode: 1912   score: 7.0   memory length: 495650   epsilon: 0.21661102000906152    steps: 350    lr: 6.400000000000001e-06     evaluation reward: 7.82\n",
      "episode: 1913   score: 11.0   memory length: 496201   epsilon: 0.21552004000905461    steps: 551    lr: 6.400000000000001e-06     evaluation reward: 7.89\n",
      "episode: 1914   score: 9.0   memory length: 496670   epsilon: 0.21459142000904874    steps: 469    lr: 6.400000000000001e-06     evaluation reward: 7.91\n",
      "episode: 1915   score: 8.0   memory length: 497078   epsilon: 0.21378358000904363    steps: 408    lr: 6.400000000000001e-06     evaluation reward: 7.95\n",
      "episode: 1916   score: 7.0   memory length: 497483   epsilon: 0.21298168000903855    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 7.97\n",
      "episode: 1917   score: 8.0   memory length: 497905   epsilon: 0.21214612000903327    steps: 422    lr: 6.400000000000001e-06     evaluation reward: 7.97\n",
      "episode: 1918   score: 11.0   memory length: 498418   epsilon: 0.21113038000902684    steps: 513    lr: 6.400000000000001e-06     evaluation reward: 7.99\n",
      "episode: 1919   score: 5.0   memory length: 498726   epsilon: 0.21052054000902298    steps: 308    lr: 6.400000000000001e-06     evaluation reward: 7.95\n",
      "episode: 1920   score: 6.0   memory length: 499045   epsilon: 0.209888920009019    steps: 319    lr: 6.400000000000001e-06     evaluation reward: 7.97\n",
      "episode: 1921   score: 5.0   memory length: 499371   epsilon: 0.2092434400090149    steps: 326    lr: 6.400000000000001e-06     evaluation reward: 7.97\n",
      "episode: 1922   score: 7.0   memory length: 499760   epsilon: 0.20847322000901003    steps: 389    lr: 6.400000000000001e-06     evaluation reward: 8.0\n",
      "episode: 1923   score: 13.0   memory length: 500340   epsilon: 0.20732482000900276    steps: 580    lr: 2.560000000000001e-06     evaluation reward: 8.03\n",
      "episode: 1924   score: 11.0   memory length: 500906   epsilon: 0.20620414000899567    steps: 566    lr: 2.560000000000001e-06     evaluation reward: 8.08\n",
      "episode: 1925   score: 4.0   memory length: 501147   epsilon: 0.20572696000899265    steps: 241    lr: 2.560000000000001e-06     evaluation reward: 8.07\n",
      "episode: 1926   score: 5.0   memory length: 501433   epsilon: 0.20516068000898907    steps: 286    lr: 2.560000000000001e-06     evaluation reward: 8.06\n",
      "episode: 1927   score: 9.0   memory length: 501904   epsilon: 0.20422810000898317    steps: 471    lr: 2.560000000000001e-06     evaluation reward: 8.04\n",
      "episode: 1928   score: 14.0   memory length: 502447   epsilon: 0.20315296000897637    steps: 543    lr: 2.560000000000001e-06     evaluation reward: 8.15\n",
      "episode: 1929   score: 12.0   memory length: 503039   epsilon: 0.20198080000896895    steps: 592    lr: 2.560000000000001e-06     evaluation reward: 8.15\n",
      "episode: 1930   score: 10.0   memory length: 503507   epsilon: 0.2010541600089631    steps: 468    lr: 2.560000000000001e-06     evaluation reward: 8.19\n",
      "episode: 1931   score: 10.0   memory length: 503977   epsilon: 0.2001235600089572    steps: 470    lr: 2.560000000000001e-06     evaluation reward: 8.2\n",
      "episode: 1932   score: 9.0   memory length: 504421   epsilon: 0.19924444000895164    steps: 444    lr: 2.560000000000001e-06     evaluation reward: 8.19\n",
      "episode: 1933   score: 4.0   memory length: 504684   epsilon: 0.19872370000894835    steps: 263    lr: 2.560000000000001e-06     evaluation reward: 8.12\n",
      "episode: 1934   score: 11.0   memory length: 505192   epsilon: 0.19771786000894198    steps: 508    lr: 2.560000000000001e-06     evaluation reward: 8.19\n",
      "episode: 1935   score: 7.0   memory length: 505597   epsilon: 0.1969159600089369    steps: 405    lr: 2.560000000000001e-06     evaluation reward: 8.22\n",
      "episode: 1936   score: 5.0   memory length: 505926   epsilon: 0.1962645400089328    steps: 329    lr: 2.560000000000001e-06     evaluation reward: 8.22\n",
      "episode: 1937   score: 4.0   memory length: 506181   epsilon: 0.1957596400089296    steps: 255    lr: 2.560000000000001e-06     evaluation reward: 8.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1938   score: 8.0   memory length: 506461   epsilon: 0.19520524000892608    steps: 280    lr: 2.560000000000001e-06     evaluation reward: 8.25\n",
      "episode: 1939   score: 12.0   memory length: 507020   epsilon: 0.19409842000891908    steps: 559    lr: 2.560000000000001e-06     evaluation reward: 8.33\n",
      "episode: 1940   score: 12.0   memory length: 507567   epsilon: 0.19301536000891223    steps: 547    lr: 2.560000000000001e-06     evaluation reward: 8.35\n",
      "episode: 1941   score: 9.0   memory length: 508048   epsilon: 0.1920629800089062    steps: 481    lr: 2.560000000000001e-06     evaluation reward: 8.39\n",
      "episode: 1942   score: 7.0   memory length: 508413   epsilon: 0.19134028000890163    steps: 365    lr: 2.560000000000001e-06     evaluation reward: 8.41\n",
      "episode: 1943   score: 22.0   memory length: 509040   epsilon: 0.19009882000889378    steps: 627    lr: 2.560000000000001e-06     evaluation reward: 8.6\n",
      "episode: 1944   score: 15.0   memory length: 509638   epsilon: 0.18891478000888628    steps: 598    lr: 2.560000000000001e-06     evaluation reward: 8.7\n",
      "episode: 1945   score: 10.0   memory length: 510158   epsilon: 0.18788518000887977    steps: 520    lr: 2.560000000000001e-06     evaluation reward: 8.66\n",
      "episode: 1946   score: 12.0   memory length: 510701   epsilon: 0.18681004000887297    steps: 543    lr: 2.560000000000001e-06     evaluation reward: 8.67\n",
      "episode: 1947   score: 4.0   memory length: 510943   epsilon: 0.18633088000886994    steps: 242    lr: 2.560000000000001e-06     evaluation reward: 8.68\n",
      "episode: 1948   score: 13.0   memory length: 511519   epsilon: 0.18519040000886272    steps: 576    lr: 2.560000000000001e-06     evaluation reward: 8.72\n",
      "episode: 1949   score: 8.0   memory length: 511927   epsilon: 0.1843825600088576    steps: 408    lr: 2.560000000000001e-06     evaluation reward: 8.76\n",
      "episode: 1950   score: 10.0   memory length: 512386   epsilon: 0.18347374000885186    steps: 459    lr: 2.560000000000001e-06     evaluation reward: 8.82\n",
      "episode: 1951   score: 9.0   memory length: 512857   epsilon: 0.18254116000884596    steps: 471    lr: 2.560000000000001e-06     evaluation reward: 8.87\n",
      "episode: 1952   score: 11.0   memory length: 513358   epsilon: 0.18154918000883968    steps: 501    lr: 2.560000000000001e-06     evaluation reward: 8.88\n",
      "episode: 1953   score: 9.0   memory length: 513837   epsilon: 0.18060076000883368    steps: 479    lr: 2.560000000000001e-06     evaluation reward: 8.92\n",
      "episode: 1954   score: 6.0   memory length: 514159   epsilon: 0.17996320000882965    steps: 322    lr: 2.560000000000001e-06     evaluation reward: 8.9\n",
      "episode: 1955   score: 9.0   memory length: 514613   epsilon: 0.17906428000882396    steps: 454    lr: 2.560000000000001e-06     evaluation reward: 8.83\n",
      "episode: 1956   score: 8.0   memory length: 515038   epsilon: 0.17822278000881864    steps: 425    lr: 2.560000000000001e-06     evaluation reward: 8.78\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-291524992219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rewards'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episodes vs Reward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./save_graph/breakout_dqn.png\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# save graph for training visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# every episode, plot the play time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2309\u001b[0m                 \u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_edgecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2311\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransparent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2216\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2217\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2218\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2219\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m'Software'\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \"\"\"\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         mpl.image.imsave(\n\u001b[1;32m    511\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m              (self.toolbar._wait_cursor_for_draw_cm() if self.toolbar\n\u001b[1;32m    406\u001b[0m               else nullcontext()):\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1864\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 **kwargs)\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2746\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2748\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2750\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dashes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dashOffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dashSeq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maffine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrozen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw_path\u001b[0;34m(self, gc, path, transform, rgbFace)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrgbFace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOverflowError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 raise OverflowError(\"Exceeded cell block limit (set \"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        r = np.clip(reward, -1, 1) \n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwJr3R4_FMnb"
   },
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTtmNro6FMnb"
   },
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4I_RMj0FMnb"
   },
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1607561665591,
     "user": {
      "displayName": "Darren Anco",
      "photoUrl": "",
      "userId": "04554837860039334829"
     },
     "user_tz": 360
    },
    "id": "ZfqJNqB2FMnb"
   },
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "executionInfo": {
     "elapsed": 12097,
     "status": "ok",
     "timestamp": 1607561680870,
     "user": {
      "displayName": "Darren Anco",
      "photoUrl": "",
      "userId": "04554837860039334829"
     },
     "user_tz": 360
    },
    "id": "dFv36A0pFMnc",
    "outputId": "4c486d2f-0bf1-41e4-9187-6b519d0c07e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" autoplay \n",
       "                loop controls style=\"height: 400px;\">\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f2fe5820ef0>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "agent.load_policy_net(\"./save_model/breakout_dqn_latest.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state)\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Render breakout\n",
    "    env.render()\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['ale.lives'])\n",
    "        \n",
    "    life = info['ale.lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "display.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bt_uGD1uFMnc"
   },
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4B3z8IyFMnc"
   },
   "outputs": [],
   "source": [
    "double_dqn = False # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-Z_dRJCFMnd",
    "outputId": "d94844d6-8fcb-4e24-f307-51276f3a9bb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 2.0   memory length: 198   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 1   score: 1.0   memory length: 367   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 2   score: 2.0   memory length: 585   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 3   score: 2.0   memory length: 783   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 4   score: 1.0   memory length: 951   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 5   score: 0.0   memory length: 1074   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
      "episode: 6   score: 1.0   memory length: 1225   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.2857142857142858\n",
      "episode: 7   score: 3.0   memory length: 1453   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 8   score: 0.0   memory length: 1576   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
      "episode: 9   score: 2.0   memory length: 1794   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 10   score: 2.0   memory length: 1992   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4545454545454546\n",
      "episode: 11   score: 2.0   memory length: 2213   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 12   score: 0.0   memory length: 2335   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3846153846153846\n",
      "episode: 13   score: 2.0   memory length: 2533   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4285714285714286\n",
      "episode: 14   score: 0.0   memory length: 2656   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
      "episode: 15   score: 0.0   memory length: 2779   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 16   score: 1.0   memory length: 2930   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.2352941176470589\n",
      "episode: 17   score: 0.0   memory length: 3053   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.1666666666666667\n",
      "episode: 18   score: 0.0   memory length: 3176   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.105263157894737\n",
      "episode: 19   score: 1.0   memory length: 3326   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.1\n",
      "episode: 20   score: 2.0   memory length: 3545   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.1428571428571428\n",
      "episode: 21   score: 1.0   memory length: 3696   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.1363636363636365\n",
      "episode: 22   score: 2.0   memory length: 3894   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.173913043478261\n",
      "episode: 23   score: 1.0   memory length: 4064   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.1666666666666667\n",
      "episode: 24   score: 0.0   memory length: 4186   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.12\n",
      "episode: 25   score: 0.0   memory length: 4309   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.0769230769230769\n",
      "episode: 26   score: 0.0   memory length: 4432   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.037037037037037\n",
      "episode: 27   score: 1.0   memory length: 4601   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.0357142857142858\n",
      "episode: 28   score: 0.0   memory length: 4724   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.0\n",
      "episode: 29   score: 1.0   memory length: 4893   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.0\n",
      "episode: 30   score: 0.0   memory length: 5015   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 0.967741935483871\n",
      "episode: 31   score: 2.0   memory length: 5213   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.0\n",
      "episode: 32   score: 0.0   memory length: 5336   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.9696969696969697\n",
      "episode: 33   score: 2.0   memory length: 5534   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.0\n",
      "episode: 34   score: 0.0   memory length: 5657   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.9714285714285714\n",
      "episode: 35   score: 2.0   memory length: 5854   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.0\n",
      "episode: 36   score: 0.0   memory length: 5977   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.972972972972973\n",
      "episode: 37   score: 0.0   memory length: 6099   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 0.9473684210526315\n",
      "episode: 38   score: 2.0   memory length: 6316   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 0.9743589743589743\n",
      "episode: 39   score: 1.0   memory length: 6467   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 0.975\n",
      "episode: 40   score: 0.0   memory length: 6590   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.9512195121951219\n",
      "episode: 41   score: 1.0   memory length: 6741   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 0.9523809523809523\n",
      "episode: 42   score: 1.0   memory length: 6892   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 0.9534883720930233\n",
      "episode: 43   score: 0.0   memory length: 7015   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.9318181818181818\n",
      "episode: 44   score: 2.0   memory length: 7213   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 0.9555555555555556\n",
      "episode: 45   score: 0.0   memory length: 7336   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.9347826086956522\n",
      "episode: 46   score: 2.0   memory length: 7534   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 0.9574468085106383\n",
      "episode: 47   score: 2.0   memory length: 7751   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 0.9791666666666666\n",
      "episode: 48   score: 0.0   memory length: 7873   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 0.9591836734693877\n",
      "episode: 49   score: 1.0   memory length: 8043   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 0.96\n",
      "episode: 50   score: 0.0   memory length: 8166   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.9411764705882353\n",
      "episode: 51   score: 1.0   memory length: 8336   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 0.9423076923076923\n",
      "episode: 52   score: 2.0   memory length: 8534   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 0.9622641509433962\n",
      "episode: 53   score: 1.0   memory length: 8703   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 0.9629629629629629\n",
      "episode: 54   score: 3.0   memory length: 8935   epsilon: 1.0    steps: 232    lr: 0.0001     evaluation reward: 1.0\n",
      "episode: 55   score: 0.0   memory length: 9058   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.9821428571428571\n",
      "episode: 56   score: 0.0   memory length: 9180   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 0.9649122807017544\n",
      "episode: 57   score: 2.0   memory length: 9378   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 0.9827586206896551\n",
      "episode: 58   score: 0.0   memory length: 9501   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.9661016949152542\n",
      "episode: 59   score: 0.0   memory length: 9624   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.95\n",
      "episode: 60   score: 3.0   memory length: 9873   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 0.9836065573770492\n",
      "episode: 61   score: 0.0   memory length: 9996   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.967741935483871\n",
      "episode: 62   score: 1.0   memory length: 10147   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 0.9682539682539683\n",
      "episode: 63   score: 1.0   memory length: 10316   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 0.96875\n",
      "episode: 64   score: 1.0   memory length: 10485   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 0.9692307692307692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 65   score: 1.0   memory length: 10654   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 0.9696969696969697\n",
      "episode: 66   score: 1.0   memory length: 10805   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 0.9701492537313433\n",
      "episode: 67   score: 1.0   memory length: 10956   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 0.9705882352941176\n",
      "episode: 68   score: 1.0   memory length: 11106   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 0.9710144927536232\n",
      "episode: 69   score: 2.0   memory length: 11303   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 0.9857142857142858\n",
      "episode: 70   score: 3.0   memory length: 11529   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.0140845070422535\n",
      "episode: 71   score: 0.0   memory length: 11652   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.0\n",
      "episode: 72   score: 2.0   memory length: 11870   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.0136986301369864\n",
      "episode: 73   score: 1.0   memory length: 12038   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.0135135135135136\n",
      "episode: 74   score: 2.0   memory length: 12254   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.0266666666666666\n",
      "episode: 75   score: 1.0   memory length: 12423   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.0263157894736843\n",
      "episode: 76   score: 1.0   memory length: 12574   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.025974025974026\n",
      "episode: 77   score: 1.0   memory length: 12743   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.0256410256410255\n",
      "episode: 78   score: 0.0   memory length: 12865   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.0126582278481013\n",
      "episode: 79   score: 2.0   memory length: 13062   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.025\n",
      "episode: 80   score: 1.0   memory length: 13233   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.0246913580246915\n",
      "episode: 81   score: 4.0   memory length: 13530   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.0609756097560976\n",
      "episode: 82   score: 1.0   memory length: 13702   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.0602409638554218\n",
      "episode: 83   score: 6.0   memory length: 14084   epsilon: 1.0    steps: 382    lr: 0.0001     evaluation reward: 1.119047619047619\n",
      "episode: 84   score: 1.0   memory length: 14235   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.1176470588235294\n",
      "episode: 85   score: 0.0   memory length: 14357   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.1046511627906976\n",
      "episode: 86   score: 1.0   memory length: 14526   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.103448275862069\n",
      "episode: 87   score: 0.0   memory length: 14648   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.0909090909090908\n",
      "episode: 88   score: 1.0   memory length: 14820   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.0898876404494382\n",
      "episode: 89   score: 2.0   memory length: 15037   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.1\n",
      "episode: 90   score: 1.0   memory length: 15207   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.098901098901099\n",
      "episode: 91   score: 1.0   memory length: 15357   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.0978260869565217\n",
      "episode: 92   score: 2.0   memory length: 15575   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.10752688172043\n",
      "episode: 93   score: 1.0   memory length: 15747   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.1063829787234043\n",
      "episode: 94   score: 0.0   memory length: 15869   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.0947368421052632\n",
      "episode: 95   score: 1.0   memory length: 16037   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.09375\n",
      "episode: 96   score: 0.0   memory length: 16160   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.0824742268041236\n",
      "episode: 97   score: 1.0   memory length: 16331   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.0816326530612246\n",
      "episode: 98   score: 1.0   memory length: 16500   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.0808080808080809\n",
      "episode: 99   score: 0.0   memory length: 16622   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.07\n",
      "episode: 100   score: 4.0   memory length: 16897   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.09\n",
      "episode: 101   score: 1.0   memory length: 17048   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.09\n",
      "episode: 102   score: 2.0   memory length: 17246   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.09\n",
      "episode: 103   score: 1.0   memory length: 17415   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.08\n",
      "episode: 104   score: 3.0   memory length: 17660   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.1\n",
      "episode: 105   score: 0.0   memory length: 17783   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.1\n",
      "episode: 106   score: 2.0   memory length: 17981   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.11\n",
      "episode: 107   score: 0.0   memory length: 18104   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.08\n",
      "episode: 108   score: 2.0   memory length: 18302   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.1\n",
      "episode: 109   score: 2.0   memory length: 18500   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.1\n",
      "episode: 110   score: 0.0   memory length: 18623   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.08\n",
      "episode: 111   score: 1.0   memory length: 18773   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.07\n",
      "episode: 112   score: 3.0   memory length: 19020   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.1\n",
      "episode: 113   score: 5.0   memory length: 19386   epsilon: 1.0    steps: 366    lr: 0.0001     evaluation reward: 1.13\n",
      "episode: 114   score: 2.0   memory length: 19603   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.15\n",
      "episode: 115   score: 2.0   memory length: 19800   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.17\n",
      "episode: 116   score: 2.0   memory length: 19997   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 117   score: 0.0   memory length: 20120   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 118   score: 1.0   memory length: 20292   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 119   score: 3.0   memory length: 20517   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 120   score: 1.0   memory length: 20668   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 121   score: 4.0   memory length: 20966   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 122   score: 1.0   memory length: 21116   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 123   score: 2.0   memory length: 21333   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 124   score: 6.0   memory length: 21660   epsilon: 1.0    steps: 327    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 125   score: 0.0   memory length: 21783   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 126   score: 1.0   memory length: 21955   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 127   score: 2.0   memory length: 22153   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 128   score: 2.0   memory length: 22350   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 129   score: 2.0   memory length: 22548   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 130   score: 0.0   memory length: 22671   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 131   score: 2.0   memory length: 22869   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 132   score: 2.0   memory length: 23049   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 133   score: 0.0   memory length: 23172   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 134   score: 2.0   memory length: 23369   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 135   score: 0.0   memory length: 23492   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 136   score: 3.0   memory length: 23736   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 137   score: 2.0   memory length: 23936   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 138   score: 1.0   memory length: 24104   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 139   score: 2.0   memory length: 24302   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 140   score: 1.0   memory length: 24453   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 141   score: 2.0   memory length: 24652   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 142   score: 5.0   memory length: 24999   epsilon: 1.0    steps: 347    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 143   score: 1.0   memory length: 25168   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 144   score: 0.0   memory length: 25291   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 145   score: 5.0   memory length: 25584   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 146   score: 1.0   memory length: 25734   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 147   score: 3.0   memory length: 25982   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 148   score: 0.0   memory length: 26104   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 149   score: 0.0   memory length: 26227   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 150   score: 1.0   memory length: 26398   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 151   score: 2.0   memory length: 26616   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 152   score: 0.0   memory length: 26738   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 153   score: 0.0   memory length: 26861   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 154   score: 0.0   memory length: 26983   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 155   score: 2.0   memory length: 27167   epsilon: 1.0    steps: 184    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 156   score: 1.0   memory length: 27335   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 157   score: 4.0   memory length: 27616   epsilon: 1.0    steps: 281    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 158   score: 3.0   memory length: 27842   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 159   score: 0.0   memory length: 27965   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 160   score: 1.0   memory length: 28116   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 161   score: 3.0   memory length: 28344   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 162   score: 2.0   memory length: 28561   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 163   score: 3.0   memory length: 28827   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 164   score: 2.0   memory length: 29024   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 165   score: 3.0   memory length: 29249   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 166   score: 1.0   memory length: 29399   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 167   score: 3.0   memory length: 29646   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 168   score: 1.0   memory length: 29798   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 169   score: 1.0   memory length: 29949   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 170   score: 0.0   memory length: 30072   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 171   score: 1.0   memory length: 30223   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 172   score: 2.0   memory length: 30440   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 173   score: 2.0   memory length: 30638   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 174   score: 2.0   memory length: 30835   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 175   score: 1.0   memory length: 30986   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 176   score: 2.0   memory length: 31206   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 177   score: 1.0   memory length: 31374   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 178   score: 1.0   memory length: 31543   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 179   score: 2.0   memory length: 31761   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 180   score: 0.0   memory length: 31883   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 181   score: 4.0   memory length: 32178   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 182   score: 0.0   memory length: 32301   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 183   score: 1.0   memory length: 32470   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 184   score: 0.0   memory length: 32593   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 185   score: 0.0   memory length: 32716   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 186   score: 0.0   memory length: 32839   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 187   score: 1.0   memory length: 33010   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 188   score: 0.0   memory length: 33133   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 189   score: 0.0   memory length: 33255   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 190   score: 1.0   memory length: 33424   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 191   score: 2.0   memory length: 33621   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 192   score: 0.0   memory length: 33744   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 193   score: 3.0   memory length: 34014   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 194   score: 1.0   memory length: 34165   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 195   score: 0.0   memory length: 34287   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 196   score: 1.0   memory length: 34459   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 197   score: 1.0   memory length: 34629   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 198   score: 0.0   memory length: 34752   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 199   score: 3.0   memory length: 34981   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 200   score: 2.0   memory length: 35196   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 201   score: 0.0   memory length: 35319   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 202   score: 1.0   memory length: 35488   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 203   score: 1.0   memory length: 35639   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 204   score: 1.0   memory length: 35809   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 205   score: 2.0   memory length: 36006   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 206   score: 0.0   memory length: 36128   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 207   score: 0.0   memory length: 36250   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 208   score: 1.0   memory length: 36419   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 209   score: 3.0   memory length: 36645   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 210   score: 2.0   memory length: 36863   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 211   score: 2.0   memory length: 37060   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 212   score: 3.0   memory length: 37309   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 213   score: 0.0   memory length: 37431   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 214   score: 0.0   memory length: 37554   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 215   score: 3.0   memory length: 37801   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 216   score: 2.0   memory length: 38019   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 217   score: 2.0   memory length: 38217   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 218   score: 0.0   memory length: 38339   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 219   score: 2.0   memory length: 38537   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 220   score: 2.0   memory length: 38737   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 221   score: 0.0   memory length: 38860   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 222   score: 2.0   memory length: 39057   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 223   score: 5.0   memory length: 39367   epsilon: 1.0    steps: 310    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 224   score: 1.0   memory length: 39539   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 225   score: 1.0   memory length: 39690   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 226   score: 0.0   memory length: 39813   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 227   score: 1.0   memory length: 39982   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 228   score: 0.0   memory length: 40104   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 229   score: 0.0   memory length: 40227   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 230   score: 2.0   memory length: 40424   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 231   score: 3.0   memory length: 40674   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 232   score: 1.0   memory length: 40842   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 233   score: 0.0   memory length: 40964   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 234   score: 0.0   memory length: 41086   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 235   score: 0.0   memory length: 41208   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 236   score: 2.0   memory length: 41406   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 237   score: 2.0   memory length: 41623   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 238   score: 2.0   memory length: 41822   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 239   score: 1.0   memory length: 41972   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 240   score: 0.0   memory length: 42095   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 241   score: 1.0   memory length: 42247   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 242   score: 3.0   memory length: 42497   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 243   score: 2.0   memory length: 42699   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 244   score: 1.0   memory length: 42867   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 245   score: 2.0   memory length: 43085   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 246   score: 2.0   memory length: 43302   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 247   score: 5.0   memory length: 43650   epsilon: 1.0    steps: 348    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 248   score: 1.0   memory length: 43800   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 249   score: 5.0   memory length: 44143   epsilon: 1.0    steps: 343    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 250   score: 1.0   memory length: 44312   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 251   score: 1.0   memory length: 44463   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 252   score: 0.0   memory length: 44585   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 253   score: 0.0   memory length: 44707   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 254   score: 2.0   memory length: 44887   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 255   score: 5.0   memory length: 45251   epsilon: 1.0    steps: 364    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 256   score: 1.0   memory length: 45419   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 257   score: 2.0   memory length: 45617   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 258   score: 2.0   memory length: 45815   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 259   score: 2.0   memory length: 46000   epsilon: 1.0    steps: 185    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 260   score: 2.0   memory length: 46197   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 261   score: 4.0   memory length: 46472   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 262   score: 0.0   memory length: 46594   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 263   score: 0.0   memory length: 46716   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 264   score: 0.0   memory length: 46839   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 265   score: 3.0   memory length: 47087   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 266   score: 3.0   memory length: 47335   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 267   score: 4.0   memory length: 47593   epsilon: 1.0    steps: 258    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 268   score: 0.0   memory length: 47716   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 269   score: 0.0   memory length: 47839   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 270   score: 3.0   memory length: 48103   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 271   score: 0.0   memory length: 48226   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 272   score: 2.0   memory length: 48444   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 273   score: 3.0   memory length: 48714   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 274   score: 0.0   memory length: 48837   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 275   score: 4.0   memory length: 49123   epsilon: 1.0    steps: 286    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 276   score: 0.0   memory length: 49245   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 277   score: 2.0   memory length: 49463   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 278   score: 2.0   memory length: 49660   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 279   score: 1.0   memory length: 49829   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 280   score: 3.0   memory length: 50055   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 281   score: 0.0   memory length: 50178   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 282   score: 1.0   memory length: 50328   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 283   score: 1.0   memory length: 50480   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 284   score: 0.0   memory length: 50603   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 285   score: 3.0   memory length: 50831   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 286   score: 1.0   memory length: 51001   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 287   score: 0.0   memory length: 51124   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 288   score: 1.0   memory length: 51293   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 289   score: 0.0   memory length: 51416   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 290   score: 3.0   memory length: 51682   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 291   score: 1.0   memory length: 51850   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 292   score: 0.0   memory length: 51972   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 293   score: 1.0   memory length: 52141   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 294   score: 1.0   memory length: 52292   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 295   score: 3.0   memory length: 52499   epsilon: 1.0    steps: 207    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 296   score: 4.0   memory length: 52758   epsilon: 1.0    steps: 259    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 297   score: 0.0   memory length: 52881   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 298   score: 0.0   memory length: 53003   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 299   score: 0.0   memory length: 53126   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 300   score: 3.0   memory length: 53394   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 301   score: 0.0   memory length: 53516   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 302   score: 0.0   memory length: 53638   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 303   score: 1.0   memory length: 53807   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 304   score: 2.0   memory length: 54005   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 305   score: 1.0   memory length: 54156   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 306   score: 2.0   memory length: 54354   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 307   score: 0.0   memory length: 54477   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 308   score: 0.0   memory length: 54600   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 309   score: 3.0   memory length: 54863   epsilon: 1.0    steps: 263    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 310   score: 1.0   memory length: 55013   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 311   score: 0.0   memory length: 55136   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 312   score: 0.0   memory length: 55259   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 313   score: 1.0   memory length: 55428   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 314   score: 1.0   memory length: 55599   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 315   score: 3.0   memory length: 55848   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 316   score: 3.0   memory length: 56094   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 317   score: 1.0   memory length: 56245   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 318   score: 1.0   memory length: 56395   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 319   score: 2.0   memory length: 56592   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 320   score: 3.0   memory length: 56835   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 321   score: 1.0   memory length: 56985   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 322   score: 1.0   memory length: 57136   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 323   score: 0.0   memory length: 57258   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 324   score: 1.0   memory length: 57427   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 325   score: 2.0   memory length: 57624   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 326   score: 3.0   memory length: 57872   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 327   score: 0.0   memory length: 57995   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 328   score: 3.0   memory length: 58245   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 329   score: 2.0   memory length: 58426   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 330   score: 2.0   memory length: 58626   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 331   score: 2.0   memory length: 58823   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 332   score: 1.0   memory length: 58992   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 333   score: 0.0   memory length: 59115   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 334   score: 2.0   memory length: 59313   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 335   score: 3.0   memory length: 59559   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 336   score: 0.0   memory length: 59682   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 337   score: 5.0   memory length: 60005   epsilon: 1.0    steps: 323    lr: 0.0001     evaluation reward: 1.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 338   score: 2.0   memory length: 60223   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 339   score: 2.0   memory length: 60441   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 340   score: 2.0   memory length: 60664   epsilon: 1.0    steps: 223    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 341   score: 0.0   memory length: 60787   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 342   score: 2.0   memory length: 60988   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 343   score: 0.0   memory length: 61111   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 344   score: 0.0   memory length: 61233   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 345   score: 4.0   memory length: 61507   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 346   score: 4.0   memory length: 61801   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 347   score: 3.0   memory length: 62068   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 348   score: 3.0   memory length: 62298   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 349   score: 5.0   memory length: 62621   epsilon: 1.0    steps: 323    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 350   score: 2.0   memory length: 62819   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 351   score: 0.0   memory length: 62942   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 352   score: 0.0   memory length: 63064   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 353   score: 0.0   memory length: 63186   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 354   score: 3.0   memory length: 63454   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 355   score: 3.0   memory length: 63718   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 356   score: 0.0   memory length: 63841   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 357   score: 1.0   memory length: 63992   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 358   score: 0.0   memory length: 64115   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 359   score: 1.0   memory length: 64265   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 360   score: 2.0   memory length: 64486   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 361   score: 2.0   memory length: 64705   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 362   score: 1.0   memory length: 64874   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 363   score: 2.0   memory length: 65072   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 364   score: 1.0   memory length: 65241   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 365   score: 0.0   memory length: 65364   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 366   score: 2.0   memory length: 65562   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 367   score: 2.0   memory length: 65760   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 368   score: 1.0   memory length: 65910   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 369   score: 1.0   memory length: 66079   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 370   score: 2.0   memory length: 66296   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 371   score: 2.0   memory length: 66513   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 372   score: 2.0   memory length: 66712   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 373   score: 1.0   memory length: 66884   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 374   score: 2.0   memory length: 67082   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 375   score: 4.0   memory length: 67395   epsilon: 1.0    steps: 313    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 376   score: 1.0   memory length: 67565   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 377   score: 3.0   memory length: 67790   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 378   score: 0.0   memory length: 67913   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 379   score: 3.0   memory length: 68161   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 380   score: 1.0   memory length: 68330   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 381   score: 2.0   memory length: 68512   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 382   score: 2.0   memory length: 68730   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 383   score: 0.0   memory length: 68853   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 384   score: 2.0   memory length: 69071   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 385   score: 1.0   memory length: 69240   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 386   score: 0.0   memory length: 69362   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 387   score: 2.0   memory length: 69578   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 388   score: 1.0   memory length: 69748   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 389   score: 3.0   memory length: 69993   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 390   score: 2.0   memory length: 70211   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 391   score: 0.0   memory length: 70334   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 392   score: 2.0   memory length: 70535   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 393   score: 3.0   memory length: 70783   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 394   score: 1.0   memory length: 70954   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 395   score: 2.0   memory length: 71153   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 396   score: 2.0   memory length: 71370   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 397   score: 2.0   memory length: 71567   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 398   score: 2.0   memory length: 71765   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 399   score: 3.0   memory length: 72029   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 400   score: 2.0   memory length: 72211   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 401   score: 2.0   memory length: 72428   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 402   score: 0.0   memory length: 72551   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 403   score: 1.0   memory length: 72702   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 404   score: 1.0   memory length: 72853   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 405   score: 0.0   memory length: 72976   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 406   score: 1.0   memory length: 73148   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 407   score: 2.0   memory length: 73365   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 408   score: 1.0   memory length: 73537   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 409   score: 0.0   memory length: 73659   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 410   score: 4.0   memory length: 73935   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 411   score: 1.0   memory length: 74086   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 412   score: 0.0   memory length: 74209   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 413   score: 1.0   memory length: 74378   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 414   score: 0.0   memory length: 74501   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 415   score: 3.0   memory length: 74747   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 416   score: 2.0   memory length: 74946   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 417   score: 1.0   memory length: 75097   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 418   score: 0.0   memory length: 75219   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 419   score: 4.0   memory length: 75509   epsilon: 1.0    steps: 290    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 420   score: 3.0   memory length: 75755   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 421   score: 2.0   memory length: 75953   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 422   score: 2.0   memory length: 76170   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 423   score: 2.0   memory length: 76368   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 424   score: 1.0   memory length: 76537   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 425   score: 1.0   memory length: 76706   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 426   score: 2.0   memory length: 76904   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 427   score: 0.0   memory length: 77026   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 428   score: 1.0   memory length: 77195   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 429   score: 2.0   memory length: 77393   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 430   score: 2.0   memory length: 77614   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 431   score: 3.0   memory length: 77882   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 432   score: 1.0   memory length: 78054   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 433   score: 0.0   memory length: 78176   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 434   score: 0.0   memory length: 78298   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 435   score: 0.0   memory length: 78421   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 436   score: 5.0   memory length: 78720   epsilon: 1.0    steps: 299    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 437   score: 3.0   memory length: 78966   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 438   score: 6.0   memory length: 79332   epsilon: 1.0    steps: 366    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 439   score: 0.0   memory length: 79454   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 440   score: 1.0   memory length: 79605   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 441   score: 0.0   memory length: 79728   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 442   score: 0.0   memory length: 79850   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 443   score: 3.0   memory length: 80101   epsilon: 1.0    steps: 251    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 444   score: 2.0   memory length: 80319   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 445   score: 0.0   memory length: 80442   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 446   score: 3.0   memory length: 80667   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 447   score: 1.0   memory length: 80817   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 448   score: 4.0   memory length: 81114   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 449   score: 0.0   memory length: 81236   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 450   score: 0.0   memory length: 81359   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 451   score: 1.0   memory length: 81530   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 452   score: 2.0   memory length: 81728   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 453   score: 3.0   memory length: 81974   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 454   score: 0.0   memory length: 82096   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 455   score: 2.0   memory length: 82293   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 456   score: 1.0   memory length: 82461   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 457   score: 0.0   memory length: 82583   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 458   score: 4.0   memory length: 82900   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 459   score: 3.0   memory length: 83127   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 460   score: 2.0   memory length: 83307   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 461   score: 2.0   memory length: 83525   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 462   score: 3.0   memory length: 83770   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 463   score: 0.0   memory length: 83892   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 464   score: 2.0   memory length: 84110   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 465   score: 3.0   memory length: 84336   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 466   score: 2.0   memory length: 84534   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 467   score: 0.0   memory length: 84656   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 468   score: 1.0   memory length: 84824   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 469   score: 2.0   memory length: 85042   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 470   score: 1.0   memory length: 85192   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 471   score: 1.0   memory length: 85343   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 472   score: 0.0   memory length: 85465   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 473   score: 3.0   memory length: 85710   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 474   score: 1.0   memory length: 85878   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 475   score: 1.0   memory length: 86046   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 476   score: 1.0   memory length: 86199   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 477   score: 1.0   memory length: 86371   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 478   score: 2.0   memory length: 86568   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 479   score: 0.0   memory length: 86691   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 480   score: 1.0   memory length: 86842   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 481   score: 0.0   memory length: 86965   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 482   score: 3.0   memory length: 87190   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 483   score: 0.0   memory length: 87313   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 484   score: 0.0   memory length: 87436   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 485   score: 1.0   memory length: 87587   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 486   score: 0.0   memory length: 87710   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 487   score: 1.0   memory length: 87880   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 488   score: 0.0   memory length: 88002   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 489   score: 4.0   memory length: 88277   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 490   score: 0.0   memory length: 88400   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 491   score: 6.0   memory length: 88754   epsilon: 1.0    steps: 354    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 492   score: 0.0   memory length: 88877   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 493   score: 2.0   memory length: 89075   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 494   score: 0.0   memory length: 89197   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 495   score: 3.0   memory length: 89426   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 496   score: 0.0   memory length: 89548   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 497   score: 1.0   memory length: 89717   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 498   score: 2.0   memory length: 89919   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 499   score: 1.0   memory length: 90069   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 500   score: 0.0   memory length: 90192   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 501   score: 1.0   memory length: 90364   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 502   score: 3.0   memory length: 90610   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 503   score: 1.0   memory length: 90779   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 504   score: 1.0   memory length: 90951   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 505   score: 1.0   memory length: 91102   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 506   score: 0.0   memory length: 91225   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 507   score: 1.0   memory length: 91376   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 508   score: 3.0   memory length: 91587   epsilon: 1.0    steps: 211    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 509   score: 0.0   memory length: 91710   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 510   score: 2.0   memory length: 91928   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 511   score: 0.0   memory length: 92051   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 512   score: 0.0   memory length: 92174   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 513   score: 2.0   memory length: 92372   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 514   score: 0.0   memory length: 92494   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 515   score: 5.0   memory length: 92804   epsilon: 1.0    steps: 310    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 516   score: 3.0   memory length: 93017   epsilon: 1.0    steps: 213    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 517   score: 0.0   memory length: 93140   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 518   score: 0.0   memory length: 93262   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 519   score: 2.0   memory length: 93479   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 520   score: 1.0   memory length: 93651   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 521   score: 0.0   memory length: 93774   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 522   score: 2.0   memory length: 93972   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 523   score: 3.0   memory length: 94218   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 524   score: 0.0   memory length: 94341   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 525   score: 8.0   memory length: 94692   epsilon: 1.0    steps: 351    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 526   score: 1.0   memory length: 94861   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 527   score: 0.0   memory length: 94984   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 528   score: 3.0   memory length: 95231   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 529   score: 0.0   memory length: 95354   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 530   score: 0.0   memory length: 95477   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 531   score: 0.0   memory length: 95600   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 532   score: 2.0   memory length: 95797   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 533   score: 0.0   memory length: 95920   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 534   score: 1.0   memory length: 96089   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 535   score: 0.0   memory length: 96211   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 536   score: 0.0   memory length: 96333   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 537   score: 1.0   memory length: 96502   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 538   score: 1.0   memory length: 96672   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 539   score: 2.0   memory length: 96869   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 540   score: 1.0   memory length: 97020   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 541   score: 2.0   memory length: 97238   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 542   score: 0.0   memory length: 97361   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 543   score: 0.0   memory length: 97484   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 544   score: 0.0   memory length: 97607   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 545   score: 3.0   memory length: 97851   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 546   score: 0.0   memory length: 97974   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 547   score: 1.0   memory length: 98144   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 548   score: 2.0   memory length: 98342   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 549   score: 1.0   memory length: 98510   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 550   score: 1.0   memory length: 98679   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 551   score: 3.0   memory length: 98925   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 552   score: 1.0   memory length: 99095   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 553   score: 12.0   memory length: 99579   epsilon: 1.0    steps: 484    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 554   score: 4.0   memory length: 99852   epsilon: 1.0    steps: 273    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 555   score: 2.0   memory length: 100050   epsilon: 0.9998990200000022    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 556   score: 2.0   memory length: 100248   epsilon: 0.9995069800000107    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 557   score: 0.0   memory length: 100371   epsilon: 0.999263440000016    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 558   score: 0.0   memory length: 100493   epsilon: 0.9990218800000212    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 559   score: 1.0   memory length: 100663   epsilon: 0.9986852800000285    steps: 170    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 560   score: 2.0   memory length: 100861   epsilon: 0.998293240000037    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 561   score: 1.0   memory length: 101032   epsilon: 0.9979546600000444    steps: 171    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 562   score: 3.0   memory length: 101279   epsilon: 0.997465600000055    steps: 247    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 563   score: 3.0   memory length: 101525   epsilon: 0.9969785200000656    steps: 246    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 564   score: 1.0   memory length: 101676   epsilon: 0.9966795400000721    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 565   score: 2.0   memory length: 101874   epsilon: 0.9962875000000806    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 566   score: 2.0   memory length: 102053   epsilon: 0.9959330800000883    steps: 179    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 567   score: 0.0   memory length: 102175   epsilon: 0.9956915200000935    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 568   score: 1.0   memory length: 102345   epsilon: 0.9953549200001008    steps: 170    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 569   score: 2.0   memory length: 102563   epsilon: 0.9949232800001102    steps: 218    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 570   score: 2.0   memory length: 102761   epsilon: 0.9945312400001187    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 571   score: 0.0   memory length: 102884   epsilon: 0.994287700000124    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 572   score: 0.0   memory length: 103007   epsilon: 0.9940441600001293    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 573   score: 0.0   memory length: 103129   epsilon: 0.9938026000001345    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 574   score: 0.0   memory length: 103251   epsilon: 0.9935610400001398    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 575   score: 2.0   memory length: 103448   epsilon: 0.9931709800001483    steps: 197    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 576   score: 1.0   memory length: 103600   epsilon: 0.9928700200001548    steps: 152    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 577   score: 0.0   memory length: 103723   epsilon: 0.9926264800001601    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 578   score: 2.0   memory length: 103921   epsilon: 0.9922344400001686    steps: 198    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 579   score: 1.0   memory length: 104090   epsilon: 0.9918998200001758    steps: 169    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 580   score: 0.0   memory length: 104213   epsilon: 0.9916562800001811    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 581   score: 1.0   memory length: 104364   epsilon: 0.9913573000001876    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 582   score: 2.0   memory length: 104562   epsilon: 0.9909652600001961    steps: 198    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 583   score: 0.0   memory length: 104685   epsilon: 0.9907217200002014    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 584   score: 0.0   memory length: 104807   epsilon: 0.9904801600002067    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 585   score: 2.0   memory length: 105025   epsilon: 0.990048520000216    steps: 218    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 586   score: 2.0   memory length: 105222   epsilon: 0.9896584600002245    steps: 197    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 587   score: 3.0   memory length: 105468   epsilon: 0.9891713800002351    steps: 246    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 588   score: 4.0   memory length: 105744   epsilon: 0.9886249000002469    steps: 276    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 589   score: 1.0   memory length: 105912   epsilon: 0.9882922600002542    steps: 168    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 590   score: 2.0   memory length: 106109   epsilon: 0.9879022000002626    steps: 197    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 591   score: 3.0   memory length: 106335   epsilon: 0.9874547200002723    steps: 226    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 592   score: 2.0   memory length: 106532   epsilon: 0.9870646600002808    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 593   score: 3.0   memory length: 106760   epsilon: 0.9866132200002906    steps: 228    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 594   score: 1.0   memory length: 106911   epsilon: 0.9863142400002971    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 595   score: 0.0   memory length: 107034   epsilon: 0.9860707000003024    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 596   score: 0.0   memory length: 107157   epsilon: 0.9858271600003077    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 597   score: 0.0   memory length: 107279   epsilon: 0.9855856000003129    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 598   score: 0.0   memory length: 107402   epsilon: 0.9853420600003182    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 599   score: 1.0   memory length: 107553   epsilon: 0.9850430800003247    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 600   score: 1.0   memory length: 107703   epsilon: 0.9847460800003311    steps: 150    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 601   score: 3.0   memory length: 107951   epsilon: 0.9842550400003418    steps: 248    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 602   score: 0.0   memory length: 108074   epsilon: 0.9840115000003471    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 603   score: 0.0   memory length: 108197   epsilon: 0.9837679600003524    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 604   score: 3.0   memory length: 108464   epsilon: 0.9832393000003639    steps: 267    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 605   score: 2.0   memory length: 108662   epsilon: 0.9828472600003724    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 606   score: 0.0   memory length: 108785   epsilon: 0.9826037200003777    steps: 123    lr: 0.0001     evaluation reward: 1.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 607   score: 3.0   memory length: 109029   epsilon: 0.9821206000003881    steps: 244    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 608   score: 2.0   memory length: 109226   epsilon: 0.9817305400003966    steps: 197    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 609   score: 1.0   memory length: 109396   epsilon: 0.9813939400004039    steps: 170    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 610   score: 3.0   memory length: 109665   epsilon: 0.9808613200004155    steps: 269    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 611   score: 1.0   memory length: 109834   epsilon: 0.9805267000004227    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 612   score: 3.0   memory length: 110063   epsilon: 0.9800732800004326    steps: 229    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 613   score: 1.0   memory length: 110234   epsilon: 0.9797347000004399    steps: 171    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 614   score: 0.0   memory length: 110356   epsilon: 0.9794931400004452    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 615   score: 1.0   memory length: 110507   epsilon: 0.9791941600004517    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 616   score: 2.0   memory length: 110704   epsilon: 0.9788041000004601    steps: 197    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 617   score: 2.0   memory length: 110922   epsilon: 0.9783724600004695    steps: 218    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 618   score: 2.0   memory length: 111102   epsilon: 0.9780160600004772    steps: 180    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 619   score: 2.0   memory length: 111302   epsilon: 0.9776200600004858    steps: 200    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 620   score: 1.0   memory length: 111453   epsilon: 0.9773210800004923    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 621   score: 1.0   memory length: 111621   epsilon: 0.9769884400004996    steps: 168    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 622   score: 1.0   memory length: 111791   epsilon: 0.9766518400005069    steps: 170    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 623   score: 2.0   memory length: 112009   epsilon: 0.9762202000005162    steps: 218    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 624   score: 6.0   memory length: 112408   epsilon: 0.9754301800005334    steps: 399    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 625   score: 0.0   memory length: 112531   epsilon: 0.9751866400005387    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 626   score: 1.0   memory length: 112682   epsilon: 0.9748876600005452    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 627   score: 0.0   memory length: 112805   epsilon: 0.9746441200005505    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 628   score: 0.0   memory length: 112927   epsilon: 0.9744025600005557    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 629   score: 4.0   memory length: 113223   epsilon: 0.9738164800005684    steps: 296    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 630   score: 2.0   memory length: 113441   epsilon: 0.9733848400005778    steps: 218    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 631   score: 3.0   memory length: 113687   epsilon: 0.9728977600005884    steps: 246    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 632   score: 4.0   memory length: 113979   epsilon: 0.9723196000006009    steps: 292    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 633   score: 0.0   memory length: 114102   epsilon: 0.9720760600006062    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 634   score: 2.0   memory length: 114299   epsilon: 0.9716860000006147    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 635   score: 0.0   memory length: 114422   epsilon: 0.97144246000062    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 636   score: 1.0   memory length: 114573   epsilon: 0.9711434800006264    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 637   score: 3.0   memory length: 114841   epsilon: 0.970612840000638    steps: 268    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 638   score: 0.0   memory length: 114964   epsilon: 0.9703693000006433    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 639   score: 1.0   memory length: 115136   epsilon: 0.9700287400006506    steps: 172    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 640   score: 0.0   memory length: 115258   epsilon: 0.9697871800006559    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 641   score: 1.0   memory length: 115429   epsilon: 0.9694486000006632    steps: 171    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 642   score: 1.0   memory length: 115598   epsilon: 0.9691139800006705    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 643   score: 1.0   memory length: 115749   epsilon: 0.968815000000677    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 644   score: 0.0   memory length: 115871   epsilon: 0.9685734400006822    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 645   score: 1.0   memory length: 116039   epsilon: 0.9682408000006895    steps: 168    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 646   score: 4.0   memory length: 116358   epsilon: 0.9676091800007032    steps: 319    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 647   score: 5.0   memory length: 116670   epsilon: 0.9669914200007166    steps: 312    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 648   score: 0.0   memory length: 116792   epsilon: 0.9667498600007218    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 649   score: 2.0   memory length: 116990   epsilon: 0.9663578200007303    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 650   score: 0.0   memory length: 117112   epsilon: 0.9661162600007356    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 651   score: 3.0   memory length: 117342   epsilon: 0.9656608600007455    steps: 230    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 652   score: 0.0   memory length: 117465   epsilon: 0.9654173200007508    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 653   score: 2.0   memory length: 117663   epsilon: 0.9650252800007593    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 654   score: 2.0   memory length: 117861   epsilon: 0.9646332400007678    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 655   score: 1.0   memory length: 118012   epsilon: 0.9643342600007743    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 656   score: 2.0   memory length: 118210   epsilon: 0.9639422200007828    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 657   score: 0.0   memory length: 118333   epsilon: 0.9636986800007881    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 658   score: 0.0   memory length: 118456   epsilon: 0.9634551400007934    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 659   score: 1.0   memory length: 118627   epsilon: 0.9631165600008007    steps: 171    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 660   score: 1.0   memory length: 118799   epsilon: 0.9627760000008081    steps: 172    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 661   score: 0.0   memory length: 118922   epsilon: 0.9625324600008134    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 662   score: 2.0   memory length: 119119   epsilon: 0.9621424000008219    steps: 197    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 663   score: 1.0   memory length: 119270   epsilon: 0.9618434200008283    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 664   score: 1.0   memory length: 119442   epsilon: 0.9615028600008357    steps: 172    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 665   score: 2.0   memory length: 119659   epsilon: 0.9610732000008451    steps: 217    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 666   score: 3.0   memory length: 119906   epsilon: 0.9605841400008557    steps: 247    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 667   score: 2.0   memory length: 120106   epsilon: 0.9601881400008643    steps: 200    lr: 0.0001     evaluation reward: 1.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 668   score: 1.0   memory length: 120257   epsilon: 0.9598891600008708    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 669   score: 1.0   memory length: 120426   epsilon: 0.959554540000878    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 670   score: 1.0   memory length: 120596   epsilon: 0.9592179400008853    steps: 170    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 671   score: 2.0   memory length: 120816   epsilon: 0.9587823400008948    steps: 220    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 672   score: 0.0   memory length: 120939   epsilon: 0.9585388000009001    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 673   score: 0.0   memory length: 121062   epsilon: 0.9582952600009054    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 674   score: 3.0   memory length: 121308   epsilon: 0.9578081800009159    steps: 246    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 675   score: 1.0   memory length: 121459   epsilon: 0.9575092000009224    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 676   score: 0.0   memory length: 121581   epsilon: 0.9572676400009277    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 677   score: 1.0   memory length: 121731   epsilon: 0.9569706400009341    steps: 150    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 678   score: 2.0   memory length: 121933   epsilon: 0.9565706800009428    steps: 202    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 679   score: 1.0   memory length: 122101   epsilon: 0.95623804000095    steps: 168    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 680   score: 1.0   memory length: 122272   epsilon: 0.9558994600009574    steps: 171    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 681   score: 0.0   memory length: 122395   epsilon: 0.9556559200009627    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 682   score: 1.0   memory length: 122563   epsilon: 0.9553232800009699    steps: 168    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 683   score: 4.0   memory length: 122819   epsilon: 0.9548164000009809    steps: 256    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 684   score: 0.0   memory length: 122942   epsilon: 0.9545728600009862    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 685   score: 0.0   memory length: 123065   epsilon: 0.9543293200009915    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 686   score: 1.0   memory length: 123235   epsilon: 0.9539927200009988    steps: 170    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 687   score: 3.0   memory length: 123481   epsilon: 0.9535056400010093    steps: 246    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 688   score: 4.0   memory length: 123779   epsilon: 0.9529156000010222    steps: 298    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 689   score: 1.0   memory length: 123930   epsilon: 0.9526166200010286    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 690   score: 2.0   memory length: 124128   epsilon: 0.9522245800010372    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 691   score: 2.0   memory length: 124325   epsilon: 0.9518345200010456    steps: 197    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 692   score: 2.0   memory length: 124543   epsilon: 0.951402880001055    steps: 218    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 693   score: 0.0   memory length: 124666   epsilon: 0.9511593400010603    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 694   score: 0.0   memory length: 124788   epsilon: 0.9509177800010655    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 695   score: 2.0   memory length: 124988   epsilon: 0.9505217800010741    steps: 200    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 696   score: 1.0   memory length: 125156   epsilon: 0.9501891400010813    steps: 168    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 697   score: 3.0   memory length: 125404   epsilon: 0.949698100001092    steps: 248    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 698   score: 2.0   memory length: 125603   epsilon: 0.9493040800011006    steps: 199    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 699   score: 0.0   memory length: 125726   epsilon: 0.9490605400011058    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 700   score: 3.0   memory length: 125974   epsilon: 0.9485695000011165    steps: 248    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 701   score: 0.0   memory length: 126096   epsilon: 0.9483279400011217    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 702   score: 1.0   memory length: 126265   epsilon: 0.947993320001129    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 703   score: 3.0   memory length: 126514   epsilon: 0.9475003000011397    steps: 249    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 704   score: 2.0   memory length: 126712   epsilon: 0.9471082600011482    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 705   score: 3.0   memory length: 126923   epsilon: 0.9466904800011573    steps: 211    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 706   score: 2.0   memory length: 127141   epsilon: 0.9462588400011667    steps: 218    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 707   score: 1.0   memory length: 127309   epsilon: 0.9459262000011739    steps: 168    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 708   score: 0.0   memory length: 127431   epsilon: 0.9456846400011791    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 709   score: 2.0   memory length: 127629   epsilon: 0.9452926000011876    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 710   score: 0.0   memory length: 127752   epsilon: 0.9450490600011929    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 711   score: 1.0   memory length: 127921   epsilon: 0.9447144400012002    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 712   score: 2.0   memory length: 128118   epsilon: 0.9443243800012087    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 713   score: 2.0   memory length: 128316   epsilon: 0.9439323400012172    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 714   score: 3.0   memory length: 128563   epsilon: 0.9434432800012278    steps: 247    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 715   score: 2.0   memory length: 128780   epsilon: 0.9430136200012371    steps: 217    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 716   score: 0.0   memory length: 128902   epsilon: 0.9427720600012424    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 717   score: 0.0   memory length: 129024   epsilon: 0.9425305000012476    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 718   score: 0.0   memory length: 129147   epsilon: 0.9422869600012529    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 719   score: 0.0   memory length: 129270   epsilon: 0.9420434200012582    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 720   score: 1.0   memory length: 129439   epsilon: 0.9417088000012654    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 721   score: 2.0   memory length: 129641   epsilon: 0.9413088400012741    steps: 202    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 722   score: 3.0   memory length: 129868   epsilon: 0.9408593800012839    steps: 227    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 723   score: 2.0   memory length: 130087   epsilon: 0.9404257600012933    steps: 219    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 724   score: 2.0   memory length: 130302   epsilon: 0.9400000600013025    steps: 215    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 725   score: 0.0   memory length: 130425   epsilon: 0.9397565200013078    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 726   score: 1.0   memory length: 130577   epsilon: 0.9394555600013144    steps: 152    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 727   score: 2.0   memory length: 130794   epsilon: 0.9390259000013237    steps: 217    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 728   score: 2.0   memory length: 131013   epsilon: 0.9385922800013331    steps: 219    lr: 0.0001     evaluation reward: 1.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 729   score: 4.0   memory length: 131291   epsilon: 0.938041840001345    steps: 278    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 730   score: 1.0   memory length: 131461   epsilon: 0.9377052400013524    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 731   score: 2.0   memory length: 131658   epsilon: 0.9373151800013608    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 732   score: 4.0   memory length: 131958   epsilon: 0.9367211800013737    steps: 300    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 733   score: 1.0   memory length: 132128   epsilon: 0.936384580001381    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 734   score: 2.0   memory length: 132326   epsilon: 0.9359925400013895    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 735   score: 1.0   memory length: 132476   epsilon: 0.935695540001396    steps: 150    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 736   score: 0.0   memory length: 132598   epsilon: 0.9354539800014012    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 737   score: 2.0   memory length: 132816   epsilon: 0.9350223400014106    steps: 218    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 738   score: 1.0   memory length: 132987   epsilon: 0.934683760001418    steps: 171    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 739   score: 3.0   memory length: 133254   epsilon: 0.9341551000014294    steps: 267    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 740   score: 0.0   memory length: 133377   epsilon: 0.9339115600014347    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 741   score: 1.0   memory length: 133528   epsilon: 0.9336125800014412    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 742   score: 0.0   memory length: 133651   epsilon: 0.9333690400014465    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 743   score: 3.0   memory length: 133899   epsilon: 0.9328780000014572    steps: 248    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 744   score: 2.0   memory length: 134116   epsilon: 0.9324483400014665    steps: 217    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 745   score: 1.0   memory length: 134285   epsilon: 0.9321137200014737    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 746   score: 4.0   memory length: 134562   epsilon: 0.9315652600014857    steps: 277    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 747   score: 0.0   memory length: 134685   epsilon: 0.9313217200014909    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 748   score: 1.0   memory length: 134835   epsilon: 0.9310247200014974    steps: 150    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 749   score: 3.0   memory length: 135081   epsilon: 0.930537640001508    steps: 246    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 750   score: 1.0   memory length: 135252   epsilon: 0.9301990600015153    steps: 171    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 751   score: 1.0   memory length: 135424   epsilon: 0.9298585000015227    steps: 172    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 752   score: 1.0   memory length: 135593   epsilon: 0.92952388000153    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 753   score: 0.0   memory length: 135716   epsilon: 0.9292803400015353    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 754   score: 1.0   memory length: 135866   epsilon: 0.9289833400015417    steps: 150    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 755   score: 1.0   memory length: 136017   epsilon: 0.9286843600015482    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 756   score: 1.0   memory length: 136168   epsilon: 0.9283853800015547    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 757   score: 0.0   memory length: 136291   epsilon: 0.92814184000156    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 758   score: 0.0   memory length: 136413   epsilon: 0.9279002800015652    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 759   score: 0.0   memory length: 136536   epsilon: 0.9276567400015705    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 760   score: 0.0   memory length: 136658   epsilon: 0.9274151800015757    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 761   score: 1.0   memory length: 136827   epsilon: 0.927080560001583    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 762   score: 3.0   memory length: 137076   epsilon: 0.9265875400015937    steps: 249    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 763   score: 1.0   memory length: 137245   epsilon: 0.926252920001601    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 764   score: 2.0   memory length: 137462   epsilon: 0.9258232600016103    steps: 217    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 765   score: 5.0   memory length: 137792   epsilon: 0.9251698600016245    steps: 330    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 766   score: 2.0   memory length: 137990   epsilon: 0.924777820001633    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 767   score: 2.0   memory length: 138188   epsilon: 0.9243857800016415    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 768   score: 2.0   memory length: 138406   epsilon: 0.9239541400016509    steps: 218    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 769   score: 0.0   memory length: 138528   epsilon: 0.9237125800016561    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 770   score: 0.0   memory length: 138651   epsilon: 0.9234690400016614    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 771   score: 1.0   memory length: 138802   epsilon: 0.9231700600016679    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 772   score: 4.0   memory length: 139057   epsilon: 0.9226651600016789    steps: 255    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 773   score: 3.0   memory length: 139302   epsilon: 0.9221800600016894    steps: 245    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 774   score: 2.0   memory length: 139520   epsilon: 0.9217484200016988    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 775   score: 4.0   memory length: 139799   epsilon: 0.9211960000017108    steps: 279    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 776   score: 1.0   memory length: 139968   epsilon: 0.920861380001718    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 777   score: 1.0   memory length: 140119   epsilon: 0.9205624000017245    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 778   score: 0.0   memory length: 140241   epsilon: 0.9203208400017298    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 779   score: 3.0   memory length: 140510   epsilon: 0.9197882200017413    steps: 269    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 780   score: 3.0   memory length: 140757   epsilon: 0.9192991600017519    steps: 247    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 781   score: 0.0   memory length: 140879   epsilon: 0.9190576000017572    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 782   score: 2.0   memory length: 141077   epsilon: 0.9186655600017657    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 783   score: 0.0   memory length: 141200   epsilon: 0.918422020001771    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 784   score: 1.0   memory length: 141350   epsilon: 0.9181250200017774    steps: 150    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 785   score: 2.0   memory length: 141548   epsilon: 0.9177329800017859    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 786   score: 2.0   memory length: 141766   epsilon: 0.9173013400017953    steps: 218    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 787   score: 1.0   memory length: 141934   epsilon: 0.9169687000018025    steps: 168    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 788   score: 3.0   memory length: 142161   epsilon: 0.9165192400018123    steps: 227    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 789   score: 0.0   memory length: 142284   epsilon: 0.9162757000018176    steps: 123    lr: 0.0001     evaluation reward: 1.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 790   score: 3.0   memory length: 142511   epsilon: 0.9158262400018273    steps: 227    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 791   score: 1.0   memory length: 142683   epsilon: 0.9154856800018347    steps: 172    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 792   score: 1.0   memory length: 142852   epsilon: 0.915151060001842    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 793   score: 0.0   memory length: 142975   epsilon: 0.9149075200018473    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 794   score: 1.0   memory length: 143144   epsilon: 0.9145729000018545    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 795   score: 0.0   memory length: 143267   epsilon: 0.9143293600018598    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 796   score: 3.0   memory length: 143536   epsilon: 0.9137967400018714    steps: 269    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 797   score: 0.0   memory length: 143659   epsilon: 0.9135532000018767    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 798   score: 1.0   memory length: 143830   epsilon: 0.913214620001884    steps: 171    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 799   score: 2.0   memory length: 144028   epsilon: 0.9128225800018925    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 800   score: 2.0   memory length: 144248   epsilon: 0.912386980001902    steps: 220    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 801   score: 1.0   memory length: 144400   epsilon: 0.9120860200019085    steps: 152    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 802   score: 0.0   memory length: 144523   epsilon: 0.9118424800019138    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 803   score: 1.0   memory length: 144674   epsilon: 0.9115435000019203    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 804   score: 1.0   memory length: 144845   epsilon: 0.9112049200019277    steps: 171    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 805   score: 3.0   memory length: 145073   epsilon: 0.9107534800019375    steps: 228    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 806   score: 0.0   memory length: 145196   epsilon: 0.9105099400019427    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 807   score: 2.0   memory length: 145376   epsilon: 0.9101535400019505    steps: 180    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 808   score: 4.0   memory length: 145665   epsilon: 0.9095813200019629    steps: 289    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 809   score: 1.0   memory length: 145816   epsilon: 0.9092823400019694    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 810   score: 0.0   memory length: 145939   epsilon: 0.9090388000019747    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 811   score: 1.0   memory length: 146109   epsilon: 0.908702200001982    steps: 170    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 812   score: 3.0   memory length: 146358   epsilon: 0.9082091800019927    steps: 249    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 813   score: 1.0   memory length: 146529   epsilon: 0.907870600002    steps: 171    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 814   score: 0.0   memory length: 146652   epsilon: 0.9076270600020053    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 815   score: 0.0   memory length: 146775   epsilon: 0.9073835200020106    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 816   score: 0.0   memory length: 146898   epsilon: 0.9071399800020159    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 817   score: 0.0   memory length: 147021   epsilon: 0.9068964400020212    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 818   score: 1.0   memory length: 147191   epsilon: 0.9065598400020285    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 819   score: 3.0   memory length: 147460   epsilon: 0.9060272200020401    steps: 269    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 820   score: 0.0   memory length: 147583   epsilon: 0.9057836800020453    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 821   score: 0.0   memory length: 147705   epsilon: 0.9055421200020506    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 822   score: 0.0   memory length: 147828   epsilon: 0.9052985800020559    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 823   score: 2.0   memory length: 148026   epsilon: 0.9049065400020644    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 824   score: 2.0   memory length: 148226   epsilon: 0.904510540002073    steps: 200    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 825   score: 2.0   memory length: 148424   epsilon: 0.9041185000020815    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 826   score: 6.0   memory length: 148770   epsilon: 0.9034334200020964    steps: 346    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 827   score: 0.0   memory length: 148892   epsilon: 0.9031918600021016    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 828   score: 0.0   memory length: 149014   epsilon: 0.9029503000021069    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 829   score: 1.0   memory length: 149165   epsilon: 0.9026513200021133    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 830   score: 2.0   memory length: 149363   epsilon: 0.9022592800021219    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 831   score: 2.0   memory length: 149561   epsilon: 0.9018672400021304    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 832   score: 1.0   memory length: 149729   epsilon: 0.9015346000021376    steps: 168    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 833   score: 2.0   memory length: 149927   epsilon: 0.9011425600021461    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 834   score: 2.0   memory length: 150109   epsilon: 0.9007822000021539    steps: 182    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 835   score: 1.0   memory length: 150278   epsilon: 0.9004475800021612    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 836   score: 0.0   memory length: 150401   epsilon: 0.9002040400021665    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 837   score: 2.0   memory length: 150618   epsilon: 0.8997743800021758    steps: 217    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 838   score: 0.0   memory length: 150741   epsilon: 0.8995308400021811    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 839   score: 1.0   memory length: 150892   epsilon: 0.8992318600021876    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 840   score: 1.0   memory length: 151061   epsilon: 0.8988972400021948    steps: 169    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 841   score: 2.0   memory length: 151281   epsilon: 0.8984616400022043    steps: 220    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 842   score: 2.0   memory length: 151479   epsilon: 0.8980696000022128    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 843   score: 3.0   memory length: 151727   epsilon: 0.8975785600022235    steps: 248    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 844   score: 0.0   memory length: 151849   epsilon: 0.8973370000022287    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 845   score: 1.0   memory length: 152019   epsilon: 0.897000400002236    steps: 170    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 846   score: 2.0   memory length: 152238   epsilon: 0.8965667800022454    steps: 219    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 847   score: 0.0   memory length: 152361   epsilon: 0.8963232400022507    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 848   score: 4.0   memory length: 152635   epsilon: 0.8957807200022625    steps: 274    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 849   score: 4.0   memory length: 152932   epsilon: 0.8951926600022753    steps: 297    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 850   score: 1.0   memory length: 153101   epsilon: 0.8948580400022825    steps: 169    lr: 0.0001     evaluation reward: 1.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 851   score: 3.0   memory length: 153369   epsilon: 0.894327400002294    steps: 268    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 852   score: 1.0   memory length: 153520   epsilon: 0.8940284200023005    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 853   score: 0.0   memory length: 153642   epsilon: 0.8937868600023058    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 854   score: 0.0   memory length: 153765   epsilon: 0.8935433200023111    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 855   score: 0.0   memory length: 153888   epsilon: 0.8932997800023164    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 856   score: 1.0   memory length: 154039   epsilon: 0.8930008000023228    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 857   score: 1.0   memory length: 154207   epsilon: 0.8926681600023301    steps: 168    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 858   score: 2.0   memory length: 154425   epsilon: 0.8922365200023394    steps: 218    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 859   score: 2.0   memory length: 154623   epsilon: 0.891844480002348    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 860   score: 3.0   memory length: 154869   epsilon: 0.8913574000023585    steps: 246    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 861   score: 3.0   memory length: 155118   epsilon: 0.8908643800023692    steps: 249    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 862   score: 0.0   memory length: 155241   epsilon: 0.8906208400023745    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 863   score: 1.0   memory length: 155410   epsilon: 0.8902862200023818    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 864   score: 0.0   memory length: 155532   epsilon: 0.890044660002387    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 865   score: 4.0   memory length: 155851   epsilon: 0.8894130400024007    steps: 319    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 866   score: 1.0   memory length: 156019   epsilon: 0.889080400002408    steps: 168    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 867   score: 1.0   memory length: 156170   epsilon: 0.8887814200024144    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 868   score: 1.0   memory length: 156340   epsilon: 0.8884448200024218    steps: 170    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 869   score: 0.0   memory length: 156463   epsilon: 0.888201280002427    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 870   score: 2.0   memory length: 156660   epsilon: 0.8878112200024355    steps: 197    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 871   score: 3.0   memory length: 156907   epsilon: 0.8873221600024461    steps: 247    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 872   score: 1.0   memory length: 157076   epsilon: 0.8869875400024534    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 873   score: 1.0   memory length: 157247   epsilon: 0.8866489600024607    steps: 171    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 874   score: 1.0   memory length: 157415   epsilon: 0.886316320002468    steps: 168    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 875   score: 4.0   memory length: 157711   epsilon: 0.8857302400024807    steps: 296    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 876   score: 2.0   memory length: 157908   epsilon: 0.8853401800024892    steps: 197    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 877   score: 0.0   memory length: 158030   epsilon: 0.8850986200024944    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 878   score: 2.0   memory length: 158227   epsilon: 0.8847085600025029    steps: 197    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 879   score: 0.0   memory length: 158350   epsilon: 0.8844650200025082    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 880   score: 0.0   memory length: 158472   epsilon: 0.8842234600025134    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 881   score: 3.0   memory length: 158700   epsilon: 0.8837720200025232    steps: 228    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 882   score: 1.0   memory length: 158851   epsilon: 0.8834730400025297    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 883   score: 1.0   memory length: 159020   epsilon: 0.883138420002537    steps: 169    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 884   score: 0.0   memory length: 159143   epsilon: 0.8828948800025422    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 885   score: 3.0   memory length: 159388   epsilon: 0.8824097800025528    steps: 245    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 886   score: 2.0   memory length: 159569   epsilon: 0.8820514000025605    steps: 181    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 887   score: 1.0   memory length: 159738   epsilon: 0.8817167800025678    steps: 169    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 888   score: 2.0   memory length: 159938   epsilon: 0.8813207800025764    steps: 200    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 889   score: 0.0   memory length: 160060   epsilon: 0.8810792200025817    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 890   score: 0.0   memory length: 160183   epsilon: 0.8808356800025869    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 891   score: 0.0   memory length: 160306   epsilon: 0.8805921400025922    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 892   score: 2.0   memory length: 160524   epsilon: 0.8801605000026016    steps: 218    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 893   score: 0.0   memory length: 160647   epsilon: 0.8799169600026069    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 894   score: 2.0   memory length: 160844   epsilon: 0.8795269000026154    steps: 197    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 895   score: 0.0   memory length: 160967   epsilon: 0.8792833600026206    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 896   score: 1.0   memory length: 161136   epsilon: 0.8789487400026279    steps: 169    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 897   score: 0.0   memory length: 161259   epsilon: 0.8787052000026332    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 898   score: 3.0   memory length: 161506   epsilon: 0.8782161400026438    steps: 247    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 899   score: 0.0   memory length: 161628   epsilon: 0.877974580002649    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 900   score: 1.0   memory length: 161797   epsilon: 0.8776399600026563    steps: 169    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 901   score: 1.0   memory length: 161966   epsilon: 0.8773053400026636    steps: 169    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 902   score: 2.0   memory length: 162184   epsilon: 0.876873700002673    steps: 218    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 903   score: 3.0   memory length: 162427   epsilon: 0.8763925600026834    steps: 243    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 904   score: 0.0   memory length: 162549   epsilon: 0.8761510000026886    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 905   score: 1.0   memory length: 162717   epsilon: 0.8758183600026959    steps: 168    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 906   score: 1.0   memory length: 162886   epsilon: 0.8754837400027031    steps: 169    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 907   score: 0.0   memory length: 163009   epsilon: 0.8752402000027084    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 908   score: 0.0   memory length: 163132   epsilon: 0.8749966600027137    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 909   score: 2.0   memory length: 163329   epsilon: 0.8746066000027222    steps: 197    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 910   score: 0.0   memory length: 163451   epsilon: 0.8743650400027274    steps: 122    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 911   score: 2.0   memory length: 163673   epsilon: 0.873925480002737    steps: 222    lr: 0.0001     evaluation reward: 1.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 912   score: 3.0   memory length: 163919   epsilon: 0.8734384000027475    steps: 246    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 913   score: 2.0   memory length: 164137   epsilon: 0.8730067600027569    steps: 218    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 914   score: 1.0   memory length: 164305   epsilon: 0.8726741200027641    steps: 168    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 915   score: 2.0   memory length: 164523   epsilon: 0.8722424800027735    steps: 218    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 916   score: 0.0   memory length: 164646   epsilon: 0.8719989400027788    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 917   score: 1.0   memory length: 164797   epsilon: 0.8716999600027853    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 918   score: 1.0   memory length: 164948   epsilon: 0.8714009800027918    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 919   score: 3.0   memory length: 165193   epsilon: 0.8709158800028023    steps: 245    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 920   score: 0.0   memory length: 165316   epsilon: 0.8706723400028076    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 921   score: 0.0   memory length: 165439   epsilon: 0.8704288000028129    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 922   score: 3.0   memory length: 165703   epsilon: 0.8699060800028242    steps: 264    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 923   score: 1.0   memory length: 165853   epsilon: 0.8696090800028307    steps: 150    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 924   score: 3.0   memory length: 166119   epsilon: 0.8690824000028421    steps: 266    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 925   score: 0.0   memory length: 166242   epsilon: 0.8688388600028474    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 926   score: 2.0   memory length: 166460   epsilon: 0.8684072200028568    steps: 218    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 927   score: 0.0   memory length: 166582   epsilon: 0.868165660002862    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 928   score: 1.0   memory length: 166751   epsilon: 0.8678310400028693    steps: 169    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 929   score: 2.0   memory length: 166968   epsilon: 0.8674013800028786    steps: 217    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 930   score: 2.0   memory length: 167166   epsilon: 0.8670093400028871    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 931   score: 3.0   memory length: 167391   epsilon: 0.8665638400028968    steps: 225    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 932   score: 2.0   memory length: 167591   epsilon: 0.8661678400029054    steps: 200    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 933   score: 1.0   memory length: 167760   epsilon: 0.8658332200029126    steps: 169    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 934   score: 0.0   memory length: 167882   epsilon: 0.8655916600029179    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 935   score: 2.0   memory length: 168100   epsilon: 0.8651600200029272    steps: 218    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 936   score: 2.0   memory length: 168318   epsilon: 0.8647283800029366    steps: 218    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 937   score: 0.0   memory length: 168440   epsilon: 0.8644868200029419    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 938   score: 2.0   memory length: 168657   epsilon: 0.8640571600029512    steps: 217    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 939   score: 0.0   memory length: 168780   epsilon: 0.8638136200029565    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 940   score: 1.0   memory length: 168949   epsilon: 0.8634790000029637    steps: 169    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 941   score: 1.0   memory length: 169119   epsilon: 0.863142400002971    steps: 170    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 942   score: 2.0   memory length: 169317   epsilon: 0.8627503600029796    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 943   score: 1.0   memory length: 169485   epsilon: 0.8624177200029868    steps: 168    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 944   score: 1.0   memory length: 169653   epsilon: 0.862085080002994    steps: 168    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 945   score: 1.0   memory length: 169823   epsilon: 0.8617484800030013    steps: 170    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 946   score: 0.0   memory length: 169945   epsilon: 0.8615069200030065    steps: 122    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 947   score: 2.0   memory length: 170126   epsilon: 0.8611485400030143    steps: 181    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 948   score: 0.0   memory length: 170248   epsilon: 0.8609069800030196    steps: 122    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 949   score: 2.0   memory length: 170447   epsilon: 0.8605129600030281    steps: 199    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 950   score: 0.0   memory length: 170569   epsilon: 0.8602714000030334    steps: 122    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 951   score: 3.0   memory length: 170813   epsilon: 0.8597882800030439    steps: 244    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 952   score: 1.0   memory length: 170985   epsilon: 0.8594477200030513    steps: 172    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 953   score: 2.0   memory length: 171165   epsilon: 0.859091320003059    steps: 180    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 954   score: 2.0   memory length: 171363   epsilon: 0.8586992800030675    steps: 198    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 955   score: 1.0   memory length: 171532   epsilon: 0.8583646600030748    steps: 169    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 956   score: 2.0   memory length: 171754   epsilon: 0.8579251000030843    steps: 222    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 957   score: 5.0   memory length: 172099   epsilon: 0.8572420000030991    steps: 345    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 958   score: 0.0   memory length: 172221   epsilon: 0.8570004400031044    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 959   score: 3.0   memory length: 172489   epsilon: 0.8564698000031159    steps: 268    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 960   score: 2.0   memory length: 172708   epsilon: 0.8560361800031253    steps: 219    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 961   score: 2.0   memory length: 172906   epsilon: 0.8556441400031338    steps: 198    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 962   score: 0.0   memory length: 173029   epsilon: 0.8554006000031391    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 963   score: 1.0   memory length: 173179   epsilon: 0.8551036000031456    steps: 150    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 964   score: 2.0   memory length: 173376   epsilon: 0.854713540003154    steps: 197    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 965   score: 2.0   memory length: 173574   epsilon: 0.8543215000031625    steps: 198    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 966   score: 3.0   memory length: 173821   epsilon: 0.8538324400031732    steps: 247    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 967   score: 5.0   memory length: 174166   epsilon: 0.853149340003188    steps: 345    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 968   score: 0.0   memory length: 174289   epsilon: 0.8529058000031933    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 969   score: 0.0   memory length: 174412   epsilon: 0.8526622600031986    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 970   score: 1.0   memory length: 174583   epsilon: 0.8523236800032059    steps: 171    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 971   score: 0.0   memory length: 174705   epsilon: 0.8520821200032112    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 972   score: 2.0   memory length: 174923   epsilon: 0.8516504800032205    steps: 218    lr: 0.0001     evaluation reward: 1.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 973   score: 1.0   memory length: 175074   epsilon: 0.851351500003227    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 974   score: 1.0   memory length: 175244   epsilon: 0.8510149000032343    steps: 170    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 975   score: 3.0   memory length: 175490   epsilon: 0.8505278200032449    steps: 246    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 976   score: 1.0   memory length: 175641   epsilon: 0.8502288400032514    steps: 151    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 977   score: 1.0   memory length: 175810   epsilon: 0.8498942200032586    steps: 169    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 978   score: 0.0   memory length: 175933   epsilon: 0.8496506800032639    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 979   score: 2.0   memory length: 176151   epsilon: 0.8492190400032733    steps: 218    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 980   score: 0.0   memory length: 176273   epsilon: 0.8489774800032786    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 981   score: 0.0   memory length: 176395   epsilon: 0.8487359200032838    steps: 122    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 982   score: 2.0   memory length: 176592   epsilon: 0.8483458600032923    steps: 197    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 983   score: 1.0   memory length: 176742   epsilon: 0.8480488600032987    steps: 150    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 984   score: 2.0   memory length: 176957   epsilon: 0.847623160003308    steps: 215    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 985   score: 2.0   memory length: 177159   epsilon: 0.8472232000033166    steps: 202    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 986   score: 3.0   memory length: 177387   epsilon: 0.8467717600033264    steps: 228    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 987   score: 2.0   memory length: 177607   epsilon: 0.8463361600033359    steps: 220    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 988   score: 1.0   memory length: 177775   epsilon: 0.8460035200033431    steps: 168    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 989   score: 1.0   memory length: 177944   epsilon: 0.8456689000033504    steps: 169    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 990   score: 0.0   memory length: 178066   epsilon: 0.8454273400033556    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 991   score: 0.0   memory length: 178188   epsilon: 0.8451857800033609    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 992   score: 0.0   memory length: 178310   epsilon: 0.8449442200033661    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 993   score: 0.0   memory length: 178433   epsilon: 0.8447006800033714    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 994   score: 2.0   memory length: 178631   epsilon: 0.8443086400033799    steps: 198    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 995   score: 1.0   memory length: 178799   epsilon: 0.8439760000033871    steps: 168    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 996   score: 2.0   memory length: 178996   epsilon: 0.8435859400033956    steps: 197    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 997   score: 1.0   memory length: 179167   epsilon: 0.843247360003403    steps: 171    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 998   score: 2.0   memory length: 179365   epsilon: 0.8428553200034115    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 999   score: 1.0   memory length: 179534   epsilon: 0.8425207000034187    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1000   score: 1.0   memory length: 179685   epsilon: 0.8422217200034252    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1001   score: 0.0   memory length: 179808   epsilon: 0.8419781800034305    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1002   score: 2.0   memory length: 180026   epsilon: 0.8415465400034399    steps: 218    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1003   score: 1.0   memory length: 180177   epsilon: 0.8412475600034464    steps: 151    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1004   score: 3.0   memory length: 180420   epsilon: 0.8407664200034568    steps: 243    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1005   score: 0.0   memory length: 180543   epsilon: 0.8405228800034621    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1006   score: 0.0   memory length: 180666   epsilon: 0.8402793400034674    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1007   score: 2.0   memory length: 180863   epsilon: 0.8398892800034758    steps: 197    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1008   score: 0.0   memory length: 180986   epsilon: 0.8396457400034811    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1009   score: 2.0   memory length: 181184   epsilon: 0.8392537000034896    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1010   score: 0.0   memory length: 181306   epsilon: 0.8390121400034949    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1011   score: 3.0   memory length: 181575   epsilon: 0.8384795200035065    steps: 269    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1012   score: 2.0   memory length: 181776   epsilon: 0.8380815400035151    steps: 201    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1013   score: 0.0   memory length: 181898   epsilon: 0.8378399800035203    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1014   score: 2.0   memory length: 182095   epsilon: 0.8374499200035288    steps: 197    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1015   score: 1.0   memory length: 182264   epsilon: 0.8371153000035361    steps: 169    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1016   score: 1.0   memory length: 182415   epsilon: 0.8368163200035426    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1017   score: 5.0   memory length: 182760   epsilon: 0.8361332200035574    steps: 345    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1018   score: 3.0   memory length: 182988   epsilon: 0.8356817800035672    steps: 228    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1019   score: 2.0   memory length: 183186   epsilon: 0.8352897400035757    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1020   score: 1.0   memory length: 183337   epsilon: 0.8349907600035822    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1021   score: 3.0   memory length: 183581   epsilon: 0.8345076400035927    steps: 244    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1022   score: 0.0   memory length: 183704   epsilon: 0.834264100003598    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1023   score: 3.0   memory length: 183971   epsilon: 0.8337354400036094    steps: 267    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1024   score: 0.0   memory length: 184094   epsilon: 0.8334919000036147    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1025   score: 3.0   memory length: 184341   epsilon: 0.8330028400036253    steps: 247    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1026   score: 4.0   memory length: 184621   epsilon: 0.8324484400036374    steps: 280    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1027   score: 0.0   memory length: 184744   epsilon: 0.8322049000036427    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1028   score: 1.0   memory length: 184915   epsilon: 0.83186632000365    steps: 171    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1029   score: 0.0   memory length: 185038   epsilon: 0.8316227800036553    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1030   score: 0.0   memory length: 185161   epsilon: 0.8313792400036606    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1031   score: 2.0   memory length: 185358   epsilon: 0.8309891800036691    steps: 197    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1032   score: 2.0   memory length: 185556   epsilon: 0.8305971400036776    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1033   score: 2.0   memory length: 185774   epsilon: 0.8301655000036869    steps: 218    lr: 0.0001     evaluation reward: 1.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1034   score: 2.0   memory length: 185972   epsilon: 0.8297734600036955    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1035   score: 2.0   memory length: 186174   epsilon: 0.8293735000037041    steps: 202    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1036   score: 1.0   memory length: 186324   epsilon: 0.8290765000037106    steps: 150    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1037   score: 1.0   memory length: 186492   epsilon: 0.8287438600037178    steps: 168    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1038   score: 2.0   memory length: 186692   epsilon: 0.8283478600037264    steps: 200    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1039   score: 1.0   memory length: 186843   epsilon: 0.8280488800037329    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1040   score: 0.0   memory length: 186966   epsilon: 0.8278053400037382    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1041   score: 0.0   memory length: 187089   epsilon: 0.8275618000037435    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1042   score: 2.0   memory length: 187286   epsilon: 0.8271717400037519    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1043   score: 1.0   memory length: 187437   epsilon: 0.8268727600037584    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1044   score: 3.0   memory length: 187703   epsilon: 0.8263460800037699    steps: 266    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1045   score: 0.0   memory length: 187826   epsilon: 0.8261025400037751    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1046   score: 1.0   memory length: 187995   epsilon: 0.8257679200037824    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1047   score: 1.0   memory length: 188146   epsilon: 0.8254689400037889    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1048   score: 3.0   memory length: 188392   epsilon: 0.8249818600037995    steps: 246    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1049   score: 2.0   memory length: 188592   epsilon: 0.8245858600038081    steps: 200    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1050   score: 0.0   memory length: 188714   epsilon: 0.8243443000038133    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1051   score: 0.0   memory length: 188836   epsilon: 0.8241027400038186    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1052   score: 1.0   memory length: 188987   epsilon: 0.823803760003825    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1053   score: 0.0   memory length: 189110   epsilon: 0.8235602200038303    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1054   score: 2.0   memory length: 189309   epsilon: 0.8231662000038389    steps: 199    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1055   score: 1.0   memory length: 189481   epsilon: 0.8228256400038463    steps: 172    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1056   score: 0.0   memory length: 189603   epsilon: 0.8225840800038515    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1057   score: 2.0   memory length: 189820   epsilon: 0.8221544200038609    steps: 217    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1058   score: 2.0   memory length: 190040   epsilon: 0.8217188200038703    steps: 220    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1059   score: 0.0   memory length: 190162   epsilon: 0.8214772600038756    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1060   score: 2.0   memory length: 190379   epsilon: 0.8210476000038849    steps: 217    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1061   score: 2.0   memory length: 190577   epsilon: 0.8206555600038934    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1062   score: 1.0   memory length: 190727   epsilon: 0.8203585600038998    steps: 150    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1063   score: 1.0   memory length: 190879   epsilon: 0.8200576000039064    steps: 152    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1064   score: 2.0   memory length: 191078   epsilon: 0.8196635800039149    steps: 199    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1065   score: 2.0   memory length: 191275   epsilon: 0.8192735200039234    steps: 197    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1066   score: 0.0   memory length: 191398   epsilon: 0.8190299800039287    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1067   score: 2.0   memory length: 191615   epsilon: 0.818600320003938    steps: 217    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 1068   score: 2.0   memory length: 191812   epsilon: 0.8182102600039465    steps: 197    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1069   score: 1.0   memory length: 191982   epsilon: 0.8178736600039538    steps: 170    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1070   score: 1.0   memory length: 192132   epsilon: 0.8175766600039602    steps: 150    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1071   score: 2.0   memory length: 192347   epsilon: 0.8171509600039695    steps: 215    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1072   score: 1.0   memory length: 192516   epsilon: 0.8168163400039767    steps: 169    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1073   score: 2.0   memory length: 192733   epsilon: 0.8163866800039861    steps: 217    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1074   score: 0.0   memory length: 192855   epsilon: 0.8161451200039913    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1075   score: 0.0   memory length: 192978   epsilon: 0.8159015800039966    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 1076   score: 0.0   memory length: 193101   epsilon: 0.8156580400040019    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 1077   score: 1.0   memory length: 193252   epsilon: 0.8153590600040084    steps: 151    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 1078   score: 3.0   memory length: 193521   epsilon: 0.8148264400040199    steps: 269    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1079   score: 0.0   memory length: 193644   epsilon: 0.8145829000040252    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 1080   score: 2.0   memory length: 193842   epsilon: 0.8141908600040337    steps: 198    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1081   score: 4.0   memory length: 194139   epsilon: 0.8136028000040465    steps: 297    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1082   score: 1.0   memory length: 194311   epsilon: 0.8132622400040539    steps: 172    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1083   score: 1.0   memory length: 194480   epsilon: 0.8129276200040612    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1084   score: 1.0   memory length: 194649   epsilon: 0.8125930000040684    steps: 169    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1085   score: 4.0   memory length: 194965   epsilon: 0.811967320004082    steps: 316    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1086   score: 0.0   memory length: 195087   epsilon: 0.8117257600040872    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1087   score: 0.0   memory length: 195210   epsilon: 0.8114822200040925    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1088   score: 1.0   memory length: 195361   epsilon: 0.811183240004099    steps: 151    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1089   score: 0.0   memory length: 195484   epsilon: 0.8109397000041043    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 1090   score: 2.0   memory length: 195664   epsilon: 0.810583300004112    steps: 180    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1091   score: 0.0   memory length: 195786   epsilon: 0.8103417400041173    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1092   score: 2.0   memory length: 195984   epsilon: 0.8099497000041258    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1093   score: 0.0   memory length: 196106   epsilon: 0.809708140004131    steps: 122    lr: 0.0001     evaluation reward: 1.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1094   score: 4.0   memory length: 196403   epsilon: 0.8091200800041438    steps: 297    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1095   score: 3.0   memory length: 196648   epsilon: 0.8086349800041543    steps: 245    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1096   score: 5.0   memory length: 196969   epsilon: 0.8079994000041681    steps: 321    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1097   score: 2.0   memory length: 197186   epsilon: 0.8075697400041775    steps: 217    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1098   score: 0.0   memory length: 197309   epsilon: 0.8073262000041828    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1099   score: 2.0   memory length: 197526   epsilon: 0.8068965400041921    steps: 217    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1100   score: 2.0   memory length: 197724   epsilon: 0.8065045000042006    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1101   score: 2.0   memory length: 197921   epsilon: 0.8061144400042091    steps: 197    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1102   score: 0.0   memory length: 198044   epsilon: 0.8058709000042144    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1103   score: 1.0   memory length: 198195   epsilon: 0.8055719200042208    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1104   score: 2.0   memory length: 198396   epsilon: 0.8051739400042295    steps: 201    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1105   score: 2.0   memory length: 198594   epsilon: 0.804781900004238    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1106   score: 1.0   memory length: 198745   epsilon: 0.8044829200042445    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1107   score: 2.0   memory length: 198944   epsilon: 0.804088900004253    steps: 199    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1108   score: 3.0   memory length: 199210   epsilon: 0.8035622200042645    steps: 266    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 1109   score: 1.0   memory length: 199362   epsilon: 0.803261260004271    steps: 152    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1110   score: 2.0   memory length: 199560   epsilon: 0.8028692200042795    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 1111   score: 1.0   memory length: 199710   epsilon: 0.802572220004286    steps: 150    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1112   score: 1.0   memory length: 199878   epsilon: 0.8022395800042932    steps: 168    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1113   score: 2.0   memory length: 200076   epsilon: 0.8018475400043017    steps: 198    lr: 4e-05     evaluation reward: 1.46\n",
      "episode: 1114   score: 1.0   memory length: 200245   epsilon: 0.801512920004309    steps: 169    lr: 4e-05     evaluation reward: 1.45\n",
      "episode: 1115   score: 4.0   memory length: 200559   epsilon: 0.8008912000043225    steps: 314    lr: 4e-05     evaluation reward: 1.48\n",
      "episode: 1116   score: 2.0   memory length: 200756   epsilon: 0.8005011400043309    steps: 197    lr: 4e-05     evaluation reward: 1.49\n",
      "episode: 1117   score: 4.0   memory length: 201053   epsilon: 0.7999130800043437    steps: 297    lr: 4e-05     evaluation reward: 1.48\n",
      "episode: 1118   score: 2.0   memory length: 201250   epsilon: 0.7995230200043522    steps: 197    lr: 4e-05     evaluation reward: 1.47\n",
      "episode: 1119   score: 1.0   memory length: 201418   epsilon: 0.7991903800043594    steps: 168    lr: 4e-05     evaluation reward: 1.46\n",
      "episode: 1120   score: 1.0   memory length: 201589   epsilon: 0.7988518000043667    steps: 171    lr: 4e-05     evaluation reward: 1.46\n",
      "episode: 1121   score: 2.0   memory length: 201771   epsilon: 0.7984914400043746    steps: 182    lr: 4e-05     evaluation reward: 1.45\n",
      "episode: 1122   score: 4.0   memory length: 202050   epsilon: 0.7979390200043865    steps: 279    lr: 4e-05     evaluation reward: 1.49\n",
      "episode: 1123   score: 2.0   memory length: 202247   epsilon: 0.797548960004395    steps: 197    lr: 4e-05     evaluation reward: 1.48\n",
      "episode: 1124   score: 4.0   memory length: 202542   epsilon: 0.7969648600044077    steps: 295    lr: 4e-05     evaluation reward: 1.52\n",
      "episode: 1125   score: 0.0   memory length: 202665   epsilon: 0.796721320004413    steps: 123    lr: 4e-05     evaluation reward: 1.49\n",
      "episode: 1126   score: 2.0   memory length: 202864   epsilon: 0.7963273000044215    steps: 199    lr: 4e-05     evaluation reward: 1.47\n",
      "episode: 1127   score: 2.0   memory length: 203062   epsilon: 0.79593526000443    steps: 198    lr: 4e-05     evaluation reward: 1.49\n",
      "episode: 1128   score: 6.0   memory length: 203455   epsilon: 0.7951571200044469    steps: 393    lr: 4e-05     evaluation reward: 1.54\n",
      "episode: 1129   score: 3.0   memory length: 203701   epsilon: 0.7946700400044575    steps: 246    lr: 4e-05     evaluation reward: 1.57\n",
      "episode: 1130   score: 2.0   memory length: 203899   epsilon: 0.794278000004466    steps: 198    lr: 4e-05     evaluation reward: 1.59\n",
      "episode: 1131   score: 3.0   memory length: 204125   epsilon: 0.7938305200044757    steps: 226    lr: 4e-05     evaluation reward: 1.6\n",
      "episode: 1132   score: 3.0   memory length: 204354   epsilon: 0.7933771000044856    steps: 229    lr: 4e-05     evaluation reward: 1.61\n",
      "episode: 1133   score: 2.0   memory length: 204572   epsilon: 0.792945460004495    steps: 218    lr: 4e-05     evaluation reward: 1.61\n",
      "episode: 1134   score: 3.0   memory length: 204784   epsilon: 0.7925257000045041    steps: 212    lr: 4e-05     evaluation reward: 1.62\n",
      "episode: 1135   score: 2.0   memory length: 204982   epsilon: 0.7921336600045126    steps: 198    lr: 4e-05     evaluation reward: 1.62\n",
      "episode: 1136   score: 2.0   memory length: 205179   epsilon: 0.791743600004521    steps: 197    lr: 4e-05     evaluation reward: 1.63\n",
      "episode: 1137   score: 3.0   memory length: 205404   epsilon: 0.7912981000045307    steps: 225    lr: 4e-05     evaluation reward: 1.65\n",
      "episode: 1138   score: 1.0   memory length: 205572   epsilon: 0.7909654600045379    steps: 168    lr: 4e-05     evaluation reward: 1.64\n",
      "episode: 1139   score: 1.0   memory length: 205741   epsilon: 0.7906308400045452    steps: 169    lr: 4e-05     evaluation reward: 1.64\n",
      "episode: 1140   score: 2.0   memory length: 205942   epsilon: 0.7902328600045538    steps: 201    lr: 4e-05     evaluation reward: 1.66\n",
      "episode: 1141   score: 5.0   memory length: 206309   epsilon: 0.7895062000045696    steps: 367    lr: 4e-05     evaluation reward: 1.71\n",
      "episode: 1142   score: 1.0   memory length: 206460   epsilon: 0.7892072200045761    steps: 151    lr: 4e-05     evaluation reward: 1.7\n",
      "episode: 1143   score: 3.0   memory length: 206707   epsilon: 0.7887181600045867    steps: 247    lr: 4e-05     evaluation reward: 1.72\n",
      "episode: 1144   score: 2.0   memory length: 206905   epsilon: 0.7883261200045952    steps: 198    lr: 4e-05     evaluation reward: 1.71\n",
      "episode: 1145   score: 0.0   memory length: 207027   epsilon: 0.7880845600046005    steps: 122    lr: 4e-05     evaluation reward: 1.71\n",
      "episode: 1146   score: 2.0   memory length: 207226   epsilon: 0.787690540004609    steps: 199    lr: 4e-05     evaluation reward: 1.72\n",
      "episode: 1147   score: 2.0   memory length: 207424   epsilon: 0.7872985000046175    steps: 198    lr: 4e-05     evaluation reward: 1.73\n",
      "episode: 1148   score: 3.0   memory length: 207650   epsilon: 0.7868510200046273    steps: 226    lr: 4e-05     evaluation reward: 1.73\n",
      "episode: 1149   score: 0.0   memory length: 207773   epsilon: 0.7866074800046325    steps: 123    lr: 4e-05     evaluation reward: 1.71\n",
      "episode: 1150   score: 2.0   memory length: 207971   epsilon: 0.786215440004641    steps: 198    lr: 4e-05     evaluation reward: 1.73\n",
      "episode: 1151   score: 1.0   memory length: 208122   epsilon: 0.7859164600046475    steps: 151    lr: 4e-05     evaluation reward: 1.74\n",
      "episode: 1152   score: 0.0   memory length: 208244   epsilon: 0.7856749000046528    steps: 122    lr: 4e-05     evaluation reward: 1.73\n",
      "episode: 1153   score: 2.0   memory length: 208461   epsilon: 0.7852452400046621    steps: 217    lr: 4e-05     evaluation reward: 1.75\n",
      "episode: 1154   score: 0.0   memory length: 208584   epsilon: 0.7850017000046674    steps: 123    lr: 4e-05     evaluation reward: 1.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1155   score: 2.0   memory length: 208781   epsilon: 0.7846116400046759    steps: 197    lr: 4e-05     evaluation reward: 1.74\n",
      "episode: 1156   score: 1.0   memory length: 208931   epsilon: 0.7843146400046823    steps: 150    lr: 4e-05     evaluation reward: 1.75\n",
      "episode: 1157   score: 1.0   memory length: 209082   epsilon: 0.7840156600046888    steps: 151    lr: 4e-05     evaluation reward: 1.74\n",
      "episode: 1158   score: 1.0   memory length: 209233   epsilon: 0.7837166800046953    steps: 151    lr: 4e-05     evaluation reward: 1.73\n",
      "episode: 1159   score: 2.0   memory length: 209430   epsilon: 0.7833266200047038    steps: 197    lr: 4e-05     evaluation reward: 1.75\n",
      "episode: 1160   score: 1.0   memory length: 209581   epsilon: 0.7830276400047103    steps: 151    lr: 4e-05     evaluation reward: 1.74\n",
      "episode: 1161   score: 4.0   memory length: 209877   epsilon: 0.782441560004723    steps: 296    lr: 4e-05     evaluation reward: 1.76\n",
      "episode: 1162   score: 1.0   memory length: 210047   epsilon: 0.7821049600047303    steps: 170    lr: 4e-05     evaluation reward: 1.76\n",
      "episode: 1163   score: 3.0   memory length: 210293   epsilon: 0.7816178800047409    steps: 246    lr: 4e-05     evaluation reward: 1.78\n",
      "episode: 1164   score: 3.0   memory length: 210560   epsilon: 0.7810892200047523    steps: 267    lr: 4e-05     evaluation reward: 1.79\n",
      "episode: 1165   score: 2.0   memory length: 210758   epsilon: 0.7806971800047608    steps: 198    lr: 4e-05     evaluation reward: 1.79\n",
      "episode: 1166   score: 2.0   memory length: 210955   epsilon: 0.7803071200047693    steps: 197    lr: 4e-05     evaluation reward: 1.81\n",
      "episode: 1167   score: 3.0   memory length: 211181   epsilon: 0.779859640004779    steps: 226    lr: 4e-05     evaluation reward: 1.82\n",
      "episode: 1168   score: 3.0   memory length: 211429   epsilon: 0.7793686000047897    steps: 248    lr: 4e-05     evaluation reward: 1.83\n",
      "episode: 1169   score: 1.0   memory length: 211580   epsilon: 0.7790696200047962    steps: 151    lr: 4e-05     evaluation reward: 1.83\n",
      "episode: 1170   score: 1.0   memory length: 211731   epsilon: 0.7787706400048027    steps: 151    lr: 4e-05     evaluation reward: 1.83\n",
      "episode: 1171   score: 0.0   memory length: 211854   epsilon: 0.778527100004808    steps: 123    lr: 4e-05     evaluation reward: 1.81\n",
      "episode: 1172   score: 1.0   memory length: 212005   epsilon: 0.7782281200048144    steps: 151    lr: 4e-05     evaluation reward: 1.81\n",
      "episode: 1173   score: 2.0   memory length: 212203   epsilon: 0.777836080004823    steps: 198    lr: 4e-05     evaluation reward: 1.81\n",
      "episode: 1174   score: 0.0   memory length: 212326   epsilon: 0.7775925400048282    steps: 123    lr: 4e-05     evaluation reward: 1.81\n",
      "episode: 1175   score: 3.0   memory length: 212553   epsilon: 0.777143080004838    steps: 227    lr: 4e-05     evaluation reward: 1.84\n",
      "episode: 1176   score: 0.0   memory length: 212676   epsilon: 0.7768995400048433    steps: 123    lr: 4e-05     evaluation reward: 1.84\n",
      "episode: 1177   score: 2.0   memory length: 212874   epsilon: 0.7765075000048518    steps: 198    lr: 4e-05     evaluation reward: 1.85\n",
      "episode: 1178   score: 2.0   memory length: 213072   epsilon: 0.7761154600048603    steps: 198    lr: 4e-05     evaluation reward: 1.84\n",
      "episode: 1179   score: 3.0   memory length: 213301   epsilon: 0.7756620400048702    steps: 229    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1180   score: 4.0   memory length: 213576   epsilon: 0.775117540004882    steps: 275    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1181   score: 2.0   memory length: 213774   epsilon: 0.7747255000048905    steps: 198    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1182   score: 2.0   memory length: 213972   epsilon: 0.774333460004899    steps: 198    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1183   score: 3.0   memory length: 214197   epsilon: 0.7738879600049087    steps: 225    lr: 4e-05     evaluation reward: 1.9\n",
      "episode: 1184   score: 2.0   memory length: 214395   epsilon: 0.7734959200049172    steps: 198    lr: 4e-05     evaluation reward: 1.91\n",
      "episode: 1185   score: 1.0   memory length: 214546   epsilon: 0.7731969400049237    steps: 151    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1186   score: 3.0   memory length: 214792   epsilon: 0.7727098600049342    steps: 246    lr: 4e-05     evaluation reward: 1.91\n",
      "episode: 1187   score: 1.0   memory length: 214943   epsilon: 0.7724108800049407    steps: 151    lr: 4e-05     evaluation reward: 1.92\n",
      "episode: 1188   score: 2.0   memory length: 215159   epsilon: 0.77198320000495    steps: 216    lr: 4e-05     evaluation reward: 1.93\n",
      "episode: 1189   score: 3.0   memory length: 215406   epsilon: 0.7714941400049606    steps: 247    lr: 4e-05     evaluation reward: 1.96\n",
      "episode: 1190   score: 2.0   memory length: 215605   epsilon: 0.7711001200049692    steps: 199    lr: 4e-05     evaluation reward: 1.96\n",
      "episode: 1191   score: 2.0   memory length: 215787   epsilon: 0.770739760004977    steps: 182    lr: 4e-05     evaluation reward: 1.98\n",
      "episode: 1192   score: 3.0   memory length: 216014   epsilon: 0.7702903000049868    steps: 227    lr: 4e-05     evaluation reward: 1.99\n",
      "episode: 1193   score: 2.0   memory length: 216212   epsilon: 0.7698982600049953    steps: 198    lr: 4e-05     evaluation reward: 2.01\n",
      "episode: 1194   score: 1.0   memory length: 216363   epsilon: 0.7695992800050018    steps: 151    lr: 4e-05     evaluation reward: 1.98\n",
      "episode: 1195   score: 2.0   memory length: 216562   epsilon: 0.7692052600050103    steps: 199    lr: 4e-05     evaluation reward: 1.97\n",
      "episode: 1196   score: 0.0   memory length: 216684   epsilon: 0.7689637000050156    steps: 122    lr: 4e-05     evaluation reward: 1.92\n",
      "episode: 1197   score: 4.0   memory length: 216963   epsilon: 0.7684112800050276    steps: 279    lr: 4e-05     evaluation reward: 1.94\n",
      "episode: 1198   score: 2.0   memory length: 217160   epsilon: 0.768021220005036    steps: 197    lr: 4e-05     evaluation reward: 1.96\n",
      "episode: 1199   score: 3.0   memory length: 217406   epsilon: 0.7675341400050466    steps: 246    lr: 4e-05     evaluation reward: 1.97\n",
      "episode: 1200   score: 1.0   memory length: 217574   epsilon: 0.7672015000050538    steps: 168    lr: 4e-05     evaluation reward: 1.96\n",
      "episode: 1201   score: 0.0   memory length: 217696   epsilon: 0.7669599400050591    steps: 122    lr: 4e-05     evaluation reward: 1.94\n",
      "episode: 1202   score: 5.0   memory length: 218020   epsilon: 0.766318420005073    steps: 324    lr: 4e-05     evaluation reward: 1.99\n",
      "episode: 1203   score: 0.0   memory length: 218143   epsilon: 0.7660748800050783    steps: 123    lr: 4e-05     evaluation reward: 1.98\n",
      "episode: 1204   score: 2.0   memory length: 218340   epsilon: 0.7656848200050868    steps: 197    lr: 4e-05     evaluation reward: 1.98\n",
      "episode: 1205   score: 3.0   memory length: 218603   epsilon: 0.7651640800050981    steps: 263    lr: 4e-05     evaluation reward: 1.99\n",
      "episode: 1206   score: 1.0   memory length: 218773   epsilon: 0.7648274800051054    steps: 170    lr: 4e-05     evaluation reward: 1.99\n",
      "episode: 1207   score: 2.0   memory length: 218970   epsilon: 0.7644374200051138    steps: 197    lr: 4e-05     evaluation reward: 1.99\n",
      "episode: 1208   score: 0.0   memory length: 219092   epsilon: 0.7641958600051191    steps: 122    lr: 4e-05     evaluation reward: 1.96\n",
      "episode: 1209   score: 1.0   memory length: 219261   epsilon: 0.7638612400051263    steps: 169    lr: 4e-05     evaluation reward: 1.96\n",
      "episode: 1210   score: 2.0   memory length: 219480   epsilon: 0.7634276200051358    steps: 219    lr: 4e-05     evaluation reward: 1.96\n",
      "episode: 1211   score: 4.0   memory length: 219767   epsilon: 0.7628593600051481    steps: 287    lr: 4e-05     evaluation reward: 1.99\n",
      "episode: 1212   score: 2.0   memory length: 219964   epsilon: 0.7624693000051566    steps: 197    lr: 4e-05     evaluation reward: 2.0\n",
      "episode: 1213   score: 0.0   memory length: 220087   epsilon: 0.7622257600051618    steps: 123    lr: 4e-05     evaluation reward: 1.98\n",
      "episode: 1214   score: 4.0   memory length: 220360   epsilon: 0.7616852200051736    steps: 273    lr: 4e-05     evaluation reward: 2.01\n",
      "episode: 1215   score: 0.0   memory length: 220483   epsilon: 0.7614416800051789    steps: 123    lr: 4e-05     evaluation reward: 1.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1216   score: 1.0   memory length: 220655   epsilon: 0.7611011200051863    steps: 172    lr: 4e-05     evaluation reward: 1.96\n",
      "episode: 1217   score: 3.0   memory length: 220902   epsilon: 0.7606120600051969    steps: 247    lr: 4e-05     evaluation reward: 1.95\n",
      "episode: 1218   score: 3.0   memory length: 221111   epsilon: 0.7601982400052059    steps: 209    lr: 4e-05     evaluation reward: 1.96\n",
      "episode: 1219   score: 0.0   memory length: 221234   epsilon: 0.7599547000052111    steps: 123    lr: 4e-05     evaluation reward: 1.95\n",
      "episode: 1220   score: 2.0   memory length: 221431   epsilon: 0.7595646400052196    steps: 197    lr: 4e-05     evaluation reward: 1.96\n",
      "episode: 1221   score: 3.0   memory length: 221678   epsilon: 0.7590755800052302    steps: 247    lr: 4e-05     evaluation reward: 1.97\n",
      "episode: 1222   score: 1.0   memory length: 221828   epsilon: 0.7587785800052367    steps: 150    lr: 4e-05     evaluation reward: 1.94\n",
      "episode: 1223   score: 0.0   memory length: 221951   epsilon: 0.758535040005242    steps: 123    lr: 4e-05     evaluation reward: 1.92\n",
      "episode: 1224   score: 1.0   memory length: 222121   epsilon: 0.7581984400052493    steps: 170    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1225   score: 4.0   memory length: 222397   epsilon: 0.7576519600052611    steps: 276    lr: 4e-05     evaluation reward: 1.93\n",
      "episode: 1226   score: 4.0   memory length: 222673   epsilon: 0.757105480005273    steps: 276    lr: 4e-05     evaluation reward: 1.95\n",
      "episode: 1227   score: 3.0   memory length: 222920   epsilon: 0.7566164200052836    steps: 247    lr: 4e-05     evaluation reward: 1.96\n",
      "episode: 1228   score: 2.0   memory length: 223100   epsilon: 0.7562600200052914    steps: 180    lr: 4e-05     evaluation reward: 1.92\n",
      "episode: 1229   score: 0.0   memory length: 223223   epsilon: 0.7560164800052966    steps: 123    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1230   score: 2.0   memory length: 223421   epsilon: 0.7556244400053052    steps: 198    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1231   score: 2.0   memory length: 223619   epsilon: 0.7552324000053137    steps: 198    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1232   score: 4.0   memory length: 223911   epsilon: 0.7546542400053262    steps: 292    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1233   score: 3.0   memory length: 224137   epsilon: 0.7542067600053359    steps: 226    lr: 4e-05     evaluation reward: 1.9\n",
      "episode: 1234   score: 4.0   memory length: 224411   epsilon: 0.7536642400053477    steps: 274    lr: 4e-05     evaluation reward: 1.91\n",
      "episode: 1235   score: 6.0   memory length: 224732   epsilon: 0.7530286600053615    steps: 321    lr: 4e-05     evaluation reward: 1.95\n",
      "episode: 1236   score: 4.0   memory length: 225026   epsilon: 0.7524465400053741    steps: 294    lr: 4e-05     evaluation reward: 1.97\n",
      "episode: 1237   score: 2.0   memory length: 225223   epsilon: 0.7520564800053826    steps: 197    lr: 4e-05     evaluation reward: 1.96\n",
      "episode: 1238   score: 1.0   memory length: 225393   epsilon: 0.7517198800053899    steps: 170    lr: 4e-05     evaluation reward: 1.96\n",
      "episode: 1239   score: 2.0   memory length: 225591   epsilon: 0.7513278400053984    steps: 198    lr: 4e-05     evaluation reward: 1.97\n",
      "episode: 1240   score: 3.0   memory length: 225836   epsilon: 0.750842740005409    steps: 245    lr: 4e-05     evaluation reward: 1.98\n",
      "episode: 1241   score: 1.0   memory length: 225987   epsilon: 0.7505437600054155    steps: 151    lr: 4e-05     evaluation reward: 1.94\n",
      "episode: 1242   score: 3.0   memory length: 226212   epsilon: 0.7500982600054251    steps: 225    lr: 4e-05     evaluation reward: 1.96\n",
      "episode: 1243   score: 2.0   memory length: 226410   epsilon: 0.7497062200054336    steps: 198    lr: 4e-05     evaluation reward: 1.95\n",
      "episode: 1244   score: 3.0   memory length: 226636   epsilon: 0.7492587400054433    steps: 226    lr: 4e-05     evaluation reward: 1.96\n",
      "episode: 1245   score: 2.0   memory length: 226834   epsilon: 0.7488667000054519    steps: 198    lr: 4e-05     evaluation reward: 1.98\n",
      "episode: 1246   score: 2.0   memory length: 227031   epsilon: 0.7484766400054603    steps: 197    lr: 4e-05     evaluation reward: 1.98\n",
      "episode: 1247   score: 2.0   memory length: 227228   epsilon: 0.7480865800054688    steps: 197    lr: 4e-05     evaluation reward: 1.98\n",
      "episode: 1248   score: 3.0   memory length: 227472   epsilon: 0.7476034600054793    steps: 244    lr: 4e-05     evaluation reward: 1.98\n",
      "episode: 1249   score: 1.0   memory length: 227623   epsilon: 0.7473044800054858    steps: 151    lr: 4e-05     evaluation reward: 1.99\n",
      "episode: 1250   score: 1.0   memory length: 227773   epsilon: 0.7470074800054922    steps: 150    lr: 4e-05     evaluation reward: 1.98\n",
      "episode: 1251   score: 3.0   memory length: 228020   epsilon: 0.7465184200055028    steps: 247    lr: 4e-05     evaluation reward: 2.0\n",
      "episode: 1252   score: 3.0   memory length: 228246   epsilon: 0.7460709400055126    steps: 226    lr: 4e-05     evaluation reward: 2.03\n",
      "episode: 1253   score: 1.0   memory length: 228398   epsilon: 0.7457699800055191    steps: 152    lr: 4e-05     evaluation reward: 2.02\n",
      "episode: 1254   score: 3.0   memory length: 228645   epsilon: 0.7452809200055297    steps: 247    lr: 4e-05     evaluation reward: 2.05\n",
      "episode: 1255   score: 3.0   memory length: 228911   epsilon: 0.7447542400055411    steps: 266    lr: 4e-05     evaluation reward: 2.06\n",
      "episode: 1256   score: 2.0   memory length: 229109   epsilon: 0.7443622000055496    steps: 198    lr: 4e-05     evaluation reward: 2.07\n",
      "episode: 1257   score: 3.0   memory length: 229353   epsilon: 0.7438790800055601    steps: 244    lr: 4e-05     evaluation reward: 2.09\n",
      "episode: 1258   score: 2.0   memory length: 229550   epsilon: 0.7434890200055686    steps: 197    lr: 4e-05     evaluation reward: 2.1\n",
      "episode: 1259   score: 4.0   memory length: 229810   epsilon: 0.7429742200055798    steps: 260    lr: 4e-05     evaluation reward: 2.12\n",
      "episode: 1260   score: 0.0   memory length: 229932   epsilon: 0.742732660005585    steps: 122    lr: 4e-05     evaluation reward: 2.11\n",
      "episode: 1261   score: 6.0   memory length: 230305   epsilon: 0.741994120005601    steps: 373    lr: 4e-05     evaluation reward: 2.13\n",
      "episode: 1262   score: 4.0   memory length: 230601   epsilon: 0.7414080400056138    steps: 296    lr: 4e-05     evaluation reward: 2.16\n",
      "episode: 1263   score: 1.0   memory length: 230772   epsilon: 0.7410694600056211    steps: 171    lr: 4e-05     evaluation reward: 2.14\n",
      "episode: 1264   score: 2.0   memory length: 230988   epsilon: 0.7406417800056304    steps: 216    lr: 4e-05     evaluation reward: 2.13\n",
      "episode: 1265   score: 2.0   memory length: 231186   epsilon: 0.7402497400056389    steps: 198    lr: 4e-05     evaluation reward: 2.13\n",
      "episode: 1266   score: 4.0   memory length: 231479   epsilon: 0.7396696000056515    steps: 293    lr: 4e-05     evaluation reward: 2.15\n",
      "episode: 1267   score: 2.0   memory length: 231677   epsilon: 0.73927756000566    steps: 198    lr: 4e-05     evaluation reward: 2.14\n",
      "episode: 1268   score: 3.0   memory length: 231924   epsilon: 0.7387885000056706    steps: 247    lr: 4e-05     evaluation reward: 2.14\n",
      "episode: 1269   score: 9.0   memory length: 232340   epsilon: 0.7379648200056885    steps: 416    lr: 4e-05     evaluation reward: 2.22\n",
      "episode: 1270   score: 4.0   memory length: 232636   epsilon: 0.7373787400057012    steps: 296    lr: 4e-05     evaluation reward: 2.25\n",
      "episode: 1271   score: 3.0   memory length: 232882   epsilon: 0.7368916600057118    steps: 246    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1272   score: 3.0   memory length: 233129   epsilon: 0.7364026000057224    steps: 247    lr: 4e-05     evaluation reward: 2.3\n",
      "episode: 1273   score: 1.0   memory length: 233301   epsilon: 0.7360620400057298    steps: 172    lr: 4e-05     evaluation reward: 2.29\n",
      "episode: 1274   score: 1.0   memory length: 233472   epsilon: 0.7357234600057372    steps: 171    lr: 4e-05     evaluation reward: 2.3\n",
      "episode: 1275   score: 3.0   memory length: 233701   epsilon: 0.735270040005747    steps: 229    lr: 4e-05     evaluation reward: 2.3\n",
      "episode: 1276   score: 1.0   memory length: 233852   epsilon: 0.7349710600057535    steps: 151    lr: 4e-05     evaluation reward: 2.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1277   score: 0.0   memory length: 233975   epsilon: 0.7347275200057588    steps: 123    lr: 4e-05     evaluation reward: 2.29\n",
      "episode: 1278   score: 1.0   memory length: 234145   epsilon: 0.7343909200057661    steps: 170    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1279   score: 0.0   memory length: 234268   epsilon: 0.7341473800057714    steps: 123    lr: 4e-05     evaluation reward: 2.25\n",
      "episode: 1280   score: 2.0   memory length: 234484   epsilon: 0.7337197000057807    steps: 216    lr: 4e-05     evaluation reward: 2.23\n",
      "episode: 1281   score: 6.0   memory length: 234880   epsilon: 0.7329356200057977    steps: 396    lr: 4e-05     evaluation reward: 2.27\n",
      "episode: 1282   score: 3.0   memory length: 235106   epsilon: 0.7324881400058074    steps: 226    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1283   score: 2.0   memory length: 235303   epsilon: 0.7320980800058159    steps: 197    lr: 4e-05     evaluation reward: 2.27\n",
      "episode: 1284   score: 3.0   memory length: 235547   epsilon: 0.7316149600058264    steps: 244    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1285   score: 4.0   memory length: 235863   epsilon: 0.73098928000584    steps: 316    lr: 4e-05     evaluation reward: 2.31\n",
      "episode: 1286   score: 1.0   memory length: 236034   epsilon: 0.7306507000058473    steps: 171    lr: 4e-05     evaluation reward: 2.29\n",
      "episode: 1287   score: 3.0   memory length: 236260   epsilon: 0.730203220005857    steps: 226    lr: 4e-05     evaluation reward: 2.31\n",
      "episode: 1288   score: 3.0   memory length: 236486   epsilon: 0.7297557400058667    steps: 226    lr: 4e-05     evaluation reward: 2.32\n",
      "episode: 1289   score: 5.0   memory length: 236828   epsilon: 0.7290785800058814    steps: 342    lr: 4e-05     evaluation reward: 2.34\n",
      "episode: 1290   score: 2.0   memory length: 237046   epsilon: 0.7286469400058908    steps: 218    lr: 4e-05     evaluation reward: 2.34\n",
      "episode: 1291   score: 3.0   memory length: 237313   epsilon: 0.7281182800059023    steps: 267    lr: 4e-05     evaluation reward: 2.35\n",
      "episode: 1292   score: 2.0   memory length: 237511   epsilon: 0.7277262400059108    steps: 198    lr: 4e-05     evaluation reward: 2.34\n",
      "episode: 1293   score: 3.0   memory length: 237737   epsilon: 0.7272787600059205    steps: 226    lr: 4e-05     evaluation reward: 2.35\n",
      "episode: 1294   score: 3.0   memory length: 237963   epsilon: 0.7268312800059302    steps: 226    lr: 4e-05     evaluation reward: 2.37\n",
      "episode: 1295   score: 5.0   memory length: 238291   epsilon: 0.7261818400059443    steps: 328    lr: 4e-05     evaluation reward: 2.4\n",
      "episode: 1296   score: 4.0   memory length: 238566   epsilon: 0.7256373400059561    steps: 275    lr: 4e-05     evaluation reward: 2.44\n",
      "episode: 1297   score: 2.0   memory length: 238764   epsilon: 0.7252453000059647    steps: 198    lr: 4e-05     evaluation reward: 2.42\n",
      "episode: 1298   score: 0.0   memory length: 238887   epsilon: 0.7250017600059699    steps: 123    lr: 4e-05     evaluation reward: 2.4\n",
      "episode: 1299   score: 4.0   memory length: 239181   epsilon: 0.7244196400059826    steps: 294    lr: 4e-05     evaluation reward: 2.41\n",
      "episode: 1300   score: 1.0   memory length: 239350   epsilon: 0.7240850200059898    steps: 169    lr: 4e-05     evaluation reward: 2.41\n",
      "episode: 1301   score: 3.0   memory length: 239612   epsilon: 0.7235662600060011    steps: 262    lr: 4e-05     evaluation reward: 2.44\n",
      "episode: 1302   score: 3.0   memory length: 239860   epsilon: 0.7230752200060118    steps: 248    lr: 4e-05     evaluation reward: 2.42\n",
      "episode: 1303   score: 2.0   memory length: 240057   epsilon: 0.7226851600060202    steps: 197    lr: 4e-05     evaluation reward: 2.44\n",
      "episode: 1304   score: 3.0   memory length: 240270   epsilon: 0.7222634200060294    steps: 213    lr: 4e-05     evaluation reward: 2.45\n",
      "episode: 1305   score: 3.0   memory length: 240496   epsilon: 0.7218159400060391    steps: 226    lr: 4e-05     evaluation reward: 2.45\n",
      "episode: 1306   score: 2.0   memory length: 240714   epsilon: 0.7213843000060485    steps: 218    lr: 4e-05     evaluation reward: 2.46\n",
      "episode: 1307   score: 1.0   memory length: 240886   epsilon: 0.7210437400060559    steps: 172    lr: 4e-05     evaluation reward: 2.45\n",
      "episode: 1308   score: 2.0   memory length: 241084   epsilon: 0.7206517000060644    steps: 198    lr: 4e-05     evaluation reward: 2.47\n",
      "episode: 1309   score: 8.0   memory length: 241548   epsilon: 0.7197329800060843    steps: 464    lr: 4e-05     evaluation reward: 2.54\n",
      "episode: 1310   score: 3.0   memory length: 241796   epsilon: 0.719241940006095    steps: 248    lr: 4e-05     evaluation reward: 2.55\n",
      "episode: 1311   score: 3.0   memory length: 242043   epsilon: 0.7187528800061056    steps: 247    lr: 4e-05     evaluation reward: 2.54\n",
      "episode: 1312   score: 6.0   memory length: 242396   epsilon: 0.7180539400061208    steps: 353    lr: 4e-05     evaluation reward: 2.58\n",
      "episode: 1313   score: 2.0   memory length: 242614   epsilon: 0.7176223000061301    steps: 218    lr: 4e-05     evaluation reward: 2.6\n",
      "episode: 1314   score: 2.0   memory length: 242812   epsilon: 0.7172302600061387    steps: 198    lr: 4e-05     evaluation reward: 2.58\n",
      "episode: 1315   score: 6.0   memory length: 243188   epsilon: 0.7164857800061548    steps: 376    lr: 4e-05     evaluation reward: 2.64\n",
      "episode: 1316   score: 3.0   memory length: 243396   epsilon: 0.7160739400061638    steps: 208    lr: 4e-05     evaluation reward: 2.66\n",
      "episode: 1317   score: 2.0   memory length: 243614   epsilon: 0.7156423000061731    steps: 218    lr: 4e-05     evaluation reward: 2.65\n",
      "episode: 1318   score: 2.0   memory length: 243832   epsilon: 0.7152106600061825    steps: 218    lr: 4e-05     evaluation reward: 2.64\n",
      "episode: 1319   score: 0.0   memory length: 243954   epsilon: 0.7149691000061877    steps: 122    lr: 4e-05     evaluation reward: 2.64\n",
      "episode: 1320   score: 2.0   memory length: 244152   epsilon: 0.7145770600061963    steps: 198    lr: 4e-05     evaluation reward: 2.64\n",
      "episode: 1321   score: 3.0   memory length: 244377   epsilon: 0.7141315600062059    steps: 225    lr: 4e-05     evaluation reward: 2.64\n",
      "episode: 1322   score: 4.0   memory length: 244653   epsilon: 0.7135850800062178    steps: 276    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1323   score: 2.0   memory length: 244870   epsilon: 0.7131554200062271    steps: 217    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1324   score: 0.0   memory length: 244992   epsilon: 0.7129138600062324    steps: 122    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1325   score: 3.0   memory length: 245257   epsilon: 0.7123891600062437    steps: 265    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1326   score: 2.0   memory length: 245454   epsilon: 0.7119991000062522    steps: 197    lr: 4e-05     evaluation reward: 2.65\n",
      "episode: 1327   score: 3.0   memory length: 245679   epsilon: 0.7115536000062619    steps: 225    lr: 4e-05     evaluation reward: 2.65\n",
      "episode: 1328   score: 2.0   memory length: 245879   epsilon: 0.7111576000062705    steps: 200    lr: 4e-05     evaluation reward: 2.65\n",
      "episode: 1329   score: 3.0   memory length: 246126   epsilon: 0.7106685400062811    steps: 247    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1330   score: 1.0   memory length: 246295   epsilon: 0.7103339200062884    steps: 169    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1331   score: 0.0   memory length: 246417   epsilon: 0.7100923600062936    steps: 122    lr: 4e-05     evaluation reward: 2.65\n",
      "episode: 1332   score: 3.0   memory length: 246642   epsilon: 0.7096468600063033    steps: 225    lr: 4e-05     evaluation reward: 2.64\n",
      "episode: 1333   score: 3.0   memory length: 246889   epsilon: 0.7091578000063139    steps: 247    lr: 4e-05     evaluation reward: 2.64\n",
      "episode: 1334   score: 3.0   memory length: 247116   epsilon: 0.7087083400063237    steps: 227    lr: 4e-05     evaluation reward: 2.63\n",
      "episode: 1335   score: 0.0   memory length: 247239   epsilon: 0.7084648000063289    steps: 123    lr: 4e-05     evaluation reward: 2.57\n",
      "episode: 1336   score: 4.0   memory length: 247496   epsilon: 0.70795594000634    steps: 257    lr: 4e-05     evaluation reward: 2.57\n",
      "episode: 1337   score: 3.0   memory length: 247726   epsilon: 0.7075005400063499    steps: 230    lr: 4e-05     evaluation reward: 2.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1338   score: 3.0   memory length: 247952   epsilon: 0.7070530600063596    steps: 226    lr: 4e-05     evaluation reward: 2.6\n",
      "episode: 1339   score: 2.0   memory length: 248171   epsilon: 0.706619440006369    steps: 219    lr: 4e-05     evaluation reward: 2.6\n",
      "episode: 1340   score: 4.0   memory length: 248488   epsilon: 0.7059917800063826    steps: 317    lr: 4e-05     evaluation reward: 2.61\n",
      "episode: 1341   score: 0.0   memory length: 248610   epsilon: 0.7057502200063879    steps: 122    lr: 4e-05     evaluation reward: 2.6\n",
      "episode: 1342   score: 7.0   memory length: 249014   epsilon: 0.7049503000064052    steps: 404    lr: 4e-05     evaluation reward: 2.64\n",
      "episode: 1343   score: 3.0   memory length: 249259   epsilon: 0.7044652000064158    steps: 245    lr: 4e-05     evaluation reward: 2.65\n",
      "episode: 1344   score: 3.0   memory length: 249485   epsilon: 0.7040177200064255    steps: 226    lr: 4e-05     evaluation reward: 2.65\n",
      "episode: 1345   score: 3.0   memory length: 249711   epsilon: 0.7035702400064352    steps: 226    lr: 4e-05     evaluation reward: 2.66\n",
      "episode: 1346   score: 2.0   memory length: 249909   epsilon: 0.7031782000064437    steps: 198    lr: 4e-05     evaluation reward: 2.66\n",
      "episode: 1347   score: 2.0   memory length: 250107   epsilon: 0.7027861600064522    steps: 198    lr: 4e-05     evaluation reward: 2.66\n",
      "episode: 1348   score: 3.0   memory length: 250336   epsilon: 0.7023327400064621    steps: 229    lr: 4e-05     evaluation reward: 2.66\n",
      "episode: 1349   score: 2.0   memory length: 250555   epsilon: 0.7018991200064715    steps: 219    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1350   score: 3.0   memory length: 250781   epsilon: 0.7014516400064812    steps: 226    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1351   score: 4.0   memory length: 251077   epsilon: 0.7008655600064939    steps: 296    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1352   score: 2.0   memory length: 251275   epsilon: 0.7004735200065024    steps: 198    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1353   score: 2.0   memory length: 251473   epsilon: 0.7000814800065109    steps: 198    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1354   score: 4.0   memory length: 251728   epsilon: 0.6995765800065219    steps: 255    lr: 4e-05     evaluation reward: 2.71\n",
      "episode: 1355   score: 3.0   memory length: 251958   epsilon: 0.6991211800065318    steps: 230    lr: 4e-05     evaluation reward: 2.71\n",
      "episode: 1356   score: 1.0   memory length: 252129   epsilon: 0.6987826000065391    steps: 171    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1357   score: 3.0   memory length: 252354   epsilon: 0.6983371000065488    steps: 225    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1358   score: 4.0   memory length: 252629   epsilon: 0.6977926000065606    steps: 275    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1359   score: 3.0   memory length: 252875   epsilon: 0.6973055200065712    steps: 246    lr: 4e-05     evaluation reward: 2.71\n",
      "episode: 1360   score: 3.0   memory length: 253121   epsilon: 0.6968184400065818    steps: 246    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1361   score: 3.0   memory length: 253366   epsilon: 0.6963333400065923    steps: 245    lr: 4e-05     evaluation reward: 2.71\n",
      "episode: 1362   score: 5.0   memory length: 253733   epsilon: 0.6956066800066081    steps: 367    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1363   score: 6.0   memory length: 254069   epsilon: 0.6949414000066225    steps: 336    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1364   score: 4.0   memory length: 254344   epsilon: 0.6943969000066343    steps: 275    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1365   score: 3.0   memory length: 254609   epsilon: 0.6938722000066457    steps: 265    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1366   score: 3.0   memory length: 254835   epsilon: 0.6934247200066554    steps: 226    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1367   score: 4.0   memory length: 255148   epsilon: 0.6928049800066689    steps: 313    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1368   score: 2.0   memory length: 255346   epsilon: 0.6924129400066774    steps: 198    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1369   score: 3.0   memory length: 255592   epsilon: 0.691925860006688    steps: 246    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1370   score: 3.0   memory length: 255839   epsilon: 0.6914368000066986    steps: 247    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1371   score: 3.0   memory length: 256086   epsilon: 0.6909477400067092    steps: 247    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1372   score: 2.0   memory length: 256304   epsilon: 0.6905161000067186    steps: 218    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1373   score: 3.0   memory length: 256533   epsilon: 0.6900626800067284    steps: 229    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1374   score: 3.0   memory length: 256761   epsilon: 0.6896112400067382    steps: 228    lr: 4e-05     evaluation reward: 2.76\n",
      "episode: 1375   score: 3.0   memory length: 256986   epsilon: 0.6891657400067479    steps: 225    lr: 4e-05     evaluation reward: 2.76\n",
      "episode: 1376   score: 2.0   memory length: 257188   epsilon: 0.6887657800067566    steps: 202    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1377   score: 3.0   memory length: 257414   epsilon: 0.6883183000067663    steps: 226    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1378   score: 6.0   memory length: 257810   epsilon: 0.6875342200067833    steps: 396    lr: 4e-05     evaluation reward: 2.85\n",
      "episode: 1379   score: 4.0   memory length: 258107   epsilon: 0.6869461600067961    steps: 297    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1380   score: 2.0   memory length: 258305   epsilon: 0.6865541200068046    steps: 198    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1381   score: 0.0   memory length: 258427   epsilon: 0.6863125600068098    steps: 122    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1382   score: 2.0   memory length: 258625   epsilon: 0.6859205200068184    steps: 198    lr: 4e-05     evaluation reward: 2.82\n",
      "episode: 1383   score: 4.0   memory length: 258922   epsilon: 0.6853324600068311    steps: 297    lr: 4e-05     evaluation reward: 2.84\n",
      "episode: 1384   score: 5.0   memory length: 259231   epsilon: 0.6847206400068444    steps: 309    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1385   score: 2.0   memory length: 259429   epsilon: 0.6843286000068529    steps: 198    lr: 4e-05     evaluation reward: 2.84\n",
      "episode: 1386   score: 2.0   memory length: 259627   epsilon: 0.6839365600068614    steps: 198    lr: 4e-05     evaluation reward: 2.85\n",
      "episode: 1387   score: 2.0   memory length: 259845   epsilon: 0.6835049200068708    steps: 218    lr: 4e-05     evaluation reward: 2.84\n",
      "episode: 1388   score: 2.0   memory length: 260027   epsilon: 0.6831445600068786    steps: 182    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1389   score: 3.0   memory length: 260253   epsilon: 0.6826970800068883    steps: 226    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1390   score: 2.0   memory length: 260435   epsilon: 0.6823367200068962    steps: 182    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1391   score: 3.0   memory length: 260680   epsilon: 0.6818516200069067    steps: 245    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1392   score: 4.0   memory length: 260956   epsilon: 0.6813051400069186    steps: 276    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1393   score: 3.0   memory length: 261183   epsilon: 0.6808556800069283    steps: 227    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1394   score: 3.0   memory length: 261411   epsilon: 0.6804042400069381    steps: 228    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1395   score: 4.0   memory length: 261686   epsilon: 0.6798597400069499    steps: 275    lr: 4e-05     evaluation reward: 2.82\n",
      "episode: 1396   score: 5.0   memory length: 262015   epsilon: 0.6792083200069641    steps: 329    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1397   score: 6.0   memory length: 262368   epsilon: 0.6785093800069792    steps: 353    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1398   score: 1.0   memory length: 262519   epsilon: 0.6782104000069857    steps: 151    lr: 4e-05     evaluation reward: 2.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1399   score: 4.0   memory length: 262816   epsilon: 0.6776223400069985    steps: 297    lr: 4e-05     evaluation reward: 2.88\n",
      "episode: 1400   score: 6.0   memory length: 263160   epsilon: 0.6769412200070133    steps: 344    lr: 4e-05     evaluation reward: 2.93\n",
      "episode: 1401   score: 4.0   memory length: 263436   epsilon: 0.6763947400070252    steps: 276    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1402   score: 6.0   memory length: 263742   epsilon: 0.6757888600070383    steps: 306    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1403   score: 2.0   memory length: 263940   epsilon: 0.6753968200070468    steps: 198    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1404   score: 4.0   memory length: 264241   epsilon: 0.6748008400070598    steps: 301    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1405   score: 3.0   memory length: 264486   epsilon: 0.6743157400070703    steps: 245    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1406   score: 4.0   memory length: 264803   epsilon: 0.6736880800070839    steps: 317    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1407   score: 4.0   memory length: 265099   epsilon: 0.6731020000070966    steps: 296    lr: 4e-05     evaluation reward: 3.03\n",
      "episode: 1408   score: 4.0   memory length: 265392   epsilon: 0.6725218600071092    steps: 293    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1409   score: 3.0   memory length: 265618   epsilon: 0.6720743800071189    steps: 226    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1410   score: 4.0   memory length: 265912   epsilon: 0.6714922600071316    steps: 294    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1411   score: 5.0   memory length: 266237   epsilon: 0.6708487600071456    steps: 325    lr: 4e-05     evaluation reward: 3.03\n",
      "episode: 1412   score: 4.0   memory length: 266512   epsilon: 0.6703042600071574    steps: 275    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1413   score: 2.0   memory length: 266710   epsilon: 0.6699122200071659    steps: 198    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1414   score: 4.0   memory length: 266985   epsilon: 0.6693677200071777    steps: 275    lr: 4e-05     evaluation reward: 3.03\n",
      "episode: 1415   score: 3.0   memory length: 267210   epsilon: 0.6689222200071874    steps: 225    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1416   score: 2.0   memory length: 267427   epsilon: 0.6684925600071967    steps: 217    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1417   score: 3.0   memory length: 267674   epsilon: 0.6680035000072073    steps: 247    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1418   score: 2.0   memory length: 267871   epsilon: 0.6676134400072158    steps: 197    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1419   score: 6.0   memory length: 268260   epsilon: 0.6668432200072325    steps: 389    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1420   score: 2.0   memory length: 268457   epsilon: 0.666453160007241    steps: 197    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1421   score: 6.0   memory length: 268767   epsilon: 0.6658393600072543    steps: 310    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1422   score: 3.0   memory length: 269013   epsilon: 0.6653522800072649    steps: 246    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1423   score: 3.0   memory length: 269238   epsilon: 0.6649067800072745    steps: 225    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1424   score: 3.0   memory length: 269465   epsilon: 0.6644573200072843    steps: 227    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1425   score: 4.0   memory length: 269742   epsilon: 0.6639088600072962    steps: 277    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1426   score: 3.0   memory length: 269985   epsilon: 0.6634277200073067    steps: 243    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1427   score: 2.0   memory length: 270182   epsilon: 0.6630376600073151    steps: 197    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1428   score: 3.0   memory length: 270452   epsilon: 0.6625030600073267    steps: 270    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1429   score: 2.0   memory length: 270649   epsilon: 0.6621130000073352    steps: 197    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1430   score: 2.0   memory length: 270830   epsilon: 0.661754620007343    steps: 181    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1431   score: 3.0   memory length: 271057   epsilon: 0.6613051600073527    steps: 227    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1432   score: 3.0   memory length: 271282   epsilon: 0.6608596600073624    steps: 225    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1433   score: 4.0   memory length: 271539   epsilon: 0.6603508000073735    steps: 257    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1434   score: 1.0   memory length: 271708   epsilon: 0.6600161800073807    steps: 169    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1435   score: 2.0   memory length: 271906   epsilon: 0.6596241400073892    steps: 198    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1436   score: 3.0   memory length: 272136   epsilon: 0.6591687400073991    steps: 230    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1437   score: 2.0   memory length: 272335   epsilon: 0.6587747200074077    steps: 199    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1438   score: 3.0   memory length: 272548   epsilon: 0.6583529800074168    steps: 213    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1439   score: 2.0   memory length: 272766   epsilon: 0.6579213400074262    steps: 218    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1440   score: 5.0   memory length: 273054   epsilon: 0.6573511000074386    steps: 288    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1441   score: 1.0   memory length: 273204   epsilon: 0.657054100007445    steps: 150    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1442   score: 5.0   memory length: 273529   epsilon: 0.656410600007459    steps: 325    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1443   score: 4.0   memory length: 273784   epsilon: 0.65590570000747    steps: 255    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1444   score: 4.0   memory length: 274059   epsilon: 0.6553612000074818    steps: 275    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1445   score: 2.0   memory length: 274238   epsilon: 0.6550067800074895    steps: 179    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1446   score: 3.0   memory length: 274450   epsilon: 0.6545870200074986    steps: 212    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1447   score: 8.0   memory length: 274770   epsilon: 0.6539534200075123    steps: 320    lr: 4e-05     evaluation reward: 3.24\n",
      "episode: 1448   score: 4.0   memory length: 275045   epsilon: 0.6534089200075242    steps: 275    lr: 4e-05     evaluation reward: 3.25\n",
      "episode: 1449   score: 4.0   memory length: 275339   epsilon: 0.6528268000075368    steps: 294    lr: 4e-05     evaluation reward: 3.27\n",
      "episode: 1450   score: 2.0   memory length: 275518   epsilon: 0.6524723800075445    steps: 179    lr: 4e-05     evaluation reward: 3.26\n",
      "episode: 1451   score: 3.0   memory length: 275761   epsilon: 0.6519912400075549    steps: 243    lr: 4e-05     evaluation reward: 3.25\n",
      "episode: 1452   score: 4.0   memory length: 276016   epsilon: 0.6514863400075659    steps: 255    lr: 4e-05     evaluation reward: 3.27\n",
      "episode: 1453   score: 3.0   memory length: 276261   epsilon: 0.6510012400075764    steps: 245    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1454   score: 1.0   memory length: 276412   epsilon: 0.6507022600075829    steps: 151    lr: 4e-05     evaluation reward: 3.25\n",
      "episode: 1455   score: 2.0   memory length: 276593   epsilon: 0.6503438800075907    steps: 181    lr: 4e-05     evaluation reward: 3.24\n",
      "episode: 1456   score: 5.0   memory length: 276922   epsilon: 0.6496924600076048    steps: 329    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1457   score: 2.0   memory length: 277123   epsilon: 0.6492944800076135    steps: 201    lr: 4e-05     evaluation reward: 3.27\n",
      "episode: 1458   score: 3.0   memory length: 277348   epsilon: 0.6488489800076231    steps: 225    lr: 4e-05     evaluation reward: 3.26\n",
      "episode: 1459   score: 4.0   memory length: 277603   epsilon: 0.6483440800076341    steps: 255    lr: 4e-05     evaluation reward: 3.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1460   score: 9.0   memory length: 277968   epsilon: 0.6476213800076498    steps: 365    lr: 4e-05     evaluation reward: 3.33\n",
      "episode: 1461   score: 2.0   memory length: 278150   epsilon: 0.6472610200076576    steps: 182    lr: 4e-05     evaluation reward: 3.32\n",
      "episode: 1462   score: 1.0   memory length: 278302   epsilon: 0.6469600600076642    steps: 152    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1463   score: 3.0   memory length: 278548   epsilon: 0.6464729800076747    steps: 246    lr: 4e-05     evaluation reward: 3.25\n",
      "episode: 1464   score: 2.0   memory length: 278746   epsilon: 0.6460809400076832    steps: 198    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1465   score: 4.0   memory length: 279042   epsilon: 0.645494860007696    steps: 296    lr: 4e-05     evaluation reward: 3.24\n",
      "episode: 1466   score: 6.0   memory length: 279433   epsilon: 0.6447206800077128    steps: 391    lr: 4e-05     evaluation reward: 3.27\n",
      "episode: 1467   score: 3.0   memory length: 279662   epsilon: 0.6442672600077226    steps: 229    lr: 4e-05     evaluation reward: 3.26\n",
      "episode: 1468   score: 4.0   memory length: 279955   epsilon: 0.6436871200077352    steps: 293    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1469   score: 3.0   memory length: 280200   epsilon: 0.6432020200077457    steps: 245    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1470   score: 3.0   memory length: 280446   epsilon: 0.6427149400077563    steps: 246    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1471   score: 3.0   memory length: 280692   epsilon: 0.6422278600077669    steps: 246    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1472   score: 5.0   memory length: 281018   epsilon: 0.6415823800077809    steps: 326    lr: 4e-05     evaluation reward: 3.31\n",
      "episode: 1473   score: 1.0   memory length: 281169   epsilon: 0.6412834000077874    steps: 151    lr: 4e-05     evaluation reward: 3.29\n",
      "episode: 1474   score: 3.0   memory length: 281395   epsilon: 0.6408359200077971    steps: 226    lr: 4e-05     evaluation reward: 3.29\n",
      "episode: 1475   score: 3.0   memory length: 281622   epsilon: 0.6403864600078069    steps: 227    lr: 4e-05     evaluation reward: 3.29\n",
      "episode: 1476   score: 1.0   memory length: 281772   epsilon: 0.6400894600078133    steps: 150    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1477   score: 6.0   memory length: 282168   epsilon: 0.6393053800078303    steps: 396    lr: 4e-05     evaluation reward: 3.31\n",
      "episode: 1478   score: 3.0   memory length: 282431   epsilon: 0.6387846400078416    steps: 263    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1479   score: 4.0   memory length: 282745   epsilon: 0.6381629200078551    steps: 314    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1480   score: 4.0   memory length: 283004   epsilon: 0.6376501000078663    steps: 259    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1481   score: 3.0   memory length: 283249   epsilon: 0.6371650000078768    steps: 245    lr: 4e-05     evaluation reward: 3.33\n",
      "episode: 1482   score: 8.0   memory length: 283680   epsilon: 0.6363116200078953    steps: 431    lr: 4e-05     evaluation reward: 3.39\n",
      "episode: 1483   score: 2.0   memory length: 283878   epsilon: 0.6359195800079038    steps: 198    lr: 4e-05     evaluation reward: 3.37\n",
      "episode: 1484   score: 4.0   memory length: 284153   epsilon: 0.6353750800079156    steps: 275    lr: 4e-05     evaluation reward: 3.36\n",
      "episode: 1485   score: 3.0   memory length: 284399   epsilon: 0.6348880000079262    steps: 246    lr: 4e-05     evaluation reward: 3.37\n",
      "episode: 1486   score: 2.0   memory length: 284579   epsilon: 0.634531600007934    steps: 180    lr: 4e-05     evaluation reward: 3.37\n",
      "episode: 1487   score: 3.0   memory length: 284791   epsilon: 0.6341118400079431    steps: 212    lr: 4e-05     evaluation reward: 3.38\n",
      "episode: 1488   score: 3.0   memory length: 285016   epsilon: 0.6336663400079527    steps: 225    lr: 4e-05     evaluation reward: 3.39\n",
      "episode: 1489   score: 3.0   memory length: 285242   epsilon: 0.6332188600079625    steps: 226    lr: 4e-05     evaluation reward: 3.39\n",
      "episode: 1490   score: 1.0   memory length: 285412   epsilon: 0.6328822600079698    steps: 170    lr: 4e-05     evaluation reward: 3.38\n",
      "episode: 1491   score: 4.0   memory length: 285690   epsilon: 0.6323318200079817    steps: 278    lr: 4e-05     evaluation reward: 3.39\n",
      "episode: 1492   score: 2.0   memory length: 285887   epsilon: 0.6319417600079902    steps: 197    lr: 4e-05     evaluation reward: 3.37\n",
      "episode: 1493   score: 2.0   memory length: 286105   epsilon: 0.6315101200079996    steps: 218    lr: 4e-05     evaluation reward: 3.36\n",
      "episode: 1494   score: 5.0   memory length: 286430   epsilon: 0.6308666200080135    steps: 325    lr: 4e-05     evaluation reward: 3.38\n",
      "episode: 1495   score: 7.0   memory length: 286826   epsilon: 0.6300825400080305    steps: 396    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1496   score: 3.0   memory length: 287073   epsilon: 0.6295934800080412    steps: 247    lr: 4e-05     evaluation reward: 3.39\n",
      "episode: 1497   score: 2.0   memory length: 287293   epsilon: 0.6291578800080506    steps: 220    lr: 4e-05     evaluation reward: 3.35\n",
      "episode: 1498   score: 2.0   memory length: 287493   epsilon: 0.6287618800080592    steps: 200    lr: 4e-05     evaluation reward: 3.36\n",
      "episode: 1499   score: 5.0   memory length: 287802   epsilon: 0.6281500600080725    steps: 309    lr: 4e-05     evaluation reward: 3.37\n",
      "episode: 1500   score: 4.0   memory length: 288101   epsilon: 0.6275580400080853    steps: 299    lr: 4e-05     evaluation reward: 3.35\n",
      "episode: 1501   score: 3.0   memory length: 288330   epsilon: 0.6271046200080952    steps: 229    lr: 4e-05     evaluation reward: 3.34\n",
      "episode: 1502   score: 4.0   memory length: 288626   epsilon: 0.6265185400081079    steps: 296    lr: 4e-05     evaluation reward: 3.32\n",
      "episode: 1503   score: 3.0   memory length: 288871   epsilon: 0.6260334400081184    steps: 245    lr: 4e-05     evaluation reward: 3.33\n",
      "episode: 1504   score: 3.0   memory length: 289102   epsilon: 0.6255760600081284    steps: 231    lr: 4e-05     evaluation reward: 3.32\n",
      "episode: 1505   score: 2.0   memory length: 289300   epsilon: 0.6251840200081369    steps: 198    lr: 4e-05     evaluation reward: 3.31\n",
      "episode: 1506   score: 4.0   memory length: 289596   epsilon: 0.6245979400081496    steps: 296    lr: 4e-05     evaluation reward: 3.31\n",
      "episode: 1507   score: 3.0   memory length: 289822   epsilon: 0.6241504600081593    steps: 226    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1508   score: 2.0   memory length: 290020   epsilon: 0.6237584200081678    steps: 198    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1509   score: 3.0   memory length: 290263   epsilon: 0.6232772800081783    steps: 243    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1510   score: 5.0   memory length: 290586   epsilon: 0.6226377400081922    steps: 323    lr: 4e-05     evaluation reward: 3.29\n",
      "episode: 1511   score: 4.0   memory length: 290883   epsilon: 0.6220496800082049    steps: 297    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1512   score: 6.0   memory length: 291255   epsilon: 0.6213131200082209    steps: 372    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1513   score: 7.0   memory length: 291706   epsilon: 0.6204201400082403    steps: 451    lr: 4e-05     evaluation reward: 3.35\n",
      "episode: 1514   score: 4.0   memory length: 292001   epsilon: 0.619836040008253    steps: 295    lr: 4e-05     evaluation reward: 3.35\n",
      "episode: 1515   score: 2.0   memory length: 292201   epsilon: 0.6194400400082616    steps: 200    lr: 4e-05     evaluation reward: 3.34\n",
      "episode: 1516   score: 4.0   memory length: 292463   epsilon: 0.6189212800082728    steps: 262    lr: 4e-05     evaluation reward: 3.36\n",
      "episode: 1517   score: 2.0   memory length: 292681   epsilon: 0.6184896400082822    steps: 218    lr: 4e-05     evaluation reward: 3.35\n",
      "episode: 1518   score: 3.0   memory length: 292892   epsilon: 0.6180718600082913    steps: 211    lr: 4e-05     evaluation reward: 3.36\n",
      "episode: 1519   score: 4.0   memory length: 293150   epsilon: 0.6175610200083024    steps: 258    lr: 4e-05     evaluation reward: 3.34\n",
      "episode: 1520   score: 1.0   memory length: 293320   epsilon: 0.6172244200083097    steps: 170    lr: 4e-05     evaluation reward: 3.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1521   score: 3.0   memory length: 293551   epsilon: 0.6167670400083196    steps: 231    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1522   score: 2.0   memory length: 293749   epsilon: 0.6163750000083281    steps: 198    lr: 4e-05     evaluation reward: 3.29\n",
      "episode: 1523   score: 3.0   memory length: 293977   epsilon: 0.6159235600083379    steps: 228    lr: 4e-05     evaluation reward: 3.29\n",
      "episode: 1524   score: 4.0   memory length: 294253   epsilon: 0.6153770800083498    steps: 276    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1525   score: 2.0   memory length: 294434   epsilon: 0.6150187000083576    steps: 181    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1526   score: 3.0   memory length: 294702   epsilon: 0.6144880600083691    steps: 268    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1527   score: 4.0   memory length: 294941   epsilon: 0.6140148400083794    steps: 239    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1528   score: 4.0   memory length: 295236   epsilon: 0.613430740008392    steps: 295    lr: 4e-05     evaluation reward: 3.31\n",
      "episode: 1529   score: 6.0   memory length: 295606   epsilon: 0.6126981400084079    steps: 370    lr: 4e-05     evaluation reward: 3.35\n",
      "episode: 1530   score: 2.0   memory length: 295805   epsilon: 0.6123041200084165    steps: 199    lr: 4e-05     evaluation reward: 3.35\n",
      "episode: 1531   score: 2.0   memory length: 296024   epsilon: 0.6118705000084259    steps: 219    lr: 4e-05     evaluation reward: 3.34\n",
      "episode: 1532   score: 1.0   memory length: 296175   epsilon: 0.6115715200084324    steps: 151    lr: 4e-05     evaluation reward: 3.32\n",
      "episode: 1533   score: 2.0   memory length: 296373   epsilon: 0.6111794800084409    steps: 198    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1534   score: 3.0   memory length: 296619   epsilon: 0.6106924000084515    steps: 246    lr: 4e-05     evaluation reward: 3.32\n",
      "episode: 1535   score: 3.0   memory length: 296865   epsilon: 0.6102053200084621    steps: 246    lr: 4e-05     evaluation reward: 3.33\n",
      "episode: 1536   score: 2.0   memory length: 297047   epsilon: 0.6098449600084699    steps: 182    lr: 4e-05     evaluation reward: 3.32\n",
      "episode: 1537   score: 9.0   memory length: 297479   epsilon: 0.6089896000084885    steps: 432    lr: 4e-05     evaluation reward: 3.39\n",
      "episode: 1538   score: 2.0   memory length: 297677   epsilon: 0.608597560008497    steps: 198    lr: 4e-05     evaluation reward: 3.38\n",
      "episode: 1539   score: 3.0   memory length: 297924   epsilon: 0.6081085000085076    steps: 247    lr: 4e-05     evaluation reward: 3.39\n",
      "episode: 1540   score: 6.0   memory length: 298287   epsilon: 0.6073897600085232    steps: 363    lr: 4e-05     evaluation reward: 3.4\n",
      "episode: 1541   score: 9.0   memory length: 298753   epsilon: 0.6064670800085432    steps: 466    lr: 4e-05     evaluation reward: 3.48\n",
      "episode: 1542   score: 2.0   memory length: 298950   epsilon: 0.6060770200085517    steps: 197    lr: 4e-05     evaluation reward: 3.45\n",
      "episode: 1543   score: 4.0   memory length: 299242   epsilon: 0.6054988600085642    steps: 292    lr: 4e-05     evaluation reward: 3.45\n",
      "episode: 1544   score: 3.0   memory length: 299493   epsilon: 0.605001880008575    steps: 251    lr: 4e-05     evaluation reward: 3.44\n",
      "episode: 1545   score: 6.0   memory length: 299847   epsilon: 0.6043009600085902    steps: 354    lr: 4e-05     evaluation reward: 3.48\n",
      "episode: 1546   score: 5.0   memory length: 300174   epsilon: 0.6036535000086043    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 3.5\n",
      "episode: 1547   score: 6.0   memory length: 300527   epsilon: 0.6029545600086195    steps: 353    lr: 1.6000000000000003e-05     evaluation reward: 3.48\n",
      "episode: 1548   score: 4.0   memory length: 300804   epsilon: 0.6024061000086314    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 3.48\n",
      "episode: 1549   score: 4.0   memory length: 301077   epsilon: 0.6018655600086431    steps: 273    lr: 1.6000000000000003e-05     evaluation reward: 3.48\n",
      "episode: 1550   score: 4.0   memory length: 301373   epsilon: 0.6012794800086558    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 3.5\n",
      "episode: 1551   score: 4.0   memory length: 301648   epsilon: 0.6007349800086677    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 3.51\n",
      "episode: 1552   score: 3.0   memory length: 301894   epsilon: 0.6002479000086782    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 3.5\n",
      "episode: 1553   score: 3.0   memory length: 302122   epsilon: 0.599796460008688    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 3.5\n",
      "episode: 1554   score: 5.0   memory length: 302458   epsilon: 0.5991311800087025    steps: 336    lr: 1.6000000000000003e-05     evaluation reward: 3.54\n",
      "episode: 1555   score: 4.0   memory length: 302732   epsilon: 0.5985886600087142    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 3.56\n",
      "episode: 1556   score: 4.0   memory length: 303008   epsilon: 0.5980421800087261    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.55\n",
      "episode: 1557   score: 4.0   memory length: 303287   epsilon: 0.5974897600087381    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 3.57\n",
      "episode: 1558   score: 5.0   memory length: 303614   epsilon: 0.5968423000087522    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 3.59\n",
      "episode: 1559   score: 4.0   memory length: 303890   epsilon: 0.596295820008764    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.59\n",
      "episode: 1560   score: 3.0   memory length: 304116   epsilon: 0.5958483400087737    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 3.53\n",
      "episode: 1561   score: 7.0   memory length: 304545   epsilon: 0.5949989200087922    steps: 429    lr: 1.6000000000000003e-05     evaluation reward: 3.58\n",
      "episode: 1562   score: 3.0   memory length: 304791   epsilon: 0.5945118400088027    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1563   score: 4.0   memory length: 305030   epsilon: 0.594038620008813    steps: 239    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1564   score: 4.0   memory length: 305305   epsilon: 0.5934941200088248    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 3.63\n",
      "episode: 1565   score: 6.0   memory length: 305658   epsilon: 0.59279518000884    steps: 353    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1566   score: 4.0   memory length: 305913   epsilon: 0.592290280008851    steps: 255    lr: 1.6000000000000003e-05     evaluation reward: 3.63\n",
      "episode: 1567   score: 4.0   memory length: 306190   epsilon: 0.5917418200088629    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 3.64\n",
      "episode: 1568   score: 4.0   memory length: 306486   epsilon: 0.5911557400088756    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 3.64\n",
      "episode: 1569   score: 4.0   memory length: 306777   epsilon: 0.5905795600088881    steps: 291    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1570   score: 4.0   memory length: 307053   epsilon: 0.5900330800089    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.66\n",
      "episode: 1571   score: 3.0   memory length: 307300   epsilon: 0.5895440200089106    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 3.66\n",
      "episode: 1572   score: 6.0   memory length: 307641   epsilon: 0.5888688400089253    steps: 341    lr: 1.6000000000000003e-05     evaluation reward: 3.67\n",
      "episode: 1573   score: 6.0   memory length: 308032   epsilon: 0.5880946600089421    steps: 391    lr: 1.6000000000000003e-05     evaluation reward: 3.72\n",
      "episode: 1574   score: 2.0   memory length: 308214   epsilon: 0.5877343000089499    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 3.71\n",
      "episode: 1575   score: 2.0   memory length: 308411   epsilon: 0.5873442400089584    steps: 197    lr: 1.6000000000000003e-05     evaluation reward: 3.7\n",
      "episode: 1576   score: 8.0   memory length: 308871   epsilon: 0.5864334400089781    steps: 460    lr: 1.6000000000000003e-05     evaluation reward: 3.77\n",
      "episode: 1577   score: 6.0   memory length: 309207   epsilon: 0.5857681600089926    steps: 336    lr: 1.6000000000000003e-05     evaluation reward: 3.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1578   score: 4.0   memory length: 309461   epsilon: 0.5852652400090035    steps: 254    lr: 1.6000000000000003e-05     evaluation reward: 3.78\n",
      "episode: 1579   score: 4.0   memory length: 309743   epsilon: 0.5847068800090156    steps: 282    lr: 1.6000000000000003e-05     evaluation reward: 3.78\n",
      "episode: 1580   score: 7.0   memory length: 310164   epsilon: 0.5838733000090337    steps: 421    lr: 1.6000000000000003e-05     evaluation reward: 3.81\n",
      "episode: 1581   score: 5.0   memory length: 310454   epsilon: 0.5832991000090462    steps: 290    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1582   score: 3.0   memory length: 310681   epsilon: 0.5828496400090559    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 3.78\n",
      "episode: 1583   score: 6.0   memory length: 311071   epsilon: 0.5820774400090727    steps: 390    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1584   score: 3.0   memory length: 311302   epsilon: 0.5816200600090826    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 3.81\n",
      "episode: 1585   score: 2.0   memory length: 311484   epsilon: 0.5812597000090904    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 3.8\n",
      "episode: 1586   score: 6.0   memory length: 311856   epsilon: 0.5805231400091064    steps: 372    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1587   score: 8.0   memory length: 312294   epsilon: 0.5796559000091253    steps: 438    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1588   score: 6.0   memory length: 312665   epsilon: 0.5789213200091412    steps: 371    lr: 1.6000000000000003e-05     evaluation reward: 3.92\n",
      "episode: 1589   score: 3.0   memory length: 312911   epsilon: 0.5784342400091518    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 3.92\n",
      "episode: 1590   score: 5.0   memory length: 313203   epsilon: 0.5778560800091643    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 3.96\n",
      "episode: 1591   score: 5.0   memory length: 313571   epsilon: 0.5771274400091801    steps: 368    lr: 1.6000000000000003e-05     evaluation reward: 3.97\n",
      "episode: 1592   score: 4.0   memory length: 313845   epsilon: 0.5765849200091919    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 3.99\n",
      "episode: 1593   score: 3.0   memory length: 314089   epsilon: 0.5761018000092024    steps: 244    lr: 1.6000000000000003e-05     evaluation reward: 4.0\n",
      "episode: 1594   score: 4.0   memory length: 314383   epsilon: 0.575519680009215    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 3.99\n",
      "episode: 1595   score: 4.0   memory length: 314659   epsilon: 0.5749732000092269    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.96\n",
      "episode: 1596   score: 4.0   memory length: 314937   epsilon: 0.5744227600092389    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 3.97\n",
      "episode: 1597   score: 6.0   memory length: 315280   epsilon: 0.5737436200092536    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 4.01\n",
      "episode: 1598   score: 4.0   memory length: 315557   epsilon: 0.5731951600092655    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 4.03\n",
      "episode: 1599   score: 4.0   memory length: 315834   epsilon: 0.5726467000092774    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 4.02\n",
      "episode: 1600   score: 4.0   memory length: 316129   epsilon: 0.5720626000092901    steps: 295    lr: 1.6000000000000003e-05     evaluation reward: 4.02\n",
      "episode: 1601   score: 5.0   memory length: 316472   epsilon: 0.5713834600093048    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 4.04\n",
      "episode: 1602   score: 3.0   memory length: 316685   epsilon: 0.570961720009314    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 4.03\n",
      "episode: 1603   score: 3.0   memory length: 316911   epsilon: 0.5705142400093237    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 4.03\n",
      "episode: 1604   score: 4.0   memory length: 317203   epsilon: 0.5699360800093363    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 4.04\n",
      "episode: 1605   score: 4.0   memory length: 317481   epsilon: 0.5693856400093482    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 4.06\n",
      "episode: 1606   score: 3.0   memory length: 317730   epsilon: 0.5688926200093589    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 4.05\n",
      "episode: 1607   score: 7.0   memory length: 318151   epsilon: 0.568059040009377    steps: 421    lr: 1.6000000000000003e-05     evaluation reward: 4.09\n",
      "episode: 1608   score: 3.0   memory length: 318404   epsilon: 0.5675581000093879    steps: 253    lr: 1.6000000000000003e-05     evaluation reward: 4.1\n",
      "episode: 1609   score: 5.0   memory length: 318708   epsilon: 0.566956180009401    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 4.12\n",
      "episode: 1610   score: 4.0   memory length: 319003   epsilon: 0.5663720800094136    steps: 295    lr: 1.6000000000000003e-05     evaluation reward: 4.11\n",
      "episode: 1611   score: 4.0   memory length: 319277   epsilon: 0.5658295600094254    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 4.11\n",
      "episode: 1612   score: 2.0   memory length: 319476   epsilon: 0.565435540009434    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 4.07\n",
      "episode: 1613   score: 3.0   memory length: 319702   epsilon: 0.5649880600094437    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 4.03\n",
      "episode: 1614   score: 2.0   memory length: 319902   epsilon: 0.5645920600094523    steps: 200    lr: 1.6000000000000003e-05     evaluation reward: 4.01\n",
      "episode: 1615   score: 9.0   memory length: 320375   epsilon: 0.5636555200094726    steps: 473    lr: 1.6000000000000003e-05     evaluation reward: 4.08\n",
      "episode: 1616   score: 3.0   memory length: 320602   epsilon: 0.5632060600094824    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 4.07\n",
      "episode: 1617   score: 4.0   memory length: 320857   epsilon: 0.5627011600094933    steps: 255    lr: 1.6000000000000003e-05     evaluation reward: 4.09\n",
      "episode: 1618   score: 9.0   memory length: 321344   epsilon: 0.5617369000095143    steps: 487    lr: 1.6000000000000003e-05     evaluation reward: 4.15\n",
      "episode: 1619   score: 5.0   memory length: 321650   epsilon: 0.5611310200095274    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 4.16\n",
      "episode: 1620   score: 4.0   memory length: 321892   epsilon: 0.5606518600095378    steps: 242    lr: 1.6000000000000003e-05     evaluation reward: 4.19\n",
      "episode: 1621   score: 2.0   memory length: 322090   epsilon: 0.5602598200095463    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 4.18\n",
      "episode: 1622   score: 4.0   memory length: 322368   epsilon: 0.5597093800095583    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 4.2\n",
      "episode: 1623   score: 4.0   memory length: 322664   epsilon: 0.559123300009571    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1624   score: 4.0   memory length: 322939   epsilon: 0.5585788000095828    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1625   score: 2.0   memory length: 323121   epsilon: 0.5582184400095906    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1626   score: 4.0   memory length: 323361   epsilon: 0.557743240009601    steps: 240    lr: 1.6000000000000003e-05     evaluation reward: 4.22\n",
      "episode: 1627   score: 7.0   memory length: 323768   epsilon: 0.5569373800096185    steps: 407    lr: 1.6000000000000003e-05     evaluation reward: 4.25\n",
      "episode: 1628   score: 3.0   memory length: 323994   epsilon: 0.5564899000096282    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 4.24\n",
      "episode: 1629   score: 3.0   memory length: 324243   epsilon: 0.5559968800096389    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1630   score: 3.0   memory length: 324468   epsilon: 0.5555513800096485    steps: 225    lr: 1.6000000000000003e-05     evaluation reward: 4.22\n",
      "episode: 1631   score: 3.0   memory length: 324716   epsilon: 0.5550603400096592    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 4.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1632   score: 3.0   memory length: 324941   epsilon: 0.5546148400096689    steps: 225    lr: 1.6000000000000003e-05     evaluation reward: 4.25\n",
      "episode: 1633   score: 4.0   memory length: 325220   epsilon: 0.5540624200096809    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 4.27\n",
      "episode: 1634   score: 3.0   memory length: 325449   epsilon: 0.5536090000096907    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.27\n",
      "episode: 1635   score: 4.0   memory length: 325724   epsilon: 0.5530645000097025    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 4.28\n",
      "episode: 1636   score: 8.0   memory length: 326165   epsilon: 0.5521913200097215    steps: 441    lr: 1.6000000000000003e-05     evaluation reward: 4.34\n",
      "episode: 1637   score: 3.0   memory length: 326390   epsilon: 0.5517458200097312    steps: 225    lr: 1.6000000000000003e-05     evaluation reward: 4.28\n",
      "episode: 1638   score: 3.0   memory length: 326599   epsilon: 0.5513320000097401    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 4.29\n",
      "episode: 1639   score: 3.0   memory length: 326845   epsilon: 0.5508449200097507    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 4.29\n",
      "episode: 1640   score: 5.0   memory length: 327170   epsilon: 0.5502014200097647    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 4.28\n",
      "episode: 1641   score: 6.0   memory length: 327544   epsilon: 0.5494609000097808    steps: 374    lr: 1.6000000000000003e-05     evaluation reward: 4.25\n",
      "episode: 1642   score: 5.0   memory length: 327868   epsilon: 0.5488193800097947    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 4.28\n",
      "episode: 1643   score: 4.0   memory length: 328144   epsilon: 0.5482729000098066    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 4.28\n",
      "episode: 1644   score: 4.0   memory length: 328422   epsilon: 0.5477224600098185    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 4.29\n",
      "episode: 1645   score: 4.0   memory length: 328697   epsilon: 0.5471779600098303    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 4.27\n",
      "episode: 1646   score: 3.0   memory length: 328910   epsilon: 0.5467562200098395    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 4.25\n",
      "episode: 1647   score: 4.0   memory length: 329167   epsilon: 0.5462473600098505    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 4.23\n",
      "episode: 1648   score: 3.0   memory length: 329435   epsilon: 0.545716720009862    steps: 268    lr: 1.6000000000000003e-05     evaluation reward: 4.22\n",
      "episode: 1649   score: 4.0   memory length: 329709   epsilon: 0.5451742000098738    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 4.22\n",
      "episode: 1650   score: 2.0   memory length: 329907   epsilon: 0.5447821600098823    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 4.2\n",
      "episode: 1651   score: 4.0   memory length: 330185   epsilon: 0.5442317200098943    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 4.2\n",
      "episode: 1652   score: 4.0   memory length: 330442   epsilon: 0.5437228600099053    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1653   score: 6.0   memory length: 330800   epsilon: 0.5430140200099207    steps: 358    lr: 1.6000000000000003e-05     evaluation reward: 4.24\n",
      "episode: 1654   score: 4.0   memory length: 331079   epsilon: 0.5424616000099327    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 4.23\n",
      "episode: 1655   score: 8.0   memory length: 331499   epsilon: 0.5416300000099508    steps: 420    lr: 1.6000000000000003e-05     evaluation reward: 4.27\n",
      "episode: 1656   score: 6.0   memory length: 331816   epsilon: 0.5410023400099644    steps: 317    lr: 1.6000000000000003e-05     evaluation reward: 4.29\n",
      "episode: 1657   score: 5.0   memory length: 332104   epsilon: 0.5404321000099768    steps: 288    lr: 1.6000000000000003e-05     evaluation reward: 4.3\n",
      "episode: 1658   score: 4.0   memory length: 332379   epsilon: 0.5398876000099886    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 4.29\n",
      "episode: 1659   score: 3.0   memory length: 332608   epsilon: 0.5394341800099984    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.28\n",
      "episode: 1660   score: 6.0   memory length: 332938   epsilon: 0.5387807800100126    steps: 330    lr: 1.6000000000000003e-05     evaluation reward: 4.31\n",
      "episode: 1661   score: 5.0   memory length: 333262   epsilon: 0.5381392600100265    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 4.29\n",
      "episode: 1662   score: 5.0   memory length: 333583   epsilon: 0.5375036800100403    steps: 321    lr: 1.6000000000000003e-05     evaluation reward: 4.31\n",
      "episode: 1663   score: 7.0   memory length: 334027   epsilon: 0.5366245600100594    steps: 444    lr: 1.6000000000000003e-05     evaluation reward: 4.34\n",
      "episode: 1664   score: 4.0   memory length: 334302   epsilon: 0.5360800600100712    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 4.34\n",
      "episode: 1665   score: 3.0   memory length: 334512   epsilon: 0.5356642600100803    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 4.31\n",
      "episode: 1666   score: 4.0   memory length: 334806   epsilon: 0.5350821400100929    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 4.31\n",
      "episode: 1667   score: 5.0   memory length: 335105   epsilon: 0.5344901200101058    steps: 299    lr: 1.6000000000000003e-05     evaluation reward: 4.32\n",
      "episode: 1668   score: 3.0   memory length: 335331   epsilon: 0.5340426400101155    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 4.31\n",
      "episode: 1669   score: 2.0   memory length: 335513   epsilon: 0.5336822800101233    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 4.29\n",
      "episode: 1670   score: 4.0   memory length: 335772   epsilon: 0.5331694600101344    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 4.29\n",
      "episode: 1671   score: 4.0   memory length: 336032   epsilon: 0.5326546600101456    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 4.3\n",
      "episode: 1672   score: 3.0   memory length: 336301   epsilon: 0.5321220400101572    steps: 269    lr: 1.6000000000000003e-05     evaluation reward: 4.27\n",
      "episode: 1673   score: 5.0   memory length: 336608   epsilon: 0.5315141800101704    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 4.26\n",
      "episode: 1674   score: 4.0   memory length: 336885   epsilon: 0.5309657200101823    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 4.28\n",
      "episode: 1675   score: 3.0   memory length: 337151   epsilon: 0.5304390400101937    steps: 266    lr: 1.6000000000000003e-05     evaluation reward: 4.29\n",
      "episode: 1676   score: 4.0   memory length: 337431   epsilon: 0.5298846400102057    steps: 280    lr: 1.6000000000000003e-05     evaluation reward: 4.25\n",
      "episode: 1677   score: 9.0   memory length: 337925   epsilon: 0.528906520010227    steps: 494    lr: 1.6000000000000003e-05     evaluation reward: 4.28\n",
      "episode: 1678   score: 4.0   memory length: 338188   epsilon: 0.5283857800102383    steps: 263    lr: 1.6000000000000003e-05     evaluation reward: 4.28\n",
      "episode: 1679   score: 3.0   memory length: 338437   epsilon: 0.527892760010249    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 4.27\n",
      "episode: 1680   score: 7.0   memory length: 338878   epsilon: 0.5270195800102679    steps: 441    lr: 1.6000000000000003e-05     evaluation reward: 4.27\n",
      "episode: 1681   score: 3.0   memory length: 339107   epsilon: 0.5265661600102778    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.25\n",
      "episode: 1682   score: 5.0   memory length: 339431   epsilon: 0.5259246400102917    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 4.27\n",
      "episode: 1683   score: 3.0   memory length: 339658   epsilon: 0.5254751800103015    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 4.24\n",
      "episode: 1684   score: 5.0   memory length: 339986   epsilon: 0.5248257400103156    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 4.26\n",
      "episode: 1685   score: 2.0   memory length: 340168   epsilon: 0.5244653800103234    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 4.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1686   score: 7.0   memory length: 340558   epsilon: 0.5236931800103402    steps: 390    lr: 1.6000000000000003e-05     evaluation reward: 4.27\n",
      "episode: 1687   score: 5.0   memory length: 340867   epsilon: 0.5230813600103534    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 4.24\n",
      "episode: 1688   score: 7.0   memory length: 341291   epsilon: 0.5222418400103717    steps: 424    lr: 1.6000000000000003e-05     evaluation reward: 4.25\n",
      "episode: 1689   score: 4.0   memory length: 341531   epsilon: 0.521766640010382    steps: 240    lr: 1.6000000000000003e-05     evaluation reward: 4.26\n",
      "episode: 1690   score: 3.0   memory length: 341781   epsilon: 0.5212716400103927    steps: 250    lr: 1.6000000000000003e-05     evaluation reward: 4.24\n",
      "episode: 1691   score: 4.0   memory length: 342042   epsilon: 0.5207548600104039    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 4.23\n",
      "episode: 1692   score: 7.0   memory length: 342463   epsilon: 0.519921280010422    steps: 421    lr: 1.6000000000000003e-05     evaluation reward: 4.26\n",
      "episode: 1693   score: 3.0   memory length: 342712   epsilon: 0.5194282600104327    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 4.26\n",
      "episode: 1694   score: 15.0   memory length: 343153   epsilon: 0.5185550800104517    steps: 441    lr: 1.6000000000000003e-05     evaluation reward: 4.37\n",
      "episode: 1695   score: 6.0   memory length: 343505   epsilon: 0.5178581200104668    steps: 352    lr: 1.6000000000000003e-05     evaluation reward: 4.39\n",
      "episode: 1696   score: 7.0   memory length: 343928   epsilon: 0.517020580010485    steps: 423    lr: 1.6000000000000003e-05     evaluation reward: 4.42\n",
      "episode: 1697   score: 6.0   memory length: 344302   epsilon: 0.5162800600105011    steps: 374    lr: 1.6000000000000003e-05     evaluation reward: 4.42\n",
      "episode: 1698   score: 4.0   memory length: 344563   epsilon: 0.5157632800105123    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 4.42\n",
      "episode: 1699   score: 8.0   memory length: 344997   epsilon: 0.514903960010531    steps: 434    lr: 1.6000000000000003e-05     evaluation reward: 4.46\n",
      "episode: 1700   score: 5.0   memory length: 345321   epsilon: 0.5142624400105449    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 4.47\n",
      "episode: 1701   score: 5.0   memory length: 345626   epsilon: 0.513658540010558    steps: 305    lr: 1.6000000000000003e-05     evaluation reward: 4.47\n",
      "episode: 1702   score: 2.0   memory length: 345807   epsilon: 0.5133001600105658    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 4.46\n",
      "episode: 1703   score: 5.0   memory length: 346136   epsilon: 0.5126487400105799    steps: 329    lr: 1.6000000000000003e-05     evaluation reward: 4.48\n",
      "episode: 1704   score: 9.0   memory length: 346603   epsilon: 0.5117240800106    steps: 467    lr: 1.6000000000000003e-05     evaluation reward: 4.53\n",
      "episode: 1705   score: 4.0   memory length: 346878   epsilon: 0.5111795800106118    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 4.53\n",
      "episode: 1706   score: 1.0   memory length: 347029   epsilon: 0.5108806000106183    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 4.51\n",
      "episode: 1707   score: 6.0   memory length: 347399   epsilon: 0.5101480000106342    steps: 370    lr: 1.6000000000000003e-05     evaluation reward: 4.5\n",
      "episode: 1708   score: 7.0   memory length: 347744   epsilon: 0.509464900010649    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 4.54\n",
      "episode: 1709   score: 6.0   memory length: 348097   epsilon: 0.5087659600106642    steps: 353    lr: 1.6000000000000003e-05     evaluation reward: 4.55\n",
      "episode: 1710   score: 8.0   memory length: 348564   epsilon: 0.5078413000106843    steps: 467    lr: 1.6000000000000003e-05     evaluation reward: 4.59\n",
      "episode: 1711   score: 5.0   memory length: 348893   epsilon: 0.5071898800106984    steps: 329    lr: 1.6000000000000003e-05     evaluation reward: 4.6\n",
      "episode: 1712   score: 6.0   memory length: 349246   epsilon: 0.5064909400107136    steps: 353    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
      "episode: 1713   score: 6.0   memory length: 349617   epsilon: 0.5057563600107295    steps: 371    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
      "episode: 1714   score: 4.0   memory length: 349897   epsilon: 0.5052019600107416    steps: 280    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
      "episode: 1715   score: 4.0   memory length: 350137   epsilon: 0.5047267600107519    steps: 240    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
      "episode: 1716   score: 6.0   memory length: 350481   epsilon: 0.5040456400107667    steps: 344    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
      "episode: 1717   score: 4.0   memory length: 350740   epsilon: 0.5035328200107778    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
      "episode: 1718   score: 5.0   memory length: 351066   epsilon: 0.5028873400107918    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
      "episode: 1719   score: 4.0   memory length: 351340   epsilon: 0.5023448200108036    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 4.62\n",
      "episode: 1720   score: 4.0   memory length: 351620   epsilon: 0.5017904200108156    steps: 280    lr: 1.6000000000000003e-05     evaluation reward: 4.62\n",
      "episode: 1721   score: 4.0   memory length: 351879   epsilon: 0.5012776000108268    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
      "episode: 1722   score: 2.0   memory length: 352076   epsilon: 0.5008875400108352    steps: 197    lr: 1.6000000000000003e-05     evaluation reward: 4.62\n",
      "episode: 1723   score: 4.0   memory length: 352333   epsilon: 0.5003786800108463    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 4.62\n",
      "episode: 1724   score: 5.0   memory length: 352643   epsilon: 0.499764880010853    steps: 310    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
      "episode: 1725   score: 4.0   memory length: 352901   epsilon: 0.4992540400108498    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
      "episode: 1726   score: 5.0   memory length: 353206   epsilon: 0.49865014001084595    steps: 305    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
      "episode: 1727   score: 5.0   memory length: 353515   epsilon: 0.4980383200108421    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
      "episode: 1728   score: 4.0   memory length: 353790   epsilon: 0.49749382001083864    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
      "episode: 1729   score: 7.0   memory length: 354181   epsilon: 0.49671964001083374    steps: 391    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
      "episode: 1730   score: 3.0   memory length: 354410   epsilon: 0.49626622001083087    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
      "episode: 1731   score: 2.0   memory length: 354608   epsilon: 0.4958741800108284    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 4.68\n",
      "episode: 1732   score: 3.0   memory length: 354838   epsilon: 0.4954187800108255    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 4.68\n",
      "episode: 1733   score: 2.0   memory length: 355056   epsilon: 0.4949871400108228    steps: 218    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
      "episode: 1734   score: 4.0   memory length: 355348   epsilon: 0.4944089800108191    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
      "episode: 1735   score: 4.0   memory length: 355605   epsilon: 0.4939001200108159    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
      "episode: 1736   score: 4.0   memory length: 355863   epsilon: 0.49338928001081267    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
      "episode: 1737   score: 5.0   memory length: 356186   epsilon: 0.4927497400108086    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
      "episode: 1738   score: 11.0   memory length: 356753   epsilon: 0.4916270800108015    steps: 567    lr: 1.6000000000000003e-05     evaluation reward: 4.73\n",
      "episode: 1739   score: 4.0   memory length: 357055   epsilon: 0.49102912001079774    steps: 302    lr: 1.6000000000000003e-05     evaluation reward: 4.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1740   score: 6.0   memory length: 357420   epsilon: 0.49030642001079316    steps: 365    lr: 1.6000000000000003e-05     evaluation reward: 4.75\n",
      "episode: 1741   score: 4.0   memory length: 357700   epsilon: 0.48975202001078966    steps: 280    lr: 1.6000000000000003e-05     evaluation reward: 4.73\n",
      "episode: 1742   score: 4.0   memory length: 357975   epsilon: 0.4892075200107862    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 4.72\n",
      "episode: 1743   score: 4.0   memory length: 358253   epsilon: 0.48865708001078273    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 4.72\n",
      "episode: 1744   score: 8.0   memory length: 358686   epsilon: 0.4877997400107773    steps: 433    lr: 1.6000000000000003e-05     evaluation reward: 4.76\n",
      "episode: 1745   score: 3.0   memory length: 358932   epsilon: 0.4873126600107742    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 4.75\n",
      "episode: 1746   score: 9.0   memory length: 359421   epsilon: 0.4863444400107681    steps: 489    lr: 1.6000000000000003e-05     evaluation reward: 4.81\n",
      "episode: 1747   score: 7.0   memory length: 359811   epsilon: 0.4855722400107632    steps: 390    lr: 1.6000000000000003e-05     evaluation reward: 4.84\n",
      "episode: 1748   score: 7.0   memory length: 360230   epsilon: 0.48474262001075796    steps: 419    lr: 1.6000000000000003e-05     evaluation reward: 4.88\n",
      "episode: 1749   score: 6.0   memory length: 360573   epsilon: 0.48406348001075367    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 4.9\n",
      "episode: 1750   score: 2.0   memory length: 360771   epsilon: 0.4836714400107512    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 4.9\n",
      "episode: 1751   score: 5.0   memory length: 361079   epsilon: 0.4830616000107473    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 4.91\n",
      "episode: 1752   score: 6.0   memory length: 361434   epsilon: 0.4823587000107429    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 4.93\n",
      "episode: 1753   score: 14.0   memory length: 362005   epsilon: 0.4812281200107357    steps: 571    lr: 1.6000000000000003e-05     evaluation reward: 5.01\n",
      "episode: 1754   score: 5.0   memory length: 362317   epsilon: 0.4806103600107318    steps: 312    lr: 1.6000000000000003e-05     evaluation reward: 5.02\n",
      "episode: 1755   score: 7.0   memory length: 362741   epsilon: 0.4797708400107265    steps: 424    lr: 1.6000000000000003e-05     evaluation reward: 5.01\n",
      "episode: 1756   score: 7.0   memory length: 363085   epsilon: 0.4790897200107222    steps: 344    lr: 1.6000000000000003e-05     evaluation reward: 5.02\n",
      "episode: 1757   score: 4.0   memory length: 363326   epsilon: 0.4786125400107192    steps: 241    lr: 1.6000000000000003e-05     evaluation reward: 5.01\n",
      "episode: 1758   score: 7.0   memory length: 363688   epsilon: 0.47789578001071464    steps: 362    lr: 1.6000000000000003e-05     evaluation reward: 5.04\n",
      "episode: 1759   score: 5.0   memory length: 364031   epsilon: 0.47721664001071035    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 5.06\n",
      "episode: 1760   score: 2.0   memory length: 364229   epsilon: 0.47682460001070787    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 5.02\n",
      "episode: 1761   score: 4.0   memory length: 364508   epsilon: 0.47627218001070437    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 5.01\n",
      "episode: 1762   score: 7.0   memory length: 364911   epsilon: 0.4754742400106993    steps: 403    lr: 1.6000000000000003e-05     evaluation reward: 5.03\n",
      "episode: 1763   score: 4.0   memory length: 365190   epsilon: 0.4749218200106958    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 5.0\n",
      "episode: 1764   score: 6.0   memory length: 365563   epsilon: 0.47418328001069115    steps: 373    lr: 1.6000000000000003e-05     evaluation reward: 5.02\n",
      "episode: 1765   score: 6.0   memory length: 365924   epsilon: 0.47346850001068663    steps: 361    lr: 1.6000000000000003e-05     evaluation reward: 5.05\n",
      "episode: 1766   score: 6.0   memory length: 366241   epsilon: 0.47284084001068266    steps: 317    lr: 1.6000000000000003e-05     evaluation reward: 5.07\n",
      "episode: 1767   score: 6.0   memory length: 366609   epsilon: 0.47211220001067805    steps: 368    lr: 1.6000000000000003e-05     evaluation reward: 5.08\n",
      "episode: 1768   score: 3.0   memory length: 366835   epsilon: 0.4716647200106752    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 5.08\n",
      "episode: 1769   score: 5.0   memory length: 367139   epsilon: 0.4710628000106714    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 5.11\n",
      "episode: 1770   score: 10.0   memory length: 367565   epsilon: 0.4702193200106661    steps: 426    lr: 1.6000000000000003e-05     evaluation reward: 5.17\n",
      "episode: 1771   score: 6.0   memory length: 367926   epsilon: 0.46950454001066155    steps: 361    lr: 1.6000000000000003e-05     evaluation reward: 5.19\n",
      "episode: 1772   score: 4.0   memory length: 368183   epsilon: 0.46899568001065833    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 5.2\n",
      "episode: 1773   score: 4.0   memory length: 368458   epsilon: 0.4684511800106549    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 5.19\n",
      "episode: 1774   score: 4.0   memory length: 368759   epsilon: 0.4678552000106511    steps: 301    lr: 1.6000000000000003e-05     evaluation reward: 5.19\n",
      "episode: 1775   score: 9.0   memory length: 369247   epsilon: 0.466888960010645    steps: 488    lr: 1.6000000000000003e-05     evaluation reward: 5.25\n",
      "episode: 1776   score: 5.0   memory length: 369550   epsilon: 0.4662890200106412    steps: 303    lr: 1.6000000000000003e-05     evaluation reward: 5.26\n",
      "episode: 1777   score: 8.0   memory length: 369967   epsilon: 0.465463360010636    steps: 417    lr: 1.6000000000000003e-05     evaluation reward: 5.25\n",
      "episode: 1778   score: 4.0   memory length: 370241   epsilon: 0.46492084001063255    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 5.25\n",
      "episode: 1779   score: 11.0   memory length: 370815   epsilon: 0.46378432001062536    steps: 574    lr: 1.6000000000000003e-05     evaluation reward: 5.33\n",
      "episode: 1780   score: 5.0   memory length: 371123   epsilon: 0.4631744800106215    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 5.31\n",
      "episode: 1781   score: 5.0   memory length: 371470   epsilon: 0.46248742001061716    steps: 347    lr: 1.6000000000000003e-05     evaluation reward: 5.33\n",
      "episode: 1782   score: 4.0   memory length: 371732   epsilon: 0.4619686600106139    steps: 262    lr: 1.6000000000000003e-05     evaluation reward: 5.32\n",
      "episode: 1783   score: 3.0   memory length: 371961   epsilon: 0.461515240010611    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 5.32\n",
      "episode: 1784   score: 11.0   memory length: 372501   epsilon: 0.46044604001060424    steps: 540    lr: 1.6000000000000003e-05     evaluation reward: 5.38\n",
      "episode: 1785   score: 6.0   memory length: 372835   epsilon: 0.45978472001060006    steps: 334    lr: 1.6000000000000003e-05     evaluation reward: 5.42\n",
      "episode: 1786   score: 2.0   memory length: 373032   epsilon: 0.4593946600105976    steps: 197    lr: 1.6000000000000003e-05     evaluation reward: 5.37\n",
      "episode: 1787   score: 7.0   memory length: 373411   epsilon: 0.45864424001059284    steps: 379    lr: 1.6000000000000003e-05     evaluation reward: 5.39\n",
      "episode: 1788   score: 9.0   memory length: 373862   epsilon: 0.4577512600105872    steps: 451    lr: 1.6000000000000003e-05     evaluation reward: 5.41\n",
      "episode: 1789   score: 6.0   memory length: 374203   epsilon: 0.4570760800105829    steps: 341    lr: 1.6000000000000003e-05     evaluation reward: 5.43\n",
      "episode: 1790   score: 4.0   memory length: 374478   epsilon: 0.4565315800105795    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 5.44\n",
      "episode: 1791   score: 4.0   memory length: 374776   epsilon: 0.45594154001057574    steps: 298    lr: 1.6000000000000003e-05     evaluation reward: 5.44\n",
      "episode: 1792   score: 5.0   memory length: 375100   epsilon: 0.4553000200105717    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 5.42\n",
      "episode: 1793   score: 4.0   memory length: 375379   epsilon: 0.4547476000105682    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 5.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1794   score: 11.0   memory length: 375818   epsilon: 0.4538783800105627    steps: 439    lr: 1.6000000000000003e-05     evaluation reward: 5.39\n",
      "episode: 1795   score: 7.0   memory length: 376242   epsilon: 0.4530388600105574    steps: 424    lr: 1.6000000000000003e-05     evaluation reward: 5.4\n",
      "episode: 1796   score: 6.0   memory length: 376597   epsilon: 0.45233596001055293    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 5.39\n",
      "episode: 1797   score: 3.0   memory length: 376806   epsilon: 0.4519221400105503    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 5.36\n",
      "episode: 1798   score: 8.0   memory length: 377229   epsilon: 0.451084600010545    steps: 423    lr: 1.6000000000000003e-05     evaluation reward: 5.4\n",
      "episode: 1799   score: 5.0   memory length: 377538   epsilon: 0.45047278001054114    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 5.37\n",
      "episode: 1800   score: 6.0   memory length: 377909   epsilon: 0.4497382000105365    steps: 371    lr: 1.6000000000000003e-05     evaluation reward: 5.38\n",
      "episode: 1801   score: 9.0   memory length: 378450   epsilon: 0.4486670200105297    steps: 541    lr: 1.6000000000000003e-05     evaluation reward: 5.42\n",
      "episode: 1802   score: 3.0   memory length: 378696   epsilon: 0.44817994001052663    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 5.43\n",
      "episode: 1803   score: 5.0   memory length: 378984   epsilon: 0.447609700010523    steps: 288    lr: 1.6000000000000003e-05     evaluation reward: 5.43\n",
      "episode: 1804   score: 7.0   memory length: 379230   epsilon: 0.44712262001051994    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 5.41\n",
      "episode: 1805   score: 7.0   memory length: 379647   epsilon: 0.4462969600105147    steps: 417    lr: 1.6000000000000003e-05     evaluation reward: 5.44\n",
      "episode: 1806   score: 8.0   memory length: 380097   epsilon: 0.4454059600105091    steps: 450    lr: 1.6000000000000003e-05     evaluation reward: 5.51\n",
      "episode: 1807   score: 4.0   memory length: 380389   epsilon: 0.4448278000105054    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 5.49\n",
      "episode: 1808   score: 6.0   memory length: 380744   epsilon: 0.444124900010501    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1809   score: 6.0   memory length: 381084   epsilon: 0.4434517000104967    steps: 340    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1810   score: 6.0   memory length: 381437   epsilon: 0.4427527600104923    steps: 353    lr: 1.6000000000000003e-05     evaluation reward: 5.46\n",
      "episode: 1811   score: 3.0   memory length: 381667   epsilon: 0.4422973600104894    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 5.44\n",
      "episode: 1812   score: 2.0   memory length: 381864   epsilon: 0.44190730001048695    steps: 197    lr: 1.6000000000000003e-05     evaluation reward: 5.4\n",
      "episode: 1813   score: 4.0   memory length: 382140   epsilon: 0.4413608200104835    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 5.38\n",
      "episode: 1814   score: 4.0   memory length: 382415   epsilon: 0.44081632001048004    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 5.38\n",
      "episode: 1815   score: 3.0   memory length: 382645   epsilon: 0.44036092001047716    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 5.37\n",
      "episode: 1816   score: 7.0   memory length: 383067   epsilon: 0.4395253600104719    steps: 422    lr: 1.6000000000000003e-05     evaluation reward: 5.38\n",
      "episode: 1817   score: 8.0   memory length: 383488   epsilon: 0.4386917800104666    steps: 421    lr: 1.6000000000000003e-05     evaluation reward: 5.42\n",
      "episode: 1818   score: 5.0   memory length: 383813   epsilon: 0.43804828001046253    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 5.42\n",
      "episode: 1819   score: 7.0   memory length: 384241   epsilon: 0.43720084001045717    steps: 428    lr: 1.6000000000000003e-05     evaluation reward: 5.45\n",
      "episode: 1820   score: 9.0   memory length: 384547   epsilon: 0.43659496001045334    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 5.5\n",
      "episode: 1821   score: 4.0   memory length: 384822   epsilon: 0.4360504600104499    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 5.5\n",
      "episode: 1822   score: 5.0   memory length: 385145   epsilon: 0.43541092001044585    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 5.53\n",
      "episode: 1823   score: 4.0   memory length: 385403   epsilon: 0.4349000800104426    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 5.53\n",
      "episode: 1824   score: 3.0   memory length: 385667   epsilon: 0.4343773600104393    steps: 264    lr: 1.6000000000000003e-05     evaluation reward: 5.51\n",
      "episode: 1825   score: 4.0   memory length: 385943   epsilon: 0.43383088001043585    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 5.51\n",
      "episode: 1826   score: 8.0   memory length: 386414   epsilon: 0.43289830001042995    steps: 471    lr: 1.6000000000000003e-05     evaluation reward: 5.54\n",
      "episode: 1827   score: 5.0   memory length: 386756   epsilon: 0.43222114001042566    steps: 342    lr: 1.6000000000000003e-05     evaluation reward: 5.54\n",
      "episode: 1828   score: 4.0   memory length: 387031   epsilon: 0.4316766400104222    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 5.54\n",
      "episode: 1829   score: 7.0   memory length: 387441   epsilon: 0.4308648400104171    steps: 410    lr: 1.6000000000000003e-05     evaluation reward: 5.54\n",
      "episode: 1830   score: 14.0   memory length: 388120   epsilon: 0.4295204200104086    steps: 679    lr: 1.6000000000000003e-05     evaluation reward: 5.65\n",
      "episode: 1831   score: 6.0   memory length: 388496   epsilon: 0.42877594001040387    steps: 376    lr: 1.6000000000000003e-05     evaluation reward: 5.69\n",
      "episode: 1832   score: 5.0   memory length: 388820   epsilon: 0.4281344200103998    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 5.71\n",
      "episode: 1833   score: 8.0   memory length: 389273   epsilon: 0.42723748001039413    steps: 453    lr: 1.6000000000000003e-05     evaluation reward: 5.77\n",
      "episode: 1834   score: 9.0   memory length: 389755   epsilon: 0.4262831200103881    steps: 482    lr: 1.6000000000000003e-05     evaluation reward: 5.82\n",
      "episode: 1835   score: 6.0   memory length: 390090   epsilon: 0.4256198200103839    steps: 335    lr: 1.6000000000000003e-05     evaluation reward: 5.84\n",
      "episode: 1836   score: 6.0   memory length: 390425   epsilon: 0.4249565200103797    steps: 335    lr: 1.6000000000000003e-05     evaluation reward: 5.86\n",
      "episode: 1837   score: 5.0   memory length: 390752   epsilon: 0.4243090600103756    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 5.86\n",
      "episode: 1838   score: 7.0   memory length: 391174   epsilon: 0.4234735000103703    steps: 422    lr: 1.6000000000000003e-05     evaluation reward: 5.82\n",
      "episode: 1839   score: 4.0   memory length: 391451   epsilon: 0.42292504001036685    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 5.82\n",
      "episode: 1840   score: 6.0   memory length: 391788   epsilon: 0.4222577800103626    steps: 337    lr: 1.6000000000000003e-05     evaluation reward: 5.82\n",
      "episode: 1841   score: 5.0   memory length: 392094   epsilon: 0.4216519000103588    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 5.83\n",
      "episode: 1842   score: 9.0   memory length: 392556   epsilon: 0.420737140010353    steps: 462    lr: 1.6000000000000003e-05     evaluation reward: 5.88\n",
      "episode: 1843   score: 4.0   memory length: 392797   epsilon: 0.42025996001035    steps: 241    lr: 1.6000000000000003e-05     evaluation reward: 5.88\n",
      "episode: 1844   score: 6.0   memory length: 393148   epsilon: 0.4195649800103456    steps: 351    lr: 1.6000000000000003e-05     evaluation reward: 5.86\n",
      "episode: 1845   score: 4.0   memory length: 393424   epsilon: 0.41901850001034213    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 5.87\n",
      "episode: 1846   score: 4.0   memory length: 393722   epsilon: 0.4184284600103384    steps: 298    lr: 1.6000000000000003e-05     evaluation reward: 5.82\n",
      "episode: 1847   score: 5.0   memory length: 394033   epsilon: 0.4178126800103345    steps: 311    lr: 1.6000000000000003e-05     evaluation reward: 5.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1848   score: 7.0   memory length: 394418   epsilon: 0.4170503800103297    steps: 385    lr: 1.6000000000000003e-05     evaluation reward: 5.8\n",
      "episode: 1849   score: 4.0   memory length: 394677   epsilon: 0.41653756001032644    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 5.78\n",
      "episode: 1850   score: 4.0   memory length: 394936   epsilon: 0.4160247400103232    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 5.8\n",
      "episode: 1851   score: 4.0   memory length: 395198   epsilon: 0.4155059800103199    steps: 262    lr: 1.6000000000000003e-05     evaluation reward: 5.79\n",
      "episode: 1852   score: 2.0   memory length: 395380   epsilon: 0.41514562001031763    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 5.75\n",
      "episode: 1853   score: 4.0   memory length: 395637   epsilon: 0.4146367600103144    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 5.65\n",
      "episode: 1854   score: 4.0   memory length: 395894   epsilon: 0.4141279000103112    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 5.64\n",
      "episode: 1855   score: 5.0   memory length: 396198   epsilon: 0.4135259800103074    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 5.62\n",
      "episode: 1856   score: 4.0   memory length: 396438   epsilon: 0.4130507800103044    steps: 240    lr: 1.6000000000000003e-05     evaluation reward: 5.59\n",
      "episode: 1857   score: 8.0   memory length: 396909   epsilon: 0.4121182000102985    steps: 471    lr: 1.6000000000000003e-05     evaluation reward: 5.63\n",
      "episode: 1858   score: 6.0   memory length: 397254   epsilon: 0.41143510001029415    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 5.62\n",
      "episode: 1859   score: 7.0   memory length: 397677   epsilon: 0.41059756001028885    steps: 423    lr: 1.6000000000000003e-05     evaluation reward: 5.64\n",
      "episode: 1860   score: 10.0   memory length: 398257   epsilon: 0.4094491600102816    steps: 580    lr: 1.6000000000000003e-05     evaluation reward: 5.72\n",
      "episode: 1861   score: 6.0   memory length: 398608   epsilon: 0.4087541800102772    steps: 351    lr: 1.6000000000000003e-05     evaluation reward: 5.74\n",
      "episode: 1862   score: 6.0   memory length: 398987   epsilon: 0.40800376001027244    steps: 379    lr: 1.6000000000000003e-05     evaluation reward: 5.73\n",
      "episode: 1863   score: 4.0   memory length: 399284   epsilon: 0.4074157000102687    steps: 297    lr: 1.6000000000000003e-05     evaluation reward: 5.73\n",
      "episode: 1864   score: 3.0   memory length: 399494   epsilon: 0.4069999000102661    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 5.7\n",
      "episode: 1865   score: 5.0   memory length: 399786   epsilon: 0.40642174001026243    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 5.69\n",
      "episode: 1866   score: 5.0   memory length: 400101   epsilon: 0.4057980400102585    steps: 315    lr: 6.400000000000001e-06     evaluation reward: 5.68\n",
      "episode: 1867   score: 7.0   memory length: 400490   epsilon: 0.4050278200102536    steps: 389    lr: 6.400000000000001e-06     evaluation reward: 5.69\n",
      "episode: 1868   score: 7.0   memory length: 400863   epsilon: 0.40428928001024894    steps: 373    lr: 6.400000000000001e-06     evaluation reward: 5.73\n",
      "episode: 1869   score: 6.0   memory length: 401235   epsilon: 0.4035527200102443    steps: 372    lr: 6.400000000000001e-06     evaluation reward: 5.74\n",
      "episode: 1870   score: 8.0   memory length: 401684   epsilon: 0.40266370001023866    steps: 449    lr: 6.400000000000001e-06     evaluation reward: 5.72\n",
      "episode: 1871   score: 3.0   memory length: 401897   epsilon: 0.402241960010236    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 5.69\n",
      "episode: 1872   score: 8.0   memory length: 402329   epsilon: 0.4013866000102306    steps: 432    lr: 6.400000000000001e-06     evaluation reward: 5.73\n",
      "episode: 1873   score: 13.0   memory length: 402953   epsilon: 0.40015108001022276    steps: 624    lr: 6.400000000000001e-06     evaluation reward: 5.82\n",
      "episode: 1874   score: 6.0   memory length: 403311   epsilon: 0.3994422400102183    steps: 358    lr: 6.400000000000001e-06     evaluation reward: 5.84\n",
      "episode: 1875   score: 8.0   memory length: 403747   epsilon: 0.3985789600102128    steps: 436    lr: 6.400000000000001e-06     evaluation reward: 5.83\n",
      "episode: 1876   score: 10.0   memory length: 404269   epsilon: 0.3975454000102063    steps: 522    lr: 6.400000000000001e-06     evaluation reward: 5.88\n",
      "episode: 1877   score: 10.0   memory length: 404805   epsilon: 0.39648412001019956    steps: 536    lr: 6.400000000000001e-06     evaluation reward: 5.9\n",
      "episode: 1878   score: 3.0   memory length: 405031   epsilon: 0.3960366400101967    steps: 226    lr: 6.400000000000001e-06     evaluation reward: 5.89\n",
      "episode: 1879   score: 7.0   memory length: 405410   epsilon: 0.395286220010192    steps: 379    lr: 6.400000000000001e-06     evaluation reward: 5.85\n",
      "episode: 1880   score: 11.0   memory length: 405945   epsilon: 0.3942269200101853    steps: 535    lr: 6.400000000000001e-06     evaluation reward: 5.91\n",
      "episode: 1881   score: 4.0   memory length: 406224   epsilon: 0.3936745000101818    steps: 279    lr: 6.400000000000001e-06     evaluation reward: 5.9\n",
      "episode: 1882   score: 6.0   memory length: 406599   epsilon: 0.3929320000101771    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 5.92\n",
      "episode: 1883   score: 8.0   memory length: 406877   epsilon: 0.3923815600101736    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 5.97\n",
      "episode: 1884   score: 7.0   memory length: 407284   epsilon: 0.3915757000101685    steps: 407    lr: 6.400000000000001e-06     evaluation reward: 5.93\n",
      "episode: 1885   score: 6.0   memory length: 407661   epsilon: 0.3908292400101638    steps: 377    lr: 6.400000000000001e-06     evaluation reward: 5.93\n",
      "episode: 1886   score: 10.0   memory length: 408254   epsilon: 0.38965510001015635    steps: 593    lr: 6.400000000000001e-06     evaluation reward: 6.01\n",
      "episode: 1887   score: 5.0   memory length: 408563   epsilon: 0.3890432800101525    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 5.99\n",
      "episode: 1888   score: 4.0   memory length: 408881   epsilon: 0.3884136400101485    steps: 318    lr: 6.400000000000001e-06     evaluation reward: 5.94\n",
      "episode: 1889   score: 4.0   memory length: 409143   epsilon: 0.3878948800101452    steps: 262    lr: 6.400000000000001e-06     evaluation reward: 5.92\n",
      "episode: 1890   score: 5.0   memory length: 409486   epsilon: 0.3872157400101409    steps: 343    lr: 6.400000000000001e-06     evaluation reward: 5.93\n",
      "episode: 1891   score: 10.0   memory length: 410009   epsilon: 0.38618020001013437    steps: 523    lr: 6.400000000000001e-06     evaluation reward: 5.99\n",
      "episode: 1892   score: 8.0   memory length: 410442   epsilon: 0.38532286001012894    steps: 433    lr: 6.400000000000001e-06     evaluation reward: 6.02\n",
      "episode: 1893   score: 12.0   memory length: 411024   epsilon: 0.38417050001012165    steps: 582    lr: 6.400000000000001e-06     evaluation reward: 6.1\n",
      "episode: 1894   score: 8.0   memory length: 411442   epsilon: 0.3833428600101164    steps: 418    lr: 6.400000000000001e-06     evaluation reward: 6.07\n",
      "episode: 1895   score: 5.0   memory length: 411788   epsilon: 0.3826577800101121    steps: 346    lr: 6.400000000000001e-06     evaluation reward: 6.05\n",
      "episode: 1896   score: 5.0   memory length: 412061   epsilon: 0.38211724001010866    steps: 273    lr: 6.400000000000001e-06     evaluation reward: 6.04\n",
      "episode: 1897   score: 7.0   memory length: 412413   epsilon: 0.38142028001010425    steps: 352    lr: 6.400000000000001e-06     evaluation reward: 6.08\n",
      "episode: 1898   score: 9.0   memory length: 412852   epsilon: 0.38055106001009875    steps: 439    lr: 6.400000000000001e-06     evaluation reward: 6.09\n",
      "episode: 1899   score: 6.0   memory length: 413205   epsilon: 0.37985212001009433    steps: 353    lr: 6.400000000000001e-06     evaluation reward: 6.1\n",
      "episode: 1900   score: 6.0   memory length: 413557   epsilon: 0.3791551600100899    steps: 352    lr: 6.400000000000001e-06     evaluation reward: 6.1\n",
      "episode: 1901   score: 7.0   memory length: 413942   epsilon: 0.3783928600100851    steps: 385    lr: 6.400000000000001e-06     evaluation reward: 6.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1902   score: 4.0   memory length: 414203   epsilon: 0.3778760800100818    steps: 261    lr: 6.400000000000001e-06     evaluation reward: 6.09\n",
      "episode: 1903   score: 6.0   memory length: 414598   epsilon: 0.3770939800100769    steps: 395    lr: 6.400000000000001e-06     evaluation reward: 6.1\n",
      "episode: 1904   score: 6.0   memory length: 414972   epsilon: 0.3763534600100722    steps: 374    lr: 6.400000000000001e-06     evaluation reward: 6.09\n",
      "episode: 1905   score: 4.0   memory length: 415265   epsilon: 0.3757733200100685    steps: 293    lr: 6.400000000000001e-06     evaluation reward: 6.06\n",
      "episode: 1906   score: 8.0   memory length: 415751   epsilon: 0.37481104001006244    steps: 486    lr: 6.400000000000001e-06     evaluation reward: 6.06\n",
      "episode: 1907   score: 9.0   memory length: 416210   epsilon: 0.3739022200100567    steps: 459    lr: 6.400000000000001e-06     evaluation reward: 6.11\n",
      "episode: 1908   score: 7.0   memory length: 416593   epsilon: 0.3731438800100519    steps: 383    lr: 6.400000000000001e-06     evaluation reward: 6.12\n",
      "episode: 1909   score: 7.0   memory length: 416990   epsilon: 0.3723578200100469    steps: 397    lr: 6.400000000000001e-06     evaluation reward: 6.13\n",
      "episode: 1910   score: 5.0   memory length: 417301   epsilon: 0.371742040010043    steps: 311    lr: 6.400000000000001e-06     evaluation reward: 6.12\n",
      "episode: 1911   score: 7.0   memory length: 417724   epsilon: 0.3709045000100377    steps: 423    lr: 6.400000000000001e-06     evaluation reward: 6.16\n",
      "episode: 1912   score: 11.0   memory length: 418226   epsilon: 0.36991054001003143    steps: 502    lr: 6.400000000000001e-06     evaluation reward: 6.25\n",
      "episode: 1913   score: 13.0   memory length: 418687   epsilon: 0.36899776001002565    steps: 461    lr: 6.400000000000001e-06     evaluation reward: 6.34\n",
      "episode: 1914   score: 8.0   memory length: 419167   epsilon: 0.36804736001001964    steps: 480    lr: 6.400000000000001e-06     evaluation reward: 6.38\n",
      "episode: 1915   score: 8.0   memory length: 419595   epsilon: 0.3671999200100143    steps: 428    lr: 6.400000000000001e-06     evaluation reward: 6.43\n",
      "episode: 1916   score: 6.0   memory length: 419916   epsilon: 0.36656434001001026    steps: 321    lr: 6.400000000000001e-06     evaluation reward: 6.42\n",
      "episode: 1917   score: 7.0   memory length: 420295   epsilon: 0.3658139200100055    steps: 379    lr: 6.400000000000001e-06     evaluation reward: 6.41\n",
      "episode: 1918   score: 10.0   memory length: 420796   epsilon: 0.36482194000999923    steps: 501    lr: 6.400000000000001e-06     evaluation reward: 6.46\n",
      "episode: 1919   score: 6.0   memory length: 421173   epsilon: 0.3640754800099945    steps: 377    lr: 6.400000000000001e-06     evaluation reward: 6.45\n",
      "episode: 1920   score: 13.0   memory length: 421661   epsilon: 0.3631092400099884    steps: 488    lr: 6.400000000000001e-06     evaluation reward: 6.49\n",
      "episode: 1921   score: 4.0   memory length: 421903   epsilon: 0.36263008000998537    steps: 242    lr: 6.400000000000001e-06     evaluation reward: 6.49\n",
      "episode: 1922   score: 6.0   memory length: 422260   epsilon: 0.3619232200099809    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 6.5\n",
      "episode: 1923   score: 2.0   memory length: 422442   epsilon: 0.3615628600099786    steps: 182    lr: 6.400000000000001e-06     evaluation reward: 6.48\n",
      "episode: 1924   score: 10.0   memory length: 422990   epsilon: 0.36047782000997175    steps: 548    lr: 6.400000000000001e-06     evaluation reward: 6.55\n",
      "episode: 1925   score: 5.0   memory length: 423296   epsilon: 0.3598719400099679    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 6.56\n",
      "episode: 1926   score: 7.0   memory length: 423719   epsilon: 0.3590344000099626    steps: 423    lr: 6.400000000000001e-06     evaluation reward: 6.55\n",
      "episode: 1927   score: 8.0   memory length: 424123   epsilon: 0.35823448000995756    steps: 404    lr: 6.400000000000001e-06     evaluation reward: 6.58\n",
      "episode: 1928   score: 7.0   memory length: 424516   epsilon: 0.35745634000995263    steps: 393    lr: 6.400000000000001e-06     evaluation reward: 6.61\n",
      "episode: 1929   score: 16.0   memory length: 425076   epsilon: 0.3563475400099456    steps: 560    lr: 6.400000000000001e-06     evaluation reward: 6.7\n",
      "episode: 1930   score: 8.0   memory length: 425515   epsilon: 0.3554783200099401    steps: 439    lr: 6.400000000000001e-06     evaluation reward: 6.64\n",
      "episode: 1931   score: 8.0   memory length: 425937   epsilon: 0.35464276000993483    steps: 422    lr: 6.400000000000001e-06     evaluation reward: 6.66\n",
      "episode: 1932   score: 4.0   memory length: 426196   epsilon: 0.3541299400099316    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 6.65\n",
      "episode: 1933   score: 3.0   memory length: 426409   epsilon: 0.3537082000099289    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 6.6\n",
      "episode: 1934   score: 4.0   memory length: 426649   epsilon: 0.3532330000099259    steps: 240    lr: 6.400000000000001e-06     evaluation reward: 6.55\n",
      "episode: 1935   score: 6.0   memory length: 427026   epsilon: 0.3524865400099212    steps: 377    lr: 6.400000000000001e-06     evaluation reward: 6.55\n",
      "episode: 1936   score: 9.0   memory length: 427501   epsilon: 0.35154604000991524    steps: 475    lr: 6.400000000000001e-06     evaluation reward: 6.58\n",
      "episode: 1937   score: 6.0   memory length: 427836   epsilon: 0.35088274000991104    steps: 335    lr: 6.400000000000001e-06     evaluation reward: 6.59\n",
      "episode: 1938   score: 9.0   memory length: 428302   epsilon: 0.3499600600099052    steps: 466    lr: 6.400000000000001e-06     evaluation reward: 6.61\n",
      "episode: 1939   score: 5.0   memory length: 428611   epsilon: 0.34934824000990133    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 6.62\n",
      "episode: 1940   score: 7.0   memory length: 429022   epsilon: 0.3485344600098962    steps: 411    lr: 6.400000000000001e-06     evaluation reward: 6.63\n",
      "episode: 1941   score: 10.0   memory length: 429544   epsilon: 0.34750090000988965    steps: 522    lr: 6.400000000000001e-06     evaluation reward: 6.68\n",
      "episode: 1942   score: 3.0   memory length: 429773   epsilon: 0.3470474800098868    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 6.62\n",
      "episode: 1943   score: 5.0   memory length: 430082   epsilon: 0.3464356600098829    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 6.63\n",
      "episode: 1944   score: 5.0   memory length: 430391   epsilon: 0.34582384000987904    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 6.62\n",
      "episode: 1945   score: 7.0   memory length: 430743   epsilon: 0.3451268800098746    steps: 352    lr: 6.400000000000001e-06     evaluation reward: 6.65\n",
      "episode: 1946   score: 12.0   memory length: 431168   epsilon: 0.3442853800098693    steps: 425    lr: 6.400000000000001e-06     evaluation reward: 6.73\n",
      "episode: 1947   score: 6.0   memory length: 431535   epsilon: 0.3435587200098647    steps: 367    lr: 6.400000000000001e-06     evaluation reward: 6.74\n",
      "episode: 1948   score: 6.0   memory length: 431893   epsilon: 0.3428498800098602    steps: 358    lr: 6.400000000000001e-06     evaluation reward: 6.73\n",
      "episode: 1949   score: 8.0   memory length: 432346   epsilon: 0.34195294000985454    steps: 453    lr: 6.400000000000001e-06     evaluation reward: 6.77\n",
      "episode: 1950   score: 3.0   memory length: 432556   epsilon: 0.3415371400098519    steps: 210    lr: 6.400000000000001e-06     evaluation reward: 6.76\n",
      "episode: 1951   score: 6.0   memory length: 432894   epsilon: 0.3408679000098477    steps: 338    lr: 6.400000000000001e-06     evaluation reward: 6.78\n",
      "episode: 1952   score: 7.0   memory length: 433300   epsilon: 0.3400640200098426    steps: 406    lr: 6.400000000000001e-06     evaluation reward: 6.83\n",
      "episode: 1953   score: 9.0   memory length: 433784   epsilon: 0.33910570000983653    steps: 484    lr: 6.400000000000001e-06     evaluation reward: 6.88\n",
      "episode: 1954   score: 6.0   memory length: 434137   epsilon: 0.3384067600098321    steps: 353    lr: 6.400000000000001e-06     evaluation reward: 6.9\n",
      "episode: 1955   score: 4.0   memory length: 434399   epsilon: 0.3378880000098288    steps: 262    lr: 6.400000000000001e-06     evaluation reward: 6.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1956   score: 6.0   memory length: 434757   epsilon: 0.33717916000982434    steps: 358    lr: 6.400000000000001e-06     evaluation reward: 6.91\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(1957):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        r = np.clip(reward, -1, 1) \n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn1.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn1.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzyBJZiVFMnd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MP5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
